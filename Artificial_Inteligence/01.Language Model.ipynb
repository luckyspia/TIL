{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 01. Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, 2, 3]) # 리스트 형태로 벡터 생성, 파이썬의 리스트를 numpy의 array(배열)로 변환\n",
    "a # numpy 내부적으로 벡터나 행렬을 효율적으로 처리하기 위해 만든 자료 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 12, 21])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b # 단순 곱 = 배열 위치에 따른 짝지은 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, b) # dot product, 곱한 수의 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b # 새로운 연산자(dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('i,i', a, b) # einstein sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([[1, 2, 3], [4, 5, 6]]) # 리스트 형태로 i번째 행을 표현, 하나의 행을 리스트로 표현\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([[1, 2], [3, 4], [5, 6]]) # c가 3열이므로, d는 무조건 3행이 와야함\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [49, 64]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [49, 64]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c @ d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0, :] # 첫 번째 행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:, 0] # 첫 번째 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0, :] @ d[:, 0] # 첫 번째 행과 첫 번째 열의 곱, 1k와 k1의 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [49, 64]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ik,kj->ij', c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline # 특정한 종류의 자연어 처리 작업을 간단히 할 수 있도록 지원\n",
    "classifier = pipeline('sentiment-analysis') \n",
    "# 트랜스포머 모형을 이용한 감성분석, 하고자하는 과제의 종류를 파이프라인에 넣어 과제에 맞는 학습된 모형을 다운받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998080134391785}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('I am glad to hear that you finally made it.') #감성분석: 긍정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9981151819229126}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('I am sorry that you hate it.') #감성분석: 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"monologg/koelectra-small-finetuned-nsmc\") # 한국어 가능 모델 적용\n",
    "# 한국어 영화평을 기준으로 감성분석이 실행되어 있는 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9803304076194763}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('이 영화 진짜 재밌다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.8447047472000122}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('이야기가 말이 안된다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c https://github.com/songys/Chatbot_data/raw/master/ChatbotData%20.csv -O chatbot.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/chatbot.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sample.txt','w',encoding='utf-8') as f: # sample.txt 라는 이름으로 저장\n",
    "  for row in df.itertuples():\n",
    "    f.write(row.Q + '\\n')\n",
    "    f.write(row.A + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer # 글자 단위의 BPE 토큰화\n",
    "bpe = CharBPETokenizer(lowercase=True) # 대문자를 소문자로 바꾸는 옵션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘를 학습하는 함수 \n",
    "# 파일이 여러 개라면 리스트 형태로 제시 \n",
    "# min_frequency = 최소 몇 번 나와야 어휘에 포함\n",
    "# vocab_size = 몇 개의 어휘를 학습할 것인가\n",
    "bpe.train(files='./data/sample.txt',min_frequency=1, vocab_size=5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2206, 1021, 797, 1875, 2251, 1038, 1540]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = bpe.encode('자연어 처리는 재밌다!')\n",
    "enc.ids #텍스트를 토큰화하여 고유번호로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연', '어</w>', '처', '리는</w>', '재밌', '다</w>', '!</w>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens #고유번호의 글자 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "with open('decomposed.txt','w',encoding='utf-8') as f:\n",
    "  for row in df.itertuples():\n",
    "    q = unicodedata.normalize('NFD', row.Q)\n",
    "    f.write(q + '\\n')\n",
    "    a = unicodedata.normalize('NFD', row.A)\n",
    "    f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head decomposed.txt \n",
    "# 윈도우 메모장에선 풀어쓰기 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = CharBPETokenizer(lowercase=True)\n",
    "bpe.train(files='decomposed.txt',min_frequency=1,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = unicodedata.normalize('NFD','자연어 처리는 재밌다!') # 풀어쓰기로 학습시켰으므로, 토큰화 시킬 때도 풀어쓰기해줘야함\n",
    "enc = bpe.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1290, 204, 299, 1547, 819, 1365, 162]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연', '어</w>', '처', '리는</w>', '재미', 'ᆻ다</w>', '!</w>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer #바이트단위 토큰화\n",
    "byte = ByteLevelBPETokenizer(lowercase=True)\n",
    "byte.train(files='./data/sample.txt', min_frequency=1, vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2500, 273, 3488, 99, 1056, 2276, 294, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = byte.encode('자연어 처리는 재밌다!')\n",
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìŀĲìĹ°', 'ìĸ´', 'Ġì²ĺë', '¦', '¬ëĬĶ', 'Ġìŀ¬ë°Į', 'ëĭ¤', '!']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens # 한글은 여러 개의 바이트로 되어있기 때문에 바이트 단위로 쪼개지면서 한글이 아닌 것처럼 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer # wordpiece model\n",
    "wp = BertWordPieceTokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.train(files='./data/sample.txt',min_frequency=1,vocab_size=5000)\n",
    "enc = wp.encode('자연어 처리는 재밌다!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1379, 201, 1014, 1028, 3349, 216, 5]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연', '##어', '처', '##리는', '재밌', '##다', '!']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens # ## <- 앞에 말에 이어서 쓰라는 의미, BPE는 끝나는 지점을 표시하고, WordPiece는 이어지는 부분을 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "sp = SentencePieceBPETokenizer() # SentencePiece는 대문자를 소문자로 변화하는 옵션이 없음\n",
    "sp.train('./data/sample.txt',min_frequency=1,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1888, 617, 1543, 1397, 1944, 225, 3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc= sp.encode('자연어 처리는 재밌다!')\n",
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁자연', '어', '▁처', '리는', '▁재밌', '다', '!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.09003057, 0.24472848, 0.66524094], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.nn.softmax([-1.0, 0.0, 1.0])\n",
    "# softmax 함수는 여러 개의 로짓을 입력 받아, 확률로 바꿔주는 함수이다. 로짓은 실수형으로 주어져야 함\n",
    "# 로짓이 상대적으로 크면 확률이 높다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.09003058, 0.24472848, 0.66524094], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp([-1.0, 0.0, 1.0]) / tf.reduce_sum(tf.exp([-1.0, 0.0, 1.0]))\n",
    "# softmax 함수는 지수함수를 바탕으로 둠, 지수함수는 -입력값을 받아도 +출력값을 반환하며 값의 대소에 영향을 받는다\n",
    "# softmax 함수의 계산식 (reudce_sum = 총합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8807971>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.sigmoid(2.0)\n",
    "# sigmoid는 하나의 값만 가지며, 로짓을 입력받아 확률을 반환한다\n",
    "# softmax에서 특수한 이진분류 케이스일 때 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.11920291, 0.880797  ], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax([0.0, 2.0]) \n",
    "# softmax는 한 쪽에 0.0으로 고정시키고, 한 쪽만 변환시키는 값과 sigmoid와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언어 모형으로 다음 단어 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "# TFAutoModelForCausalLM: LM, 언어 모형, 앞의 맥락을 통해 뒤의 내용을 예측하는 모형\n",
    "# AutoTkenizer: 미리 학습된 토크나이저를 불러올 때 사용하는 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFXLNetLMHeadModel.\n",
      "\n",
      "All the layers of TFXLNetLMHeadModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 모형 다운로드, 사전학습된 tokenizer 호출\n",
    "# 가장 작은 base 모델 사용, 크기가 클수록 용량도 크지만 성능은 좋아짐\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 호출한 tokenizer의 어휘 수\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 호출한 tokenizer의 단어 목록\n",
    "vocab = tokenizer.get_vocab()\n",
    "id2word = {i: word for word, i in vocab.items()} # 번호를 key로 하는 dictionary 형성, 번호를 통해 검색 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tail': 9424,\n",
       " 'valu': 18425,\n",
       " '▁quarterfinal': 23636,\n",
       " '▁carved': 11410,\n",
       " '▁Tre': 5738,\n",
       " '▁outlines': 20041,\n",
       " '▁populace': 28555,\n",
       " '▁settlement': 2525,\n",
       " '▁truck': 2801,\n",
       " '▁Apr': 21057,\n",
       " '▁Barr': 9353,\n",
       " 'utter': 9035,\n",
       " '▁Consulting': 22758,\n",
       " 'bli': 17704,\n",
       " '▁Jesuit': 21263,\n",
       " '▁Infrastructure': 23729,\n",
       " '▁boomers': 26990,\n",
       " '▁brewery': 27120,\n",
       " '▁Lim': 13947,\n",
       " '▁Look': 6504,\n",
       " '▁separatist': 10685,\n",
       " '▁even': 176,\n",
       " '▁certificates': 14561,\n",
       " '▁clothing': 4688,\n",
       " '▁be': 39,\n",
       " '▁stunt': 14838,\n",
       " 'Americans': 15052,\n",
       " '▁Secondary': 17239,\n",
       " 'goal': 19464,\n",
       " '▁Stefan': 10918,\n",
       " '▁competence': 21073,\n",
       " '▁resembling': 24248,\n",
       " '▁ambient': 26566,\n",
       " 'negative': 25976,\n",
       " '▁Bart': 10543,\n",
       " '▁refrain': 14333,\n",
       " '▁Tele': 9690,\n",
       " '▁magnitude': 10270,\n",
       " '▁ousted': 10787,\n",
       " '▁decorating': 23653,\n",
       " 'equity': 18265,\n",
       " 'concern': 29339,\n",
       " '▁EDI': 30700,\n",
       " '▁jurist': 30908,\n",
       " '▁excellent': 2712,\n",
       " 'cran': 16354,\n",
       " '▁reject': 8006,\n",
       " 'ker': 2261,\n",
       " '▁key': 792,\n",
       " '▁Avalanche': 27749,\n",
       " '▁Roof': 29072,\n",
       " '▁feminist': 16458,\n",
       " '▁wildly': 16856,\n",
       " '▁Fabio': 26293,\n",
       " '▁directory': 5659,\n",
       " '▁pace': 3268,\n",
       " '▁winning': 1544,\n",
       " '▁NCAA': 6801,\n",
       " '▁confrontation': 9638,\n",
       " '▁adv': 25158,\n",
       " '▁bald': 19139,\n",
       " '▁Sources': 19584,\n",
       " '▁hover': 26400,\n",
       " 'focused': 21205,\n",
       " '▁paranormal': 31391,\n",
       " '▁Ask': 9729,\n",
       " '▁Cottage': 26415,\n",
       " '▁software': 1058,\n",
       " '▁wallpaper': 21616,\n",
       " '▁execution': 5448,\n",
       " '▁rubble': 16226,\n",
       " '▁telecom': 15501,\n",
       " '▁leaders': 783,\n",
       " 'hart': 11930,\n",
       " '▁modules': 12772,\n",
       " '▁cohesive': 30133,\n",
       " '▁Brig': 18739,\n",
       " '▁Camille': 30223,\n",
       " 'Aryan': 31784,\n",
       " '▁cheerful': 19271,\n",
       " '▁donated': 8076,\n",
       " 'wing': 2816,\n",
       " '▁players': 793,\n",
       " '▁plus': 2824,\n",
       " '▁pretend': 13287,\n",
       " '▁Life': 2782,\n",
       " '▁Denver': 5748,\n",
       " '▁coupons': 17838,\n",
       " '▁saint': 16305,\n",
       " '▁Naples': 17495,\n",
       " '▁Kabila': 18573,\n",
       " 'hur': 8215,\n",
       " '▁Gage': 22739,\n",
       " '▁Rowe': 26093,\n",
       " '▁notion': 7097,\n",
       " '▁White': 924,\n",
       " '▁Avalon': 29691,\n",
       " '▁Sprint': 16951,\n",
       " 'India': 20116,\n",
       " '▁admit': 6024,\n",
       " '▁Bald': 21571,\n",
       " '▁sipping': 30581,\n",
       " '▁assembly': 4997,\n",
       " 'hammer': 22889,\n",
       " '▁Shang': 28605,\n",
       " '▁colonel': 17189,\n",
       " '▁Vera': 20847,\n",
       " 'kha': 10656,\n",
       " '▁domain': 4987,\n",
       " '▁Kenyan': 12670,\n",
       " '▁Immigration': 14845,\n",
       " '▁east': 1646,\n",
       " '▁assisted': 9643,\n",
       " 'Note': 18253,\n",
       " '▁Olympus': 31444,\n",
       " 'saving': 14219,\n",
       " '▁issues': 705,\n",
       " '▁defend': 4567,\n",
       " '▁fabulous': 14415,\n",
       " '▁Making': 13175,\n",
       " 'pic': 5005,\n",
       " '▁waived': 23709,\n",
       " 'lem': 12849,\n",
       " '▁array': 6223,\n",
       " '30,000': 16409,\n",
       " '▁tail': 5438,\n",
       " '▁Garden': 5709,\n",
       " '▁bonus': 4633,\n",
       " 'oker': 15213,\n",
       " '▁artificial': 8298,\n",
       " '▁volleyball': 15494,\n",
       " 'Mari': 24731,\n",
       " 'bound': 4785,\n",
       " '▁knot': 11792,\n",
       " '▁municipalities': 12905,\n",
       " '▁repeated': 4484,\n",
       " '▁mellow': 31416,\n",
       " '▁Officers': 18503,\n",
       " '▁whistle': 11711,\n",
       " '▁Equatorial': 31959,\n",
       " '▁biblical': 17058,\n",
       " '▁Len': 9783,\n",
       " '▁manner': 2989,\n",
       " '▁slab': 20611,\n",
       " '▁partner': 2229,\n",
       " 'ile': 4365,\n",
       " '▁fountain': 14269,\n",
       " '▁Pepsi': 20607,\n",
       " 'File': 23924,\n",
       " 'vil': 9908,\n",
       " 'ci': 2294,\n",
       " '▁Purchasing': 30732,\n",
       " '▁filmmakers': 20163,\n",
       " '23': 2812,\n",
       " '▁Con': 1387,\n",
       " '▁horde': 28494,\n",
       " '▁progressive': 9708,\n",
       " '▁hostilities': 20510,\n",
       " '▁bikini': 26679,\n",
       " 'ces': 6909,\n",
       " '▁Labrador': 27987,\n",
       " 'trophic': 26707,\n",
       " 'bra': 3890,\n",
       " 'para': 11669,\n",
       " '▁Fujimori': 23326,\n",
       " 'rator': 12742,\n",
       " '▁relying': 16419,\n",
       " '▁mythology': 20153,\n",
       " '▁looted': 25735,\n",
       " 'dicate': 12477,\n",
       " '▁time': 92,\n",
       " '▁strive': 10960,\n",
       " '▁financial': 638,\n",
       " '▁absolutely': 4242,\n",
       " '▁Lot': 19756,\n",
       " ',': 19,\n",
       " '▁robust': 9024,\n",
       " '▁electorate': 16878,\n",
       " '▁species': 1214,\n",
       " '▁mapping': 14743,\n",
       " '▁Cin': 20653,\n",
       " 'project': 25184,\n",
       " '▁Pfizer': 25587,\n",
       " '▁leftist': 13597,\n",
       " 'LM': 12269,\n",
       " '▁Owner': 16739,\n",
       " 'View': 21246,\n",
       " '▁workout': 10809,\n",
       " '▁Councillor': 28865,\n",
       " '▁Rail': 12736,\n",
       " '▁Ethan': 12255,\n",
       " '▁osteoporosis': 30095,\n",
       " '▁Just': 1641,\n",
       " '81': 5184,\n",
       " '▁cheese': 6041,\n",
       " 'tower': 20582,\n",
       " 'minating': 15129,\n",
       " '▁courthouse': 16728,\n",
       " '▁Thomas': 1672,\n",
       " 'nay': 17007,\n",
       " '▁Boxer': 27389,\n",
       " '▁accusing': 11010,\n",
       " '▁Secondly': 25598,\n",
       " '▁Hour': 18808,\n",
       " '▁purchasing': 7015,\n",
       " '▁semiconductor': 18108,\n",
       " '▁reward': 8614,\n",
       " '▁dealership': 18095,\n",
       " 'akov': 22132,\n",
       " '▁street': 1764,\n",
       " '▁arrival': 4004,\n",
       " '▁Yushchenko': 23047,\n",
       " '▁definitive': 15520,\n",
       " '▁awareness': 5454,\n",
       " '▁Reference': 17931,\n",
       " '▁prosecuted': 19354,\n",
       " '▁Balance': 23256,\n",
       " '▁Angus': 25150,\n",
       " '▁corrupt': 9453,\n",
       " '▁Arbitration': 31059,\n",
       " '▁tuna': 22852,\n",
       " '72': 4723,\n",
       " '▁arguments': 7405,\n",
       " 'script': 9671,\n",
       " '▁Marcos': 19253,\n",
       " '▁primaries': 20253,\n",
       " '▁sir': 8376,\n",
       " '▁Exclusive': 31062,\n",
       " '$': 5780,\n",
       " '▁Radovan': 29011,\n",
       " '▁rely': 6871,\n",
       " 'quality': 7516,\n",
       " '▁Sherman': 17452,\n",
       " '▁alert': 5095,\n",
       " '▁Greyhound': 31477,\n",
       " '▁parity': 28669,\n",
       " '▁Izetbegovic': 31563,\n",
       " '▁urgently': 23772,\n",
       " '▁Kind': 19058,\n",
       " 'Earl': 30651,\n",
       " '▁1877': 21074,\n",
       " '▁barge': 21398,\n",
       " '▁expectancy': 26171,\n",
       " '▁Mall': 10750,\n",
       " '▁Qin': 24478,\n",
       " '▁relating': 7376,\n",
       " '▁tradition': 3846,\n",
       " '▁Fighting': 17751,\n",
       " '▁attracted': 6400,\n",
       " '▁Skype': 28513,\n",
       " '▁coroner': 29090,\n",
       " 'cheep': 21983,\n",
       " '▁bullshit': 29565,\n",
       " 'common': 21256,\n",
       " '▁shock': 4674,\n",
       " 'iate': 16218,\n",
       " '▁meets': 5971,\n",
       " '▁Algeria': 12060,\n",
       " '▁knife': 7122,\n",
       " '▁discarded': 20571,\n",
       " '▁WI': 13093,\n",
       " '▁Omar': 10863,\n",
       " '▁LinkedIn': 31355,\n",
       " '▁clip': 9555,\n",
       " '▁enrollment': 12639,\n",
       " '▁jug': 22763,\n",
       " '▁Arkansas': 7597,\n",
       " '▁Khatami': 22722,\n",
       " '▁Warehouse': 30118,\n",
       " '1982': 25725,\n",
       " '▁picking': 7761,\n",
       " '▁LPGA': 23920,\n",
       " '▁Ivan': 9411,\n",
       " '▁Forget': 22150,\n",
       " '▁Regulatory': 26901,\n",
       " '▁bark': 11872,\n",
       " '▁niche': 12721,\n",
       " 'RL': 14096,\n",
       " '▁perpetrated': 31196,\n",
       " '▁lo': 9570,\n",
       " '▁Mills': 12029,\n",
       " '▁Communist': 4431,\n",
       " '▁accredited': 14866,\n",
       " '▁Astros': 20706,\n",
       " '▁Helicopter': 30355,\n",
       " '▁initiate': 15701,\n",
       " '▁Hack': 18997,\n",
       " '▁fees': 3407,\n",
       " '▁uncommon': 15926,\n",
       " '▁Ice': 9605,\n",
       " '▁begging': 20086,\n",
       " 'quin': 9914,\n",
       " '▁$8': 6664,\n",
       " '▁Recreation': 18056,\n",
       " '▁indignation': 31594,\n",
       " '▁licenses': 11798,\n",
       " 'form': 3362,\n",
       " '▁250': 6397,\n",
       " 'bate': 16428,\n",
       " '▁extradite': 30172,\n",
       " '▁Muslim': 2246,\n",
       " 'Man': 5916,\n",
       " '▁abilities': 8621,\n",
       " '▁cockpit': 19265,\n",
       " '▁family': 273,\n",
       " 'touch': 19945,\n",
       " '▁Monarch': 25507,\n",
       " '▁disposition': 21424,\n",
       " '▁1988,': 13181,\n",
       " 'family': 13606,\n",
       " '▁Jason': 4715,\n",
       " '▁Gav': 27652,\n",
       " '▁conditioning': 14257,\n",
       " 'Cross': 27982,\n",
       " '▁Because': 1999,\n",
       " '▁Association': 1093,\n",
       " '▁marathon': 11861,\n",
       " 'itu': 14463,\n",
       " '▁Stafford': 24377,\n",
       " '▁$800': 25078,\n",
       " '▁gang': 6564,\n",
       " '▁Decor': 23191,\n",
       " 'RM': 8833,\n",
       " '▁marshes': 30965,\n",
       " '▁1972,': 17258,\n",
       " '▁ethics': 9761,\n",
       " 'TEN': 24728,\n",
       " '▁soared': 14021,\n",
       " '▁propped': 27340,\n",
       " '▁Tran': 17650,\n",
       " 'izo': 22184,\n",
       " '▁COXNET': 31814,\n",
       " '▁Duchy': 29095,\n",
       " '▁habit': 9524,\n",
       " '▁router': 21348,\n",
       " '▁clove': 23354,\n",
       " '▁edges': 10403,\n",
       " '▁do': 112,\n",
       " 'в': 25711,\n",
       " '▁XML': 15115,\n",
       " '▁messenger': 19589,\n",
       " 'Bob': 25055,\n",
       " 'Someone': 26655,\n",
       " '▁Barak': 12462,\n",
       " '▁fingerprint': 18267,\n",
       " 'image': 24462,\n",
       " '▁urge': 6807,\n",
       " '▁says': 349,\n",
       " '▁Steve': 2875,\n",
       " '▁conversations': 9750,\n",
       " '▁Strip': 5918,\n",
       " 'Breaking': 26834,\n",
       " '▁Healing': 29952,\n",
       " '▁Wind': 8181,\n",
       " 'pass': 7647,\n",
       " '▁Turin': 19372,\n",
       " '▁Dome': 17125,\n",
       " 'By': 10093,\n",
       " 'both': 13717,\n",
       " 'oca': 19673,\n",
       " '▁instituted': 22131,\n",
       " 'PA': 5896,\n",
       " '▁generation': 2887,\n",
       " 'RET': 28644,\n",
       " '▁steak': 17219,\n",
       " '▁electrical': 5836,\n",
       " '▁wealthy': 7501,\n",
       " '▁Branson': 29851,\n",
       " 'dependent': 18160,\n",
       " '▁Sen': 2135,\n",
       " '▁mapped': 25790,\n",
       " 'filtration': 20121,\n",
       " '▁refusal': 10937,\n",
       " '▁lawyer': 2301,\n",
       " '▁waging': 24727,\n",
       " '▁another': 245,\n",
       " '▁portable': 10956,\n",
       " '▁Shore': 13365,\n",
       " '▁Oliver': 9730,\n",
       " '▁speak': 2040,\n",
       " '▁Longhorns': 31449,\n",
       " '▁makers': 7928,\n",
       " '▁admirable': 30070,\n",
       " '▁tales': 14753,\n",
       " 'hander': 15944,\n",
       " '▁Oscar': 6497,\n",
       " '▁Edwards': 7400,\n",
       " '▁lane': 14566,\n",
       " '▁Vor': 21884,\n",
       " 'anza': 13850,\n",
       " '▁romance': 10819,\n",
       " '▁hug': 11864,\n",
       " '▁Again': 7261,\n",
       " '▁liaison': 19410,\n",
       " 'FB': 16580,\n",
       " 'ART': 9825,\n",
       " '▁peaks': 17493,\n",
       " '▁Paula': 19170,\n",
       " '▁total': 652,\n",
       " '▁ideal': 3358,\n",
       " 'igen': 22625,\n",
       " 'appropriate': 26537,\n",
       " '▁Remote': 23486,\n",
       " '▁hamlet': 19222,\n",
       " 'tain': 3766,\n",
       " '▁Tom': 1928,\n",
       " '▁BBC': 4843,\n",
       " 'lou': 7604,\n",
       " '▁strongly': 3877,\n",
       " '▁improperly': 24284,\n",
       " 'MORE': 9478,\n",
       " '▁opportunities': 2361,\n",
       " '▁Cruise': 12634,\n",
       " '▁assert': 14878,\n",
       " 'FL': 4593,\n",
       " 'ghi': 19475,\n",
       " 'SPA': 22313,\n",
       " '▁drank': 17454,\n",
       " '▁cleric': 11306,\n",
       " '▁Social': 2702,\n",
       " '▁Solar': 12974,\n",
       " 'ABLE': 28593,\n",
       " '▁breath': 2713,\n",
       " '▁Guggenheim': 29578,\n",
       " '▁Bernie': 23998,\n",
       " 'morbid': 27291,\n",
       " '▁elegance': 23133,\n",
       " '▁messaging': 19534,\n",
       " '▁Hodge': 23560,\n",
       " '▁avoided': 8759,\n",
       " 'Wiki': 15966,\n",
       " '▁Soldiers': 20037,\n",
       " 'symmetry': 25504,\n",
       " 'ena': 4347,\n",
       " 'GRA': 18221,\n",
       " 'iated': 19285,\n",
       " '▁Railroad': 11841,\n",
       " '▁theologian': 25617,\n",
       " 'CHA': 19909,\n",
       " '▁inflatable': 31384,\n",
       " '▁portfolio': 5858,\n",
       " '▁Gur': 14124,\n",
       " '▁Dave': 5432,\n",
       " '▁qualities': 10855,\n",
       " '▁occupational': 22708,\n",
       " 'signature': 23306,\n",
       " 'logger': 22424,\n",
       " '▁Energ': 30058,\n",
       " '▁lawn': 10724,\n",
       " '▁installations': 15166,\n",
       " '▁Commission': 1587,\n",
       " '▁al': 1296,\n",
       " '▁Auction': 28640,\n",
       " '▁ferret': 31685,\n",
       " 'opening': 23419,\n",
       " 'saturated': 16379,\n",
       " 'bearing': 21153,\n",
       " 'law': 3044,\n",
       " '▁prosperity': 11089,\n",
       " 'Gene': 24507,\n",
       " '▁enlarge': 27661,\n",
       " '▁style': 1393,\n",
       " '▁accent': 9247,\n",
       " 'efficient': 14317,\n",
       " '▁Clarkson': 28531,\n",
       " 'thy': 9230,\n",
       " '▁social': 796,\n",
       " '▁holdings': 15514,\n",
       " 'ej': 17240,\n",
       " 'ol': 1529,\n",
       " '▁disposed': 24913,\n",
       " '▁John': 408,\n",
       " '▁monetary': 8545,\n",
       " '▁keeps': 5788,\n",
       " '▁Overnight': 29021,\n",
       " '▁quiet': 3215,\n",
       " '▁concern': 1911,\n",
       " '▁Rice': 5669,\n",
       " '▁enjoyable': 14896,\n",
       " '▁hotel': 1645,\n",
       " '▁Stamp': 21975,\n",
       " '▁Lam': 15708,\n",
       " '99': 2692,\n",
       " '▁Estonia': 14581,\n",
       " 'hopping': 24099,\n",
       " '▁Atlantic': 4088,\n",
       " '▁ports': 9481,\n",
       " 'Dirty': 27227,\n",
       " '▁gallop': 30297,\n",
       " '▁Sicily': 23106,\n",
       " '▁32': 2421,\n",
       " '▁bounded': 21522,\n",
       " '▁provisions': 6025,\n",
       " '▁censorship': 22069,\n",
       " '▁deepening': 23527,\n",
       " '▁Baden': 25211,\n",
       " '▁Bush': 635,\n",
       " '▁finger': 4586,\n",
       " '▁reliably': 28716,\n",
       " '▁granddaughter': 21302,\n",
       " '▁bear': 4151,\n",
       " 'BA': 5187,\n",
       " '▁Paramount': 19216,\n",
       " 'natal': 19487,\n",
       " '▁mule': 28227,\n",
       " '▁danced': 18125,\n",
       " '▁retort': 26515,\n",
       " 'Y': 936,\n",
       " '▁forecast': 4757,\n",
       " '▁valuation': 20748,\n",
       " '▁territorial': 11660,\n",
       " '▁Maureen': 28597,\n",
       " '▁vocals': 11440,\n",
       " '▁chancellor': 18047,\n",
       " '▁comfort': 4082,\n",
       " 'faction': 26053,\n",
       " '▁repurchase': 31119,\n",
       " '▁take': 182,\n",
       " '▁cooperate': 7702,\n",
       " '▁calves': 30189,\n",
       " '▁Rock': 3066,\n",
       " 'EHR': 20814,\n",
       " 'cro': 4736,\n",
       " 'fasci': 23868,\n",
       " '▁Loop': 23791,\n",
       " '▁visibility': 14447,\n",
       " '▁insured': 17371,\n",
       " '▁writ': 25037,\n",
       " '▁Justice': 2408,\n",
       " '▁unnatural': 26971,\n",
       " '▁pounded': 19802,\n",
       " 'sman': 14792,\n",
       " '▁nails': 16438,\n",
       " '▁Air': 1392,\n",
       " '▁abstract': 9952,\n",
       " '▁minor': 2835,\n",
       " '▁whilst': 8532,\n",
       " 'LD': 9789,\n",
       " '▁redeem': 18071,\n",
       " 'AMA': 20858,\n",
       " 'bio': 19546,\n",
       " '▁Gaz': 21578,\n",
       " 'ppa': 22226,\n",
       " '▁Sept': 2851,\n",
       " '▁Sicilian': 31595,\n",
       " 'ICE': 9887,\n",
       " '▁pit': 6062,\n",
       " '▁syntax': 24104,\n",
       " 'Pass': 24928,\n",
       " '▁BEIJING': 30607,\n",
       " '▁pouch': 23404,\n",
       " '▁violently': 21610,\n",
       " '▁enjoys': 14089,\n",
       " '▁NEWS': 22041,\n",
       " '▁spoiled': 25368,\n",
       " '▁regulators': 9739,\n",
       " '▁Spitzer': 25142,\n",
       " '▁steered': 27007,\n",
       " '▁BHP': 28837,\n",
       " '▁summoned': 15147,\n",
       " '▁seat': 1876,\n",
       " '▁rugby': 7902,\n",
       " '▁Devin': 29912,\n",
       " '▁City': 510,\n",
       " '▁hole': 3871,\n",
       " '▁Reality': 27024,\n",
       " '▁Wenger': 25391,\n",
       " '▁Aurora': 24085,\n",
       " '▁barbed': 28762,\n",
       " '*': 8652,\n",
       " 'Take': 14825,\n",
       " 'owski': 13034,\n",
       " 'rem': 14729,\n",
       " '▁nominate': 24681,\n",
       " '▁1948': 6679,\n",
       " '▁Churches': 21661,\n",
       " '▁Built': 17208,\n",
       " '▁commercially': 16918,\n",
       " '▁Hiroshima': 26292,\n",
       " '▁favorites': 15187,\n",
       " '▁Tata': 21197,\n",
       " '▁flavors': 19006,\n",
       " '▁Lafayette': 24836,\n",
       " 'plex': 13322,\n",
       " '▁Kan': 5286,\n",
       " 'corp': 13325,\n",
       " 'played': 13273,\n",
       " '▁money': 356,\n",
       " '▁Gomez': 19183,\n",
       " '▁internet': 2476,\n",
       " '▁Packers': 21394,\n",
       " '▁faded': 12859,\n",
       " 'Photo': 31234,\n",
       " '▁resignation': 6252,\n",
       " '▁Independence': 11822,\n",
       " '▁Supporters': 24024,\n",
       " '▁row': 3559,\n",
       " '▁share': 763,\n",
       " '▁Sadly': 25514,\n",
       " '▁111': 16617,\n",
       " '▁Carbon': 17906,\n",
       " '▁parking': 4169,\n",
       " '▁optimized': 24557,\n",
       " '▁duke': 25950,\n",
       " '▁repatriation': 27396,\n",
       " '▁Slobodan': 18054,\n",
       " '▁Cardenas': 31908,\n",
       " '▁1919': 9955,\n",
       " '▁Kab': 17439,\n",
       " '▁Hang': 9658,\n",
       " '▁Christie': 15792,\n",
       " '▁native': 2630,\n",
       " '▁many': 142,\n",
       " '▁Chinese': 693,\n",
       " '▁parliament': 2230,\n",
       " '▁railing': 23768,\n",
       " '▁NGO': 16678,\n",
       " '▁prosperous': 17831,\n",
       " '▁philanthropic': 30502,\n",
       " '▁Medi': 23790,\n",
       " '▁quasi': 24111,\n",
       " 'house': 2091,\n",
       " 'Positive': 27977,\n",
       " '▁assistance': 2489,\n",
       " '▁grave': 6816,\n",
       " '▁prescribe': 23489,\n",
       " '▁juror': 30699,\n",
       " '▁Hearing': 24091,\n",
       " '▁microphone': 15043,\n",
       " '▁soon': 802,\n",
       " '▁overshadowed': 22855,\n",
       " 'kuh': 20507,\n",
       " '▁Burkina': 26461,\n",
       " 'commissioned': 29631,\n",
       " '▁Viking': 23577,\n",
       " '▁Fey': 27983,\n",
       " '▁Ohio': 2984,\n",
       " '▁Exhibition': 17937,\n",
       " '▁presumption': 31548,\n",
       " '▁Hotel': 3669,\n",
       " 'arizona': 23626,\n",
       " '▁desktop': 8922,\n",
       " '▁Are': 3129,\n",
       " '▁widen': 23441,\n",
       " '▁Leipzig': 26383,\n",
       " '▁SSR': 29775,\n",
       " ')': 11,\n",
       " 'angi': 21448,\n",
       " 'oxi': 14094,\n",
       " 'ra': 840,\n",
       " '▁raised': 1390,\n",
       " '▁create': 850,\n",
       " '▁Where': 4212,\n",
       " '▁Attendance': 30741,\n",
       " '▁Ireland': 2215,\n",
       " '▁Suite': 14619,\n",
       " '▁stamped': 19593,\n",
       " '▁cups': 11344,\n",
       " '▁originally': 2339,\n",
       " '▁patches': 15820,\n",
       " '▁scrimmage': 31843,\n",
       " '▁Ferrero': 29569,\n",
       " '▁frontier': 13380,\n",
       " '▁preside': 27093,\n",
       " '▁slept': 11889,\n",
       " 'claim': 13635,\n",
       " '▁inspire': 12004,\n",
       " '▁Zedillo': 29089,\n",
       " '▁Garnett': 31413,\n",
       " '▁Te': 2710,\n",
       " '▁Rick': 5598,\n",
       " '▁Elise': 27326,\n",
       " '▁Bath': 14366,\n",
       " '▁Prefecture': 21997,\n",
       " '▁Yitzhak': 25419,\n",
       " 'shin': 13323,\n",
       " '▁colourful': 28002,\n",
       " 'FTA': 19738,\n",
       " 'Does': 20058,\n",
       " '▁Ranking': 26473,\n",
       " '▁arrogance': 26829,\n",
       " '▁obtaining': 10649,\n",
       " '▁clipping': 28271,\n",
       " '▁nascent': 30432,\n",
       " '▁barrels': 7404,\n",
       " 'ego': 12608,\n",
       " '▁school': 297,\n",
       " 'SO': 9023,\n",
       " 'iano': 12481,\n",
       " 'chy': 8358,\n",
       " 'tel': 4258,\n",
       " '▁examined': 8410,\n",
       " 'Ge': 16117,\n",
       " '▁verify': 9948,\n",
       " 'cid': 9008,\n",
       " '▁Heritage': 8790,\n",
       " '▁spur': 9360,\n",
       " '▁burned': 5488,\n",
       " '▁exchanged': 11812,\n",
       " 'Chicago': 23203,\n",
       " '▁sociology': 23346,\n",
       " '▁Bottle': 30806,\n",
       " '▁Made': 9122,\n",
       " 'haz': 9565,\n",
       " '▁Innovation': 19551,\n",
       " '▁Elliot': 22060,\n",
       " 'flavored': 27630,\n",
       " '▁GmbH': 31876,\n",
       " '▁glowing': 16926,\n",
       " '▁hormone': 13536,\n",
       " '▁Progressive': 14341,\n",
       " '▁revoked': 24172,\n",
       " '▁Latino': 9408,\n",
       " '▁toxic': 9071,\n",
       " '▁articles': 2883,\n",
       " '▁Belt': 14490,\n",
       " '▁Olympia': 17649,\n",
       " '▁timeline': 18233,\n",
       " '▁expedite': 26036,\n",
       " '▁Reaper': 30621,\n",
       " '▁Scarborough': 31496,\n",
       " '▁noon': 10269,\n",
       " '▁without': 286,\n",
       " '▁Ugandan': 18710,\n",
       " '▁hair': 1162,\n",
       " '▁youngsters': 17780,\n",
       " '▁feathers': 18570,\n",
       " '▁Constantinople': 25894,\n",
       " '▁endeavor': 13827,\n",
       " 'text': 10295,\n",
       " '▁Chan': 6716,\n",
       " '▁Best': 2745,\n",
       " '▁backyard': 15975,\n",
       " 'shirt': 8100,\n",
       " '▁raged': 25753,\n",
       " 'park': 12776,\n",
       " '▁sanitation': 22893,\n",
       " 'ogenesis': 29252,\n",
       " '▁Bjork': 30217,\n",
       " 'Pacific': 11639,\n",
       " '▁Thomson': 15199,\n",
       " '▁retire': 8872,\n",
       " '▁folded': 11928,\n",
       " '▁revealing': 11646,\n",
       " '▁Yak': 20020,\n",
       " '▁Sanchez': 11888,\n",
       " '▁Manitoba': 20339,\n",
       " '▁leagues': 13242,\n",
       " '▁harmful': 11530,\n",
       " '▁enabled': 8050,\n",
       " 'bell': 9566,\n",
       " 'NE': 7635,\n",
       " '▁pediatrician': 29803,\n",
       " 'benefit': 30632,\n",
       " '▁Stevenson': 22483,\n",
       " 'weight': 7079,\n",
       " '▁Peninsula': 9779,\n",
       " '▁contracting': 21305,\n",
       " '▁impartial': 24162,\n",
       " '▁statehood': 27005,\n",
       " '▁Hole': 22613,\n",
       " '▁Parallel': 31266,\n",
       " '▁Nickelodeon': 30876,\n",
       " 'real': 5550,\n",
       " 'zel': 9986,\n",
       " '▁adult': 3639,\n",
       " '▁sugar': 3857,\n",
       " '▁Presidential': 13319,\n",
       " 'CD': 7882,\n",
       " '▁Blank': 23678,\n",
       " 'TAL': 17682,\n",
       " '▁groom': 13340,\n",
       " 'Iraq': 16671,\n",
       " '▁2010,': 3745,\n",
       " '▁retention': 17481,\n",
       " '▁contests': 14684,\n",
       " '▁marketers': 21412,\n",
       " '▁whip': 14762,\n",
       " '▁Over': 2219,\n",
       " '▁viol': 20810,\n",
       " '▁Ethiopia': 10330,\n",
       " 'Top': 19950,\n",
       " 'lick': 8254,\n",
       " '▁tried': 1000,\n",
       " '▁fool': 8956,\n",
       " '45': 2967,\n",
       " '▁conserve': 19592,\n",
       " 'emptive': 27037,\n",
       " '▁Millennium': 18172,\n",
       " '▁nn': 16246,\n",
       " '▁busy': 3919,\n",
       " 'wisconsin': 29179,\n",
       " '▁Holden': 20400,\n",
       " '▁Naga': 23356,\n",
       " 'Network': 29980,\n",
       " '▁breeding': 10454,\n",
       " '▁Perhaps': 4979,\n",
       " 'construction': 29198,\n",
       " '▁ablaze': 29375,\n",
       " 'pid': 12470,\n",
       " '▁horse': 2958,\n",
       " 'should': 16414,\n",
       " 'gio': 12895,\n",
       " 'eater': 16901,\n",
       " '▁Ceylon': 30837,\n",
       " '▁Cornish': 30745,\n",
       " '▁firearms': 17046,\n",
       " '▁choreographer': 26108,\n",
       " '▁replacement': 4200,\n",
       " '▁recreate': 24513,\n",
       " 'rov': 11019,\n",
       " '▁Rodham': 19835,\n",
       " '▁Ber': 4302,\n",
       " 'Plus': 28528,\n",
       " 'rz': 15719,\n",
       " '▁Prince': 3526,\n",
       " '▁Taiwanese': 12341,\n",
       " '▁impacted': 21961,\n",
       " 'escu': 16519,\n",
       " '▁patients': 1569,\n",
       " 'tero': 15170,\n",
       " 'NYT': 20989,\n",
       " '▁jerk': 15034,\n",
       " '▁opaque': 28874,\n",
       " '▁Clo': 10857,\n",
       " '▁Contact': 7421,\n",
       " 'graf': 14781,\n",
       " 'nap': 12619,\n",
       " 'llo': 10221,\n",
       " '▁perception': 9974,\n",
       " '▁fiddle': 24871,\n",
       " '▁Thanksgiving': 13478,\n",
       " '▁Torres': 18654,\n",
       " '▁humming': 26642,\n",
       " '▁misguided': 29840,\n",
       " '▁waste': 3419,\n",
       " '▁dislike': 15976,\n",
       " '▁wicket': 12827,\n",
       " '▁chaotic': 20075,\n",
       " '▁middle': 1538,\n",
       " '▁Liverpool': 6851,\n",
       " '▁preschool': 23762,\n",
       " '▁dominated': 5954,\n",
       " '▁Mathematical': 30473,\n",
       " '▁connected': 3428,\n",
       " '▁subversive': 31757,\n",
       " '▁billionaire': 19095,\n",
       " '▁Stockton': 27275,\n",
       " '▁progressed': 18705,\n",
       " '▁ballistic': 20503,\n",
       " '▁golf': 3661,\n",
       " 'urge': 16567,\n",
       " '▁hypertension': 25267,\n",
       " '265': 27211,\n",
       " 'lugged': 29873,\n",
       " '▁because': 149,\n",
       " '▁slack': 17407,\n",
       " '▁candy': 13592,\n",
       " 'bil': 9810,\n",
       " 'idge': 23683,\n",
       " '▁Navajo': 28177,\n",
       " '▁electronically': 19677,\n",
       " '▁Lehman': 13844,\n",
       " '▁assortment': 21953,\n",
       " '▁prowess': 28511,\n",
       " '▁sequence': 6173,\n",
       " '▁implement': 4790,\n",
       " 'tard': 16188,\n",
       " '▁Torre': 17008,\n",
       " '▁cancel': 8468,\n",
       " '▁global': 1150,\n",
       " '▁SEC': 9358,\n",
       " '▁derive': 23235,\n",
       " '▁Korn': 28574,\n",
       " '▁Ban': 3481,\n",
       " '▁invalid': 15032,\n",
       " '▁revision': 10049,\n",
       " '▁endangered': 12133,\n",
       " 'corona': 29316,\n",
       " '▁Like': 2386,\n",
       " 'KING': 25017,\n",
       " '▁precedent': 15825,\n",
       " '▁Connie': 25435,\n",
       " '▁organizers': 10058,\n",
       " '▁lifted': 4116,\n",
       " 'containing': 30932,\n",
       " 'dies': 10647,\n",
       " '▁badminton': 29449,\n",
       " '▁Muh': 25201,\n",
       " '▁Jolie': 30565,\n",
       " '888': 11628,\n",
       " '▁System': 3656,\n",
       " '▁asthma': 14917,\n",
       " '▁exquisite': 19777,\n",
       " '▁Devon': 13202,\n",
       " '▁exactly': 2034,\n",
       " '▁Lea': 13937,\n",
       " '▁indispensable': 26435,\n",
       " '▁Bird': 8381,\n",
       " '▁biologist': 19780,\n",
       " 'HL': 9692,\n",
       " 'meth': 21446,\n",
       " '▁resemblance': 22656,\n",
       " '▁definitely': 3382,\n",
       " '▁Vale': 14102,\n",
       " '▁Speedway': 17871,\n",
       " 'lines': 11142,\n",
       " '▁fields': 3001,\n",
       " '▁popularity': 5469,\n",
       " '▁limitation': 16476,\n",
       " '▁ecstasy': 30013,\n",
       " '▁force': 779,\n",
       " '▁fiscal': 4186,\n",
       " '▁Boz': 25821,\n",
       " 'ef': 4631,\n",
       " '▁SUV': 14891,\n",
       " '▁Juda': 27726,\n",
       " 'Direct': 23769,\n",
       " '▁officials': 361,\n",
       " 'Q': 3048,\n",
       " '▁van': 3887,\n",
       " '▁Lime': 27659,\n",
       " '▁flared': 20978,\n",
       " 'ivist': 23208,\n",
       " '▁Wal': 5325,\n",
       " '▁Recommendation': 31126,\n",
       " '▁Janata': 30543,\n",
       " '▁Cheng': 17603,\n",
       " 'ica': 4969,\n",
       " 'pokker': 20331,\n",
       " '▁Apostle': 24911,\n",
       " '▁shoulder': 2741,\n",
       " '▁profits': 4102,\n",
       " '▁Ram': 5337,\n",
       " 'Le': 3780,\n",
       " '▁variations': 10595,\n",
       " '▁clearance': 13907,\n",
       " '▁amount': 1065,\n",
       " '▁broadly': 16026,\n",
       " '▁Satellite': 12783,\n",
       " '▁separatists': 19605,\n",
       " 'defunct': 20905,\n",
       " '▁Lux': 21139,\n",
       " '▁Chat': 16969,\n",
       " '▁herbs': 14520,\n",
       " ':40': 23217,\n",
       " '▁Dominique': 23504,\n",
       " '▁Monde': 25890,\n",
       " '▁snarl': 27483,\n",
       " '▁Thirteen': 24420,\n",
       " '▁ramp': 11402,\n",
       " '▁Ce': 6590,\n",
       " '▁Conditions': 20014,\n",
       " '▁emeritus': 29447,\n",
       " '▁allegiance': 21829,\n",
       " 'gged': 11988,\n",
       " '▁HMO': 25953,\n",
       " '▁direct': 1568,\n",
       " '▁suburbs': 11726,\n",
       " 'MMA': 26478,\n",
       " '▁promote': 2573,\n",
       " '▁permanent': 3243,\n",
       " '▁requiring': 7171,\n",
       " '▁Basin': 15482,\n",
       " '▁Tourist': 24704,\n",
       " '▁patch': 8063,\n",
       " '▁Nuremberg': 27794,\n",
       " '▁Bus': 6573,\n",
       " '▁ascertain': 24626,\n",
       " '▁consider': 1524,\n",
       " '▁coached': 13069,\n",
       " '▁600': 6865,\n",
       " '▁hallway': 11036,\n",
       " 'enko': 10655,\n",
       " '▁capped': 16895,\n",
       " '▁Kro': 16552,\n",
       " '▁acids': 15624,\n",
       " '▁Khmer': 18405,\n",
       " '▁turkey': 14550,\n",
       " 'ologies': 19859,\n",
       " '226': 26355,\n",
       " '▁Meridian': 31495,\n",
       " 'Modern': 31101,\n",
       " '▁fray': 22818,\n",
       " '▁surely': 8317,\n",
       " 'less': 832,\n",
       " '▁curved': 17262,\n",
       " '▁Battery': 16844,\n",
       " '▁measure': 2310,\n",
       " '▁woodland': 19554,\n",
       " '▁Stu': 19667,\n",
       " '▁necklace': 15377,\n",
       " '▁skipped': 22171,\n",
       " '▁Domino': 31617,\n",
       " '▁PENNSYLVANIA': 22672,\n",
       " '▁Turnover': 26683,\n",
       " 'upon': 14127,\n",
       " '▁code': 1713,\n",
       " '▁novelist': 16153,\n",
       " 'protein': 24765,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab # 영어로 학습된 모델, 토큰 : 토큰 번호\n",
    "# vocab.items, 토큰과 토큰 번호를 짝을 지어줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9424: 'tail',\n",
       " 18425: 'valu',\n",
       " 23636: '▁quarterfinal',\n",
       " 11410: '▁carved',\n",
       " 5738: '▁Tre',\n",
       " 20041: '▁outlines',\n",
       " 28555: '▁populace',\n",
       " 2525: '▁settlement',\n",
       " 2801: '▁truck',\n",
       " 21057: '▁Apr',\n",
       " 9353: '▁Barr',\n",
       " 9035: 'utter',\n",
       " 22758: '▁Consulting',\n",
       " 17704: 'bli',\n",
       " 21263: '▁Jesuit',\n",
       " 23729: '▁Infrastructure',\n",
       " 26990: '▁boomers',\n",
       " 27120: '▁brewery',\n",
       " 13947: '▁Lim',\n",
       " 6504: '▁Look',\n",
       " 10685: '▁separatist',\n",
       " 176: '▁even',\n",
       " 14561: '▁certificates',\n",
       " 4688: '▁clothing',\n",
       " 39: '▁be',\n",
       " 14838: '▁stunt',\n",
       " 15052: 'Americans',\n",
       " 17239: '▁Secondary',\n",
       " 19464: 'goal',\n",
       " 10918: '▁Stefan',\n",
       " 21073: '▁competence',\n",
       " 24248: '▁resembling',\n",
       " 26566: '▁ambient',\n",
       " 25976: 'negative',\n",
       " 10543: '▁Bart',\n",
       " 14333: '▁refrain',\n",
       " 9690: '▁Tele',\n",
       " 10270: '▁magnitude',\n",
       " 10787: '▁ousted',\n",
       " 23653: '▁decorating',\n",
       " 18265: 'equity',\n",
       " 29339: 'concern',\n",
       " 30700: '▁EDI',\n",
       " 30908: '▁jurist',\n",
       " 2712: '▁excellent',\n",
       " 16354: 'cran',\n",
       " 8006: '▁reject',\n",
       " 2261: 'ker',\n",
       " 792: '▁key',\n",
       " 27749: '▁Avalanche',\n",
       " 29072: '▁Roof',\n",
       " 16458: '▁feminist',\n",
       " 16856: '▁wildly',\n",
       " 26293: '▁Fabio',\n",
       " 5659: '▁directory',\n",
       " 3268: '▁pace',\n",
       " 1544: '▁winning',\n",
       " 6801: '▁NCAA',\n",
       " 9638: '▁confrontation',\n",
       " 25158: '▁adv',\n",
       " 19139: '▁bald',\n",
       " 19584: '▁Sources',\n",
       " 26400: '▁hover',\n",
       " 21205: 'focused',\n",
       " 31391: '▁paranormal',\n",
       " 9729: '▁Ask',\n",
       " 26415: '▁Cottage',\n",
       " 1058: '▁software',\n",
       " 21616: '▁wallpaper',\n",
       " 5448: '▁execution',\n",
       " 16226: '▁rubble',\n",
       " 15501: '▁telecom',\n",
       " 783: '▁leaders',\n",
       " 11930: 'hart',\n",
       " 12772: '▁modules',\n",
       " 30133: '▁cohesive',\n",
       " 18739: '▁Brig',\n",
       " 30223: '▁Camille',\n",
       " 31784: 'Aryan',\n",
       " 19271: '▁cheerful',\n",
       " 8076: '▁donated',\n",
       " 2816: 'wing',\n",
       " 793: '▁players',\n",
       " 2824: '▁plus',\n",
       " 13287: '▁pretend',\n",
       " 2782: '▁Life',\n",
       " 5748: '▁Denver',\n",
       " 17838: '▁coupons',\n",
       " 16305: '▁saint',\n",
       " 17495: '▁Naples',\n",
       " 18573: '▁Kabila',\n",
       " 8215: 'hur',\n",
       " 22739: '▁Gage',\n",
       " 26093: '▁Rowe',\n",
       " 7097: '▁notion',\n",
       " 924: '▁White',\n",
       " 29691: '▁Avalon',\n",
       " 16951: '▁Sprint',\n",
       " 20116: 'India',\n",
       " 6024: '▁admit',\n",
       " 21571: '▁Bald',\n",
       " 30581: '▁sipping',\n",
       " 4997: '▁assembly',\n",
       " 22889: 'hammer',\n",
       " 28605: '▁Shang',\n",
       " 17189: '▁colonel',\n",
       " 20847: '▁Vera',\n",
       " 10656: 'kha',\n",
       " 4987: '▁domain',\n",
       " 12670: '▁Kenyan',\n",
       " 14845: '▁Immigration',\n",
       " 1646: '▁east',\n",
       " 9643: '▁assisted',\n",
       " 18253: 'Note',\n",
       " 31444: '▁Olympus',\n",
       " 14219: 'saving',\n",
       " 705: '▁issues',\n",
       " 4567: '▁defend',\n",
       " 14415: '▁fabulous',\n",
       " 13175: '▁Making',\n",
       " 5005: 'pic',\n",
       " 23709: '▁waived',\n",
       " 12849: 'lem',\n",
       " 6223: '▁array',\n",
       " 16409: '30,000',\n",
       " 5438: '▁tail',\n",
       " 5709: '▁Garden',\n",
       " 4633: '▁bonus',\n",
       " 15213: 'oker',\n",
       " 8298: '▁artificial',\n",
       " 15494: '▁volleyball',\n",
       " 24731: 'Mari',\n",
       " 4785: 'bound',\n",
       " 11792: '▁knot',\n",
       " 12905: '▁municipalities',\n",
       " 4484: '▁repeated',\n",
       " 31416: '▁mellow',\n",
       " 18503: '▁Officers',\n",
       " 11711: '▁whistle',\n",
       " 31959: '▁Equatorial',\n",
       " 17058: '▁biblical',\n",
       " 9783: '▁Len',\n",
       " 2989: '▁manner',\n",
       " 20611: '▁slab',\n",
       " 2229: '▁partner',\n",
       " 4365: 'ile',\n",
       " 14269: '▁fountain',\n",
       " 20607: '▁Pepsi',\n",
       " 23924: 'File',\n",
       " 9908: 'vil',\n",
       " 2294: 'ci',\n",
       " 30732: '▁Purchasing',\n",
       " 20163: '▁filmmakers',\n",
       " 2812: '23',\n",
       " 1387: '▁Con',\n",
       " 28494: '▁horde',\n",
       " 9708: '▁progressive',\n",
       " 20510: '▁hostilities',\n",
       " 26679: '▁bikini',\n",
       " 6909: 'ces',\n",
       " 27987: '▁Labrador',\n",
       " 26707: 'trophic',\n",
       " 3890: 'bra',\n",
       " 11669: 'para',\n",
       " 23326: '▁Fujimori',\n",
       " 12742: 'rator',\n",
       " 16419: '▁relying',\n",
       " 20153: '▁mythology',\n",
       " 25735: '▁looted',\n",
       " 12477: 'dicate',\n",
       " 92: '▁time',\n",
       " 10960: '▁strive',\n",
       " 638: '▁financial',\n",
       " 4242: '▁absolutely',\n",
       " 19756: '▁Lot',\n",
       " 19: ',',\n",
       " 9024: '▁robust',\n",
       " 16878: '▁electorate',\n",
       " 1214: '▁species',\n",
       " 14743: '▁mapping',\n",
       " 20653: '▁Cin',\n",
       " 25184: 'project',\n",
       " 25587: '▁Pfizer',\n",
       " 13597: '▁leftist',\n",
       " 12269: 'LM',\n",
       " 16739: '▁Owner',\n",
       " 21246: 'View',\n",
       " 10809: '▁workout',\n",
       " 28865: '▁Councillor',\n",
       " 12736: '▁Rail',\n",
       " 12255: '▁Ethan',\n",
       " 30095: '▁osteoporosis',\n",
       " 1641: '▁Just',\n",
       " 5184: '81',\n",
       " 6041: '▁cheese',\n",
       " 20582: 'tower',\n",
       " 15129: 'minating',\n",
       " 16728: '▁courthouse',\n",
       " 1672: '▁Thomas',\n",
       " 17007: 'nay',\n",
       " 27389: '▁Boxer',\n",
       " 11010: '▁accusing',\n",
       " 25598: '▁Secondly',\n",
       " 18808: '▁Hour',\n",
       " 7015: '▁purchasing',\n",
       " 18108: '▁semiconductor',\n",
       " 8614: '▁reward',\n",
       " 18095: '▁dealership',\n",
       " 22132: 'akov',\n",
       " 1764: '▁street',\n",
       " 4004: '▁arrival',\n",
       " 23047: '▁Yushchenko',\n",
       " 15520: '▁definitive',\n",
       " 5454: '▁awareness',\n",
       " 17931: '▁Reference',\n",
       " 19354: '▁prosecuted',\n",
       " 23256: '▁Balance',\n",
       " 25150: '▁Angus',\n",
       " 9453: '▁corrupt',\n",
       " 31059: '▁Arbitration',\n",
       " 22852: '▁tuna',\n",
       " 4723: '72',\n",
       " 7405: '▁arguments',\n",
       " 9671: 'script',\n",
       " 19253: '▁Marcos',\n",
       " 20253: '▁primaries',\n",
       " 8376: '▁sir',\n",
       " 31062: '▁Exclusive',\n",
       " 5780: '$',\n",
       " 29011: '▁Radovan',\n",
       " 6871: '▁rely',\n",
       " 7516: 'quality',\n",
       " 17452: '▁Sherman',\n",
       " 5095: '▁alert',\n",
       " 31477: '▁Greyhound',\n",
       " 28669: '▁parity',\n",
       " 31563: '▁Izetbegovic',\n",
       " 23772: '▁urgently',\n",
       " 19058: '▁Kind',\n",
       " 30651: 'Earl',\n",
       " 21074: '▁1877',\n",
       " 21398: '▁barge',\n",
       " 26171: '▁expectancy',\n",
       " 10750: '▁Mall',\n",
       " 24478: '▁Qin',\n",
       " 7376: '▁relating',\n",
       " 3846: '▁tradition',\n",
       " 17751: '▁Fighting',\n",
       " 6400: '▁attracted',\n",
       " 28513: '▁Skype',\n",
       " 29090: '▁coroner',\n",
       " 21983: 'cheep',\n",
       " 29565: '▁bullshit',\n",
       " 21256: 'common',\n",
       " 4674: '▁shock',\n",
       " 16218: 'iate',\n",
       " 5971: '▁meets',\n",
       " 12060: '▁Algeria',\n",
       " 7122: '▁knife',\n",
       " 20571: '▁discarded',\n",
       " 13093: '▁WI',\n",
       " 10863: '▁Omar',\n",
       " 31355: '▁LinkedIn',\n",
       " 9555: '▁clip',\n",
       " 12639: '▁enrollment',\n",
       " 22763: '▁jug',\n",
       " 7597: '▁Arkansas',\n",
       " 22722: '▁Khatami',\n",
       " 30118: '▁Warehouse',\n",
       " 25725: '1982',\n",
       " 7761: '▁picking',\n",
       " 23920: '▁LPGA',\n",
       " 9411: '▁Ivan',\n",
       " 22150: '▁Forget',\n",
       " 26901: '▁Regulatory',\n",
       " 11872: '▁bark',\n",
       " 12721: '▁niche',\n",
       " 14096: 'RL',\n",
       " 31196: '▁perpetrated',\n",
       " 9570: '▁lo',\n",
       " 12029: '▁Mills',\n",
       " 4431: '▁Communist',\n",
       " 14866: '▁accredited',\n",
       " 20706: '▁Astros',\n",
       " 30355: '▁Helicopter',\n",
       " 15701: '▁initiate',\n",
       " 18997: '▁Hack',\n",
       " 3407: '▁fees',\n",
       " 15926: '▁uncommon',\n",
       " 9605: '▁Ice',\n",
       " 20086: '▁begging',\n",
       " 9914: 'quin',\n",
       " 6664: '▁$8',\n",
       " 18056: '▁Recreation',\n",
       " 31594: '▁indignation',\n",
       " 11798: '▁licenses',\n",
       " 3362: 'form',\n",
       " 6397: '▁250',\n",
       " 16428: 'bate',\n",
       " 30172: '▁extradite',\n",
       " 2246: '▁Muslim',\n",
       " 5916: 'Man',\n",
       " 8621: '▁abilities',\n",
       " 19265: '▁cockpit',\n",
       " 273: '▁family',\n",
       " 19945: 'touch',\n",
       " 25507: '▁Monarch',\n",
       " 21424: '▁disposition',\n",
       " 13181: '▁1988,',\n",
       " 13606: 'family',\n",
       " 4715: '▁Jason',\n",
       " 27652: '▁Gav',\n",
       " 14257: '▁conditioning',\n",
       " 27982: 'Cross',\n",
       " 1999: '▁Because',\n",
       " 1093: '▁Association',\n",
       " 11861: '▁marathon',\n",
       " 14463: 'itu',\n",
       " 24377: '▁Stafford',\n",
       " 25078: '▁$800',\n",
       " 6564: '▁gang',\n",
       " 23191: '▁Decor',\n",
       " 8833: 'RM',\n",
       " 30965: '▁marshes',\n",
       " 17258: '▁1972,',\n",
       " 9761: '▁ethics',\n",
       " 24728: 'TEN',\n",
       " 14021: '▁soared',\n",
       " 27340: '▁propped',\n",
       " 17650: '▁Tran',\n",
       " 22184: 'izo',\n",
       " 31814: '▁COXNET',\n",
       " 29095: '▁Duchy',\n",
       " 9524: '▁habit',\n",
       " 21348: '▁router',\n",
       " 23354: '▁clove',\n",
       " 10403: '▁edges',\n",
       " 112: '▁do',\n",
       " 25711: 'в',\n",
       " 15115: '▁XML',\n",
       " 19589: '▁messenger',\n",
       " 25055: 'Bob',\n",
       " 26655: 'Someone',\n",
       " 12462: '▁Barak',\n",
       " 18267: '▁fingerprint',\n",
       " 24462: 'image',\n",
       " 6807: '▁urge',\n",
       " 349: '▁says',\n",
       " 2875: '▁Steve',\n",
       " 9750: '▁conversations',\n",
       " 5918: '▁Strip',\n",
       " 26834: 'Breaking',\n",
       " 29952: '▁Healing',\n",
       " 8181: '▁Wind',\n",
       " 7647: 'pass',\n",
       " 19372: '▁Turin',\n",
       " 17125: '▁Dome',\n",
       " 10093: 'By',\n",
       " 13717: 'both',\n",
       " 19673: 'oca',\n",
       " 22131: '▁instituted',\n",
       " 5896: 'PA',\n",
       " 2887: '▁generation',\n",
       " 28644: 'RET',\n",
       " 17219: '▁steak',\n",
       " 5836: '▁electrical',\n",
       " 7501: '▁wealthy',\n",
       " 29851: '▁Branson',\n",
       " 18160: 'dependent',\n",
       " 2135: '▁Sen',\n",
       " 25790: '▁mapped',\n",
       " 20121: 'filtration',\n",
       " 10937: '▁refusal',\n",
       " 2301: '▁lawyer',\n",
       " 24727: '▁waging',\n",
       " 245: '▁another',\n",
       " 10956: '▁portable',\n",
       " 13365: '▁Shore',\n",
       " 9730: '▁Oliver',\n",
       " 2040: '▁speak',\n",
       " 31449: '▁Longhorns',\n",
       " 7928: '▁makers',\n",
       " 30070: '▁admirable',\n",
       " 14753: '▁tales',\n",
       " 15944: 'hander',\n",
       " 6497: '▁Oscar',\n",
       " 7400: '▁Edwards',\n",
       " 14566: '▁lane',\n",
       " 21884: '▁Vor',\n",
       " 13850: 'anza',\n",
       " 10819: '▁romance',\n",
       " 11864: '▁hug',\n",
       " 7261: '▁Again',\n",
       " 19410: '▁liaison',\n",
       " 16580: 'FB',\n",
       " 9825: 'ART',\n",
       " 17493: '▁peaks',\n",
       " 19170: '▁Paula',\n",
       " 652: '▁total',\n",
       " 3358: '▁ideal',\n",
       " 22625: 'igen',\n",
       " 26537: 'appropriate',\n",
       " 23486: '▁Remote',\n",
       " 19222: '▁hamlet',\n",
       " 3766: 'tain',\n",
       " 1928: '▁Tom',\n",
       " 4843: '▁BBC',\n",
       " 7604: 'lou',\n",
       " 3877: '▁strongly',\n",
       " 24284: '▁improperly',\n",
       " 9478: 'MORE',\n",
       " 2361: '▁opportunities',\n",
       " 12634: '▁Cruise',\n",
       " 14878: '▁assert',\n",
       " 4593: 'FL',\n",
       " 19475: 'ghi',\n",
       " 22313: 'SPA',\n",
       " 17454: '▁drank',\n",
       " 11306: '▁cleric',\n",
       " 2702: '▁Social',\n",
       " 12974: '▁Solar',\n",
       " 28593: 'ABLE',\n",
       " 2713: '▁breath',\n",
       " 29578: '▁Guggenheim',\n",
       " 23998: '▁Bernie',\n",
       " 27291: 'morbid',\n",
       " 23133: '▁elegance',\n",
       " 19534: '▁messaging',\n",
       " 23560: '▁Hodge',\n",
       " 8759: '▁avoided',\n",
       " 15966: 'Wiki',\n",
       " 20037: '▁Soldiers',\n",
       " 25504: 'symmetry',\n",
       " 4347: 'ena',\n",
       " 18221: 'GRA',\n",
       " 19285: 'iated',\n",
       " 11841: '▁Railroad',\n",
       " 25617: '▁theologian',\n",
       " 19909: 'CHA',\n",
       " 31384: '▁inflatable',\n",
       " 5858: '▁portfolio',\n",
       " 14124: '▁Gur',\n",
       " 5432: '▁Dave',\n",
       " 10855: '▁qualities',\n",
       " 22708: '▁occupational',\n",
       " 23306: 'signature',\n",
       " 22424: 'logger',\n",
       " 30058: '▁Energ',\n",
       " 10724: '▁lawn',\n",
       " 15166: '▁installations',\n",
       " 1587: '▁Commission',\n",
       " 1296: '▁al',\n",
       " 28640: '▁Auction',\n",
       " 31685: '▁ferret',\n",
       " 23419: 'opening',\n",
       " 16379: 'saturated',\n",
       " 21153: 'bearing',\n",
       " 3044: 'law',\n",
       " 11089: '▁prosperity',\n",
       " 24507: 'Gene',\n",
       " 27661: '▁enlarge',\n",
       " 1393: '▁style',\n",
       " 9247: '▁accent',\n",
       " 14317: 'efficient',\n",
       " 28531: '▁Clarkson',\n",
       " 9230: 'thy',\n",
       " 796: '▁social',\n",
       " 15514: '▁holdings',\n",
       " 17240: 'ej',\n",
       " 1529: 'ol',\n",
       " 24913: '▁disposed',\n",
       " 408: '▁John',\n",
       " 8545: '▁monetary',\n",
       " 5788: '▁keeps',\n",
       " 29021: '▁Overnight',\n",
       " 3215: '▁quiet',\n",
       " 1911: '▁concern',\n",
       " 5669: '▁Rice',\n",
       " 14896: '▁enjoyable',\n",
       " 1645: '▁hotel',\n",
       " 21975: '▁Stamp',\n",
       " 15708: '▁Lam',\n",
       " 2692: '99',\n",
       " 14581: '▁Estonia',\n",
       " 24099: 'hopping',\n",
       " 4088: '▁Atlantic',\n",
       " 9481: '▁ports',\n",
       " 27227: 'Dirty',\n",
       " 30297: '▁gallop',\n",
       " 23106: '▁Sicily',\n",
       " 2421: '▁32',\n",
       " 21522: '▁bounded',\n",
       " 6025: '▁provisions',\n",
       " 22069: '▁censorship',\n",
       " 23527: '▁deepening',\n",
       " 25211: '▁Baden',\n",
       " 635: '▁Bush',\n",
       " 4586: '▁finger',\n",
       " 28716: '▁reliably',\n",
       " 21302: '▁granddaughter',\n",
       " 4151: '▁bear',\n",
       " 5187: 'BA',\n",
       " 19216: '▁Paramount',\n",
       " 19487: 'natal',\n",
       " 28227: '▁mule',\n",
       " 18125: '▁danced',\n",
       " 26515: '▁retort',\n",
       " 936: 'Y',\n",
       " 4757: '▁forecast',\n",
       " 20748: '▁valuation',\n",
       " 11660: '▁territorial',\n",
       " 28597: '▁Maureen',\n",
       " 11440: '▁vocals',\n",
       " 18047: '▁chancellor',\n",
       " 4082: '▁comfort',\n",
       " 26053: 'faction',\n",
       " 31119: '▁repurchase',\n",
       " 182: '▁take',\n",
       " 7702: '▁cooperate',\n",
       " 30189: '▁calves',\n",
       " 3066: '▁Rock',\n",
       " 20814: 'EHR',\n",
       " 4736: 'cro',\n",
       " 23868: 'fasci',\n",
       " 23791: '▁Loop',\n",
       " 14447: '▁visibility',\n",
       " 17371: '▁insured',\n",
       " 25037: '▁writ',\n",
       " 2408: '▁Justice',\n",
       " 26971: '▁unnatural',\n",
       " 19802: '▁pounded',\n",
       " 14792: 'sman',\n",
       " 16438: '▁nails',\n",
       " 1392: '▁Air',\n",
       " 9952: '▁abstract',\n",
       " 2835: '▁minor',\n",
       " 8532: '▁whilst',\n",
       " 9789: 'LD',\n",
       " 18071: '▁redeem',\n",
       " 20858: 'AMA',\n",
       " 19546: 'bio',\n",
       " 21578: '▁Gaz',\n",
       " 22226: 'ppa',\n",
       " 2851: '▁Sept',\n",
       " 31595: '▁Sicilian',\n",
       " 9887: 'ICE',\n",
       " 6062: '▁pit',\n",
       " 24104: '▁syntax',\n",
       " 24928: 'Pass',\n",
       " 30607: '▁BEIJING',\n",
       " 23404: '▁pouch',\n",
       " 21610: '▁violently',\n",
       " 14089: '▁enjoys',\n",
       " 22041: '▁NEWS',\n",
       " 25368: '▁spoiled',\n",
       " 9739: '▁regulators',\n",
       " 25142: '▁Spitzer',\n",
       " 27007: '▁steered',\n",
       " 28837: '▁BHP',\n",
       " 15147: '▁summoned',\n",
       " 1876: '▁seat',\n",
       " 7902: '▁rugby',\n",
       " 29912: '▁Devin',\n",
       " 510: '▁City',\n",
       " 3871: '▁hole',\n",
       " 27024: '▁Reality',\n",
       " 25391: '▁Wenger',\n",
       " 24085: '▁Aurora',\n",
       " 28762: '▁barbed',\n",
       " 8652: '*',\n",
       " 14825: 'Take',\n",
       " 13034: 'owski',\n",
       " 14729: 'rem',\n",
       " 24681: '▁nominate',\n",
       " 6679: '▁1948',\n",
       " 21661: '▁Churches',\n",
       " 17208: '▁Built',\n",
       " 16918: '▁commercially',\n",
       " 26292: '▁Hiroshima',\n",
       " 15187: '▁favorites',\n",
       " 21197: '▁Tata',\n",
       " 19006: '▁flavors',\n",
       " 24836: '▁Lafayette',\n",
       " 13322: 'plex',\n",
       " 5286: '▁Kan',\n",
       " 13325: 'corp',\n",
       " 13273: 'played',\n",
       " 356: '▁money',\n",
       " 19183: '▁Gomez',\n",
       " 2476: '▁internet',\n",
       " 21394: '▁Packers',\n",
       " 12859: '▁faded',\n",
       " 31234: 'Photo',\n",
       " 6252: '▁resignation',\n",
       " 11822: '▁Independence',\n",
       " 24024: '▁Supporters',\n",
       " 3559: '▁row',\n",
       " 763: '▁share',\n",
       " 25514: '▁Sadly',\n",
       " 16617: '▁111',\n",
       " 17906: '▁Carbon',\n",
       " 4169: '▁parking',\n",
       " 24557: '▁optimized',\n",
       " 25950: '▁duke',\n",
       " 27396: '▁repatriation',\n",
       " 18054: '▁Slobodan',\n",
       " 31908: '▁Cardenas',\n",
       " 9955: '▁1919',\n",
       " 17439: '▁Kab',\n",
       " 9658: '▁Hang',\n",
       " 15792: '▁Christie',\n",
       " 2630: '▁native',\n",
       " 142: '▁many',\n",
       " 693: '▁Chinese',\n",
       " 2230: '▁parliament',\n",
       " 23768: '▁railing',\n",
       " 16678: '▁NGO',\n",
       " 17831: '▁prosperous',\n",
       " 30502: '▁philanthropic',\n",
       " 23790: '▁Medi',\n",
       " 24111: '▁quasi',\n",
       " 2091: 'house',\n",
       " 27977: 'Positive',\n",
       " 2489: '▁assistance',\n",
       " 6816: '▁grave',\n",
       " 23489: '▁prescribe',\n",
       " 30699: '▁juror',\n",
       " 24091: '▁Hearing',\n",
       " 15043: '▁microphone',\n",
       " 802: '▁soon',\n",
       " 22855: '▁overshadowed',\n",
       " 20507: 'kuh',\n",
       " 26461: '▁Burkina',\n",
       " 29631: 'commissioned',\n",
       " 23577: '▁Viking',\n",
       " 27983: '▁Fey',\n",
       " 2984: '▁Ohio',\n",
       " 17937: '▁Exhibition',\n",
       " 31548: '▁presumption',\n",
       " 3669: '▁Hotel',\n",
       " 23626: 'arizona',\n",
       " 8922: '▁desktop',\n",
       " 3129: '▁Are',\n",
       " 23441: '▁widen',\n",
       " 26383: '▁Leipzig',\n",
       " 29775: '▁SSR',\n",
       " 11: ')',\n",
       " 21448: 'angi',\n",
       " 14094: 'oxi',\n",
       " 840: 'ra',\n",
       " 1390: '▁raised',\n",
       " 850: '▁create',\n",
       " 4212: '▁Where',\n",
       " 30741: '▁Attendance',\n",
       " 2215: '▁Ireland',\n",
       " 14619: '▁Suite',\n",
       " 19593: '▁stamped',\n",
       " 11344: '▁cups',\n",
       " 2339: '▁originally',\n",
       " 15820: '▁patches',\n",
       " 31843: '▁scrimmage',\n",
       " 29569: '▁Ferrero',\n",
       " 13380: '▁frontier',\n",
       " 27093: '▁preside',\n",
       " 11889: '▁slept',\n",
       " 13635: 'claim',\n",
       " 12004: '▁inspire',\n",
       " 29089: '▁Zedillo',\n",
       " 31413: '▁Garnett',\n",
       " 2710: '▁Te',\n",
       " 5598: '▁Rick',\n",
       " 27326: '▁Elise',\n",
       " 14366: '▁Bath',\n",
       " 21997: '▁Prefecture',\n",
       " 25419: '▁Yitzhak',\n",
       " 13323: 'shin',\n",
       " 28002: '▁colourful',\n",
       " 19738: 'FTA',\n",
       " 20058: 'Does',\n",
       " 26473: '▁Ranking',\n",
       " 26829: '▁arrogance',\n",
       " 10649: '▁obtaining',\n",
       " 28271: '▁clipping',\n",
       " 30432: '▁nascent',\n",
       " 7404: '▁barrels',\n",
       " 12608: 'ego',\n",
       " 297: '▁school',\n",
       " 9023: 'SO',\n",
       " 12481: 'iano',\n",
       " 8358: 'chy',\n",
       " 4258: 'tel',\n",
       " 8410: '▁examined',\n",
       " 16117: 'Ge',\n",
       " 9948: '▁verify',\n",
       " 9008: 'cid',\n",
       " 8790: '▁Heritage',\n",
       " 9360: '▁spur',\n",
       " 5488: '▁burned',\n",
       " 11812: '▁exchanged',\n",
       " 23203: 'Chicago',\n",
       " 23346: '▁sociology',\n",
       " 30806: '▁Bottle',\n",
       " 9122: '▁Made',\n",
       " 9565: 'haz',\n",
       " 19551: '▁Innovation',\n",
       " 22060: '▁Elliot',\n",
       " 27630: 'flavored',\n",
       " 31876: '▁GmbH',\n",
       " 16926: '▁glowing',\n",
       " 13536: '▁hormone',\n",
       " 14341: '▁Progressive',\n",
       " 24172: '▁revoked',\n",
       " 9408: '▁Latino',\n",
       " 9071: '▁toxic',\n",
       " 2883: '▁articles',\n",
       " 14490: '▁Belt',\n",
       " 17649: '▁Olympia',\n",
       " 18233: '▁timeline',\n",
       " 26036: '▁expedite',\n",
       " 30621: '▁Reaper',\n",
       " 31496: '▁Scarborough',\n",
       " 10269: '▁noon',\n",
       " 286: '▁without',\n",
       " 18710: '▁Ugandan',\n",
       " 1162: '▁hair',\n",
       " 17780: '▁youngsters',\n",
       " 18570: '▁feathers',\n",
       " 25894: '▁Constantinople',\n",
       " 13827: '▁endeavor',\n",
       " 10295: 'text',\n",
       " 6716: '▁Chan',\n",
       " 2745: '▁Best',\n",
       " 15975: '▁backyard',\n",
       " 8100: 'shirt',\n",
       " 25753: '▁raged',\n",
       " 12776: 'park',\n",
       " 22893: '▁sanitation',\n",
       " 29252: 'ogenesis',\n",
       " 30217: '▁Bjork',\n",
       " 11639: 'Pacific',\n",
       " 15199: '▁Thomson',\n",
       " 8872: '▁retire',\n",
       " 11928: '▁folded',\n",
       " 11646: '▁revealing',\n",
       " 20020: '▁Yak',\n",
       " 11888: '▁Sanchez',\n",
       " 20339: '▁Manitoba',\n",
       " 13242: '▁leagues',\n",
       " 11530: '▁harmful',\n",
       " 8050: '▁enabled',\n",
       " 9566: 'bell',\n",
       " 7635: 'NE',\n",
       " 29803: '▁pediatrician',\n",
       " 30632: 'benefit',\n",
       " 22483: '▁Stevenson',\n",
       " 7079: 'weight',\n",
       " 9779: '▁Peninsula',\n",
       " 21305: '▁contracting',\n",
       " 24162: '▁impartial',\n",
       " 27005: '▁statehood',\n",
       " 22613: '▁Hole',\n",
       " 31266: '▁Parallel',\n",
       " 30876: '▁Nickelodeon',\n",
       " 5550: 'real',\n",
       " 9986: 'zel',\n",
       " 3639: '▁adult',\n",
       " 3857: '▁sugar',\n",
       " 13319: '▁Presidential',\n",
       " 7882: 'CD',\n",
       " 23678: '▁Blank',\n",
       " 17682: 'TAL',\n",
       " 13340: '▁groom',\n",
       " 16671: 'Iraq',\n",
       " 3745: '▁2010,',\n",
       " 17481: '▁retention',\n",
       " 14684: '▁contests',\n",
       " 21412: '▁marketers',\n",
       " 14762: '▁whip',\n",
       " 2219: '▁Over',\n",
       " 20810: '▁viol',\n",
       " 10330: '▁Ethiopia',\n",
       " 19950: 'Top',\n",
       " 8254: 'lick',\n",
       " 1000: '▁tried',\n",
       " 8956: '▁fool',\n",
       " 2967: '45',\n",
       " 19592: '▁conserve',\n",
       " 27037: 'emptive',\n",
       " 18172: '▁Millennium',\n",
       " 16246: '▁nn',\n",
       " 3919: '▁busy',\n",
       " 29179: 'wisconsin',\n",
       " 20400: '▁Holden',\n",
       " 23356: '▁Naga',\n",
       " 29980: 'Network',\n",
       " 10454: '▁breeding',\n",
       " 4979: '▁Perhaps',\n",
       " 29198: 'construction',\n",
       " 29375: '▁ablaze',\n",
       " 12470: 'pid',\n",
       " 2958: '▁horse',\n",
       " 16414: 'should',\n",
       " 12895: 'gio',\n",
       " 16901: 'eater',\n",
       " 30837: '▁Ceylon',\n",
       " 30745: '▁Cornish',\n",
       " 17046: '▁firearms',\n",
       " 26108: '▁choreographer',\n",
       " 4200: '▁replacement',\n",
       " 24513: '▁recreate',\n",
       " 11019: 'rov',\n",
       " 19835: '▁Rodham',\n",
       " 4302: '▁Ber',\n",
       " 28528: 'Plus',\n",
       " 15719: 'rz',\n",
       " 3526: '▁Prince',\n",
       " 12341: '▁Taiwanese',\n",
       " 21961: '▁impacted',\n",
       " 16519: 'escu',\n",
       " 1569: '▁patients',\n",
       " 15170: 'tero',\n",
       " 20989: 'NYT',\n",
       " 15034: '▁jerk',\n",
       " 28874: '▁opaque',\n",
       " 10857: '▁Clo',\n",
       " 7421: '▁Contact',\n",
       " 14781: 'graf',\n",
       " 12619: 'nap',\n",
       " 10221: 'llo',\n",
       " 9974: '▁perception',\n",
       " 24871: '▁fiddle',\n",
       " 13478: '▁Thanksgiving',\n",
       " 18654: '▁Torres',\n",
       " 26642: '▁humming',\n",
       " 29840: '▁misguided',\n",
       " 3419: '▁waste',\n",
       " 15976: '▁dislike',\n",
       " 12827: '▁wicket',\n",
       " 20075: '▁chaotic',\n",
       " 1538: '▁middle',\n",
       " 6851: '▁Liverpool',\n",
       " 23762: '▁preschool',\n",
       " 5954: '▁dominated',\n",
       " 30473: '▁Mathematical',\n",
       " 3428: '▁connected',\n",
       " 31757: '▁subversive',\n",
       " 19095: '▁billionaire',\n",
       " 27275: '▁Stockton',\n",
       " 18705: '▁progressed',\n",
       " 20503: '▁ballistic',\n",
       " 3661: '▁golf',\n",
       " 16567: 'urge',\n",
       " 25267: '▁hypertension',\n",
       " 27211: '265',\n",
       " 29873: 'lugged',\n",
       " 149: '▁because',\n",
       " 17407: '▁slack',\n",
       " 13592: '▁candy',\n",
       " 9810: 'bil',\n",
       " 23683: 'idge',\n",
       " 28177: '▁Navajo',\n",
       " 19677: '▁electronically',\n",
       " 13844: '▁Lehman',\n",
       " 21953: '▁assortment',\n",
       " 28511: '▁prowess',\n",
       " 6173: '▁sequence',\n",
       " 4790: '▁implement',\n",
       " 16188: 'tard',\n",
       " 17008: '▁Torre',\n",
       " 8468: '▁cancel',\n",
       " 1150: '▁global',\n",
       " 9358: '▁SEC',\n",
       " 23235: '▁derive',\n",
       " 28574: '▁Korn',\n",
       " 3481: '▁Ban',\n",
       " 15032: '▁invalid',\n",
       " 10049: '▁revision',\n",
       " 12133: '▁endangered',\n",
       " 29316: 'corona',\n",
       " 2386: '▁Like',\n",
       " 25017: 'KING',\n",
       " 15825: '▁precedent',\n",
       " 25435: '▁Connie',\n",
       " 10058: '▁organizers',\n",
       " 4116: '▁lifted',\n",
       " 30932: 'containing',\n",
       " 10647: 'dies',\n",
       " 29449: '▁badminton',\n",
       " 25201: '▁Muh',\n",
       " 30565: '▁Jolie',\n",
       " 11628: '888',\n",
       " 3656: '▁System',\n",
       " 14917: '▁asthma',\n",
       " 19777: '▁exquisite',\n",
       " 13202: '▁Devon',\n",
       " 2034: '▁exactly',\n",
       " 13937: '▁Lea',\n",
       " 26435: '▁indispensable',\n",
       " 8381: '▁Bird',\n",
       " 19780: '▁biologist',\n",
       " 9692: 'HL',\n",
       " 21446: 'meth',\n",
       " 22656: '▁resemblance',\n",
       " 3382: '▁definitely',\n",
       " 14102: '▁Vale',\n",
       " 17871: '▁Speedway',\n",
       " 11142: 'lines',\n",
       " 3001: '▁fields',\n",
       " 5469: '▁popularity',\n",
       " 16476: '▁limitation',\n",
       " 30013: '▁ecstasy',\n",
       " 779: '▁force',\n",
       " 4186: '▁fiscal',\n",
       " 25821: '▁Boz',\n",
       " 4631: 'ef',\n",
       " 14891: '▁SUV',\n",
       " 27726: '▁Juda',\n",
       " 23769: 'Direct',\n",
       " 361: '▁officials',\n",
       " 3048: 'Q',\n",
       " 3887: '▁van',\n",
       " 27659: '▁Lime',\n",
       " 20978: '▁flared',\n",
       " 23208: 'ivist',\n",
       " 5325: '▁Wal',\n",
       " 31126: '▁Recommendation',\n",
       " 30543: '▁Janata',\n",
       " 17603: '▁Cheng',\n",
       " 4969: 'ica',\n",
       " 20331: 'pokker',\n",
       " 24911: '▁Apostle',\n",
       " 2741: '▁shoulder',\n",
       " 4102: '▁profits',\n",
       " 5337: '▁Ram',\n",
       " 3780: 'Le',\n",
       " 10595: '▁variations',\n",
       " 13907: '▁clearance',\n",
       " 1065: '▁amount',\n",
       " 16026: '▁broadly',\n",
       " 12783: '▁Satellite',\n",
       " 19605: '▁separatists',\n",
       " 20905: 'defunct',\n",
       " 21139: '▁Lux',\n",
       " 16969: '▁Chat',\n",
       " 14520: '▁herbs',\n",
       " 23217: ':40',\n",
       " 23504: '▁Dominique',\n",
       " 25890: '▁Monde',\n",
       " 27483: '▁snarl',\n",
       " 24420: '▁Thirteen',\n",
       " 11402: '▁ramp',\n",
       " 6590: '▁Ce',\n",
       " 20014: '▁Conditions',\n",
       " 29447: '▁emeritus',\n",
       " 21829: '▁allegiance',\n",
       " 11988: 'gged',\n",
       " 25953: '▁HMO',\n",
       " 1568: '▁direct',\n",
       " 11726: '▁suburbs',\n",
       " 26478: 'MMA',\n",
       " 2573: '▁promote',\n",
       " 3243: '▁permanent',\n",
       " 7171: '▁requiring',\n",
       " 15482: '▁Basin',\n",
       " 24704: '▁Tourist',\n",
       " 8063: '▁patch',\n",
       " 27794: '▁Nuremberg',\n",
       " 6573: '▁Bus',\n",
       " 24626: '▁ascertain',\n",
       " 1524: '▁consider',\n",
       " 13069: '▁coached',\n",
       " 6865: '▁600',\n",
       " 11036: '▁hallway',\n",
       " 10655: 'enko',\n",
       " 16895: '▁capped',\n",
       " 16552: '▁Kro',\n",
       " 15624: '▁acids',\n",
       " 18405: '▁Khmer',\n",
       " 14550: '▁turkey',\n",
       " 19859: 'ologies',\n",
       " 26355: '226',\n",
       " 31495: '▁Meridian',\n",
       " 31101: 'Modern',\n",
       " 22818: '▁fray',\n",
       " 8317: '▁surely',\n",
       " 832: 'less',\n",
       " 17262: '▁curved',\n",
       " 16844: '▁Battery',\n",
       " 2310: '▁measure',\n",
       " 19554: '▁woodland',\n",
       " 19667: '▁Stu',\n",
       " 15377: '▁necklace',\n",
       " 22171: '▁skipped',\n",
       " 31617: '▁Domino',\n",
       " 22672: '▁PENNSYLVANIA',\n",
       " 26683: '▁Turnover',\n",
       " 14127: 'upon',\n",
       " 1713: '▁code',\n",
       " 16153: '▁novelist',\n",
       " 24765: 'protein',\n",
       " ...}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 만들기\n",
    "sequence = f\"Once upon a time, there was \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "input_ids = tokenizer.encode(sequence, return_tensors=\"tf\") \n",
    "#앞에선 tokenizer를 했지만 지금은 encode로 문장을 토큰화하고, 토큰의 번호를 넘겨줌\n",
    "#return_tensors 옵션을 통해 tensorflow에서 활용하는 tensor 형태로 데이터 반환환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1977,  975,   24,   92,   19,  105,   30,    4,    3]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 아이디\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 문장을 모형에 입력\n",
    "result = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로짓, result의 첫 번째 값\n",
    "# logits를 softmax 함수에 입력하면 각각의 32000개의 단어에 대한 확률이 나옴\n",
    "logits = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 9, 32000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape # 1: 문장 하나 넣어서 출력 하나, 9: 토큰 아홉 개와 그에 대한 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 단어 뒤에 나올 토큰의 확률\n",
    "next_token_logits = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32000), dtype=float32, numpy=\n",
       "array([[-14.362142, -31.215897, -30.87506 , ..., -23.4436  , -22.54225 ,\n",
       "        -30.336617]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32000), dtype=float32, numpy=\n",
       "array([[1.5699778e-05, 7.5231596e-13, 1.0578500e-12, ..., 1.7859362e-09,\n",
       "        4.3986286e-09, 1.8124531e-12]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax 함수에 넣으면 확률형태로 나옴\n",
    "tf.nn.softmax(next_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = tf.math.top_k(next_token_logits, k=10) # k개의 가장 큰값을 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁or\n",
      "▁\n",
      ",\n",
      "d\n",
      ".\n",
      "▁ever\n",
      "▁a\n",
      "▁just\n",
      "▁was\n",
      "▁and\n"
     ]
    }
   ],
   "source": [
    "for i in top.indices[0].numpy().tolist():\n",
    "    #단어 번호만 출력 #tensor 형식에서 numpy array 형식으로 바꾸고 다시 list 형태로 변환\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 마스크 언어 모형으로 문장 중간의 빈칸 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 마스크 언어 모형 파이프라인 초기화\n",
    "from transformers import pipeline\n",
    "# 모형 다운로드\n",
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마스크 토큰 = 문장의 빈칸을 나타내는 토큰\n",
    "nlp.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.41411080956459045,\n",
       "  'token': 2674,\n",
       "  'token_str': ' favorite',\n",
       "  'sequence': 'Pizza is my favorite food'},\n",
       " {'score': 0.31080371141433716,\n",
       "  'token': 5863,\n",
       "  'token_str': ' comfort',\n",
       "  'sequence': 'Pizza is my comfort food'},\n",
       " {'score': 0.13814868032932281,\n",
       "  'token': 5548,\n",
       "  'token_str': ' favourite',\n",
       "  'sequence': 'Pizza is my favourite food'},\n",
       " {'score': 0.015286203473806381,\n",
       "  'token': 3366,\n",
       "  'token_str': ' dream',\n",
       "  'sequence': 'Pizza is my dream food'},\n",
       " {'score': 0.013527827337384224,\n",
       "  'token': 6543,\n",
       "  'token_str': ' signature',\n",
       "  'sequence': 'Pizza is my signature food'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인에 문장 입력\n",
    "nlp(f\"Pizza is my {nlp.tokenizer.mask_token} food\") \n",
    "# <mask>로 작성해도 됨 \n",
    "# 빈 칸에 들어갈 토큰을 알려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 칸(마스크)가 2개 이상인 경우 직접 모형을 만들어야 한다 (pipeline 사용 X)\n",
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 모형 다운로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저의 어휘 목록 호출 -> 단어-번호의 형태에서 번호-단어의 형태로 변환\n",
    "vocab = tokenizer.get_vocab()\n",
    "id2word = {i: word for word, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 입력\n",
    "sequence = f\"Pizza is my {tokenizer.mask_token} food.\" # f-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화, 모델에 넣기 전 인코딩\n",
    "input_ids = tokenizer.encode(sequence, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 칸의 위치 찾기\n",
    "mask_token_indices = tf.where(input_ids[0] == tokenizer.mask_token_id)[0].numpy().tolist()\n",
    "# tokenizer.mask_token_id = 빈 칸을 나타내는 토큰 번호\n",
    "# tensor.flow의 where함수를 통해 빈 칸의 위치를 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형에 입력\n",
    "result = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로짓\n",
    "logits = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈 칸 위치\n",
    "i = mask_token_indices[0]\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 칸의 로짓, 9개 토큰 중에 5번째 토큰에 들어갈 로짓값\n",
    "mask_token_logits = logits[0, i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 로짓이 높은 토큰 10개\n",
    "top = tf.math.top_k(mask_token_logits, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġfavorite\n",
      "Ġcomfort\n",
      "Ġfavourite\n",
      "Ġsignature\n",
      "Ġdream\n",
      "Ġpreferred\n",
      "Ġpassion\n",
      "Ġstaple\n",
      "Ġbreakfast\n",
      "Ġeveryday\n"
     ]
    }
   ],
   "source": [
    "# 출력\n",
    "for i in top.indices.numpy().tolist():\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 빈칸이 2개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"Pizza {tokenizer.mask_token} my {tokenizer.mask_token} food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(sequence, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_indices = tf.where(input_ids[0] == tokenizer.mask_token_id)\n",
    "mask_token_indices = tf.squeeze(mask_token_indices).numpy().tolist() #squeeze 함수를 통해 indices의 형태를 간략하게 바꿈\n",
    "mask_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3 ===\n",
      "Ġis\n",
      "Ġwas\n",
      "Ġas\n",
      "Ġbecomes\n",
      "Ġequals\n",
      "Ġfor\n",
      "Ġdelivers\n",
      "Ġmakes\n",
      "Ġand\n",
      "Ġwith\n",
      "=== 5 ===\n",
      "Ġfavorite\n",
      "Ġcomfort\n",
      "Ġfavourite\n",
      "Ġjunk\n",
      "Ġbreakfast\n",
      "Ġlunch\n",
      "Ġown\n",
      "Ġsnack\n",
      "Ġfried\n",
      "Ġpreferred\n"
     ]
    }
   ],
   "source": [
    "result = model(input_ids)\n",
    "logits = result[0]\n",
    "for i in mask_token_indices:\n",
    "    print(f'=== {i} ===')\n",
    "    mask_token_logits = logits[0, i, :]\n",
    "    top = tf.math.top_k(mask_token_logits, k=10)\n",
    "    for i in top.indices.numpy().tolist():\n",
    "        print(id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기타 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# tokenizers의 pipeline의 여러 가지 기능\n",
    "# 질문 답변\n",
    "qa = pipeline(\"question-answering\") #모델 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지문\n",
    "context = \"\"\"\n",
    "Seoul, officially the Seoul Special City, is the capital and largest metropolis \n",
    "of South Korea. Seoul has a population of 9.7 million people, and forms \n",
    "the heart of the Seoul Capital Area with the surrounding Incheon metropolis and \n",
    "Gyeonggi province.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.862982451915741, 'start': 1, 'end': 6, 'answer': 'Seoul'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"where is the capital city of South Korea?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9299284219741821,\n",
       " 'start': 124,\n",
       " 'end': 135,\n",
       " 'answer': '9.7 million'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"How many people live in Seoul?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 고유명사 인식\n",
    "ner = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.9995671,\n",
       "  'index': 1,\n",
       "  'word': 'Seoul',\n",
       "  'start': 1,\n",
       "  'end': 6},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981558,\n",
       "  'index': 5,\n",
       "  'word': 'Seoul',\n",
       "  'start': 23,\n",
       "  'end': 28},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.97723013,\n",
       "  'index': 6,\n",
       "  'word': 'Special',\n",
       "  'start': 29,\n",
       "  'end': 36},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.98950726,\n",
       "  'index': 7,\n",
       "  'word': 'City',\n",
       "  'start': 37,\n",
       "  'end': 41},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9969215,\n",
       "  'index': 17,\n",
       "  'word': 'South',\n",
       "  'start': 85,\n",
       "  'end': 90},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9992411,\n",
       "  'index': 18,\n",
       "  'word': 'Korea',\n",
       "  'start': 91,\n",
       "  'end': 96},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.999453,\n",
       "  'index': 20,\n",
       "  'word': 'Seoul',\n",
       "  'start': 98,\n",
       "  'end': 103},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.98211783,\n",
       "  'index': 37,\n",
       "  'word': 'Seoul',\n",
       "  'start': 172,\n",
       "  'end': 177},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9603081,\n",
       "  'index': 38,\n",
       "  'word': 'Capital',\n",
       "  'start': 178,\n",
       "  'end': 185},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.937354,\n",
       "  'index': 39,\n",
       "  'word': 'Area',\n",
       "  'start': 186,\n",
       "  'end': 190},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.998509,\n",
       "  'index': 43,\n",
       "  'word': 'Inc',\n",
       "  'start': 212,\n",
       "  'end': 215},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.97816354,\n",
       "  'index': 44,\n",
       "  'word': '##he',\n",
       "  'start': 215,\n",
       "  'end': 217},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9987452,\n",
       "  'index': 45,\n",
       "  'word': '##on',\n",
       "  'start': 217,\n",
       "  'end': 219},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99873143,\n",
       "  'index': 49,\n",
       "  'word': 'G',\n",
       "  'start': 236,\n",
       "  'end': 237},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9916979,\n",
       "  'index': 50,\n",
       "  'word': '##ye',\n",
       "  'start': 237,\n",
       "  'end': 239},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9972524,\n",
       "  'index': 51,\n",
       "  'word': '##ong',\n",
       "  'start': 239,\n",
       "  'end': 242},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99624276,\n",
       "  'index': 52,\n",
       "  'word': '##gi',\n",
       "  'start': 242,\n",
       "  'end': 244}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# 요약, 모델이 처리할 수 있는 길이에 제약이 있어서 너무 길거나, 짧으면 안 됌, 150~1000 tokens\n",
    "summ = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A zettelkasten consists of many individual notes with ideas and other short\n",
    "pieces of information that are taken down as they occur or are acquired. The\n",
    "notes are numbered hierarchically, so that new notes may be inserted at the\n",
    "appropriate place, and contain metadata to allow the note-taker to associate\n",
    "notes with each other. For example, notes may contain tags that describe key\n",
    "aspects of the note, and they may reference other notes. The numbering,\n",
    "metadata, format and structure of the notes is subject to variation depending on\n",
    "the specific method employed. Creating and using a zettelkasten is made easier\n",
    "by taking the notes down digitally and using appropriate knowledge management\n",
    "software. But it can be and has long been done on paper using index cards. The\n",
    "method not only allows a researcher to store and retrieve information related to\n",
    "their research, but also intends to enhance creativity. Cross-referencing notes\n",
    "through tags allows the researcher to perceive connections and relationships\n",
    "between individual items of information that may not be apparent in isolation.\n",
    "These emergent aspects of the method make the zettelkasten somewhat similar to a\n",
    "neural network with which one may \"converse\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A zettelkasten consists of many individual notes with ideas that are taken down as they occur or are acquired . The numbering, metadata, format and structure of the notes is subject to variation depending on the specific method employed . Cross-referencing notes with tags allows the researcher to perceive connections and relationships between notes that may not be apparent in isolation . The method is made easier by taking the notes down digitally and using knowledge management software .'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# zero-short classification = 전이학습 시, 데이터가 전혀 필요하지 않은 것 분류\n",
    "# 미리 다양한 종류의 분류를 할 수 있도록 학습되어 있어, 분류할 때 추가 학습 없이 분류 이름을 이용해 텍스트 분류\n",
    "zs = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장\n",
    "sequence = 'Pizza is my favorite food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장을 label에 분류, label에 분류하기 위해 따로 학습이 필요 없음\n",
    "label = ['food', 'ocean', 'space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Pizza is my favorite food',\n",
       " 'labels': ['food', 'space', 'ocean'],\n",
       " 'scores': [0.9971833825111389, 0.0015179921174421906, 0.0012986757792532444]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs(sequence, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers 결정론적 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모형 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = TFAutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 시작 설정, 시작에 이어지는 문장 생성\n",
    "input_ids = tokenizer.encode('I like this movie', return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 탐욕 탐색\n",
    "# 문장의 시작을 모델에 입력, 뒷 순서를 확률에 따라 예측; max_length를 통해 최대 길이 지정(현재 50 단어), 지정 안해줄 시 문장이 끝날 때까지\n",
    "result = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like this movie. I like the way it's set up. I like the way it's set up. I like the way it's set up. I like the way it's set up. I like the way it's set up.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])\n",
    "# 같은 단어의 반복이 현 디코딩 방법론의 문제점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 빔 탐색\n",
    "# num_beams를 통해 빔의 개수 지정, early_stopping을 통해 후보군 모두가 문장이 끝났다면, 전체 길이에 도달하지 못해도 중단\n",
    "result = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like this movie. It's a lot of fun to watch. It's a lot of fun to watch. It's a lot of fun to watch. It's a lot of fun to watch. It's a lot of fun to watch.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 2-gram이 반복되는 것을 억제\n",
    "# 연속된 두 개의 표현이 반복되지 못하도록 후보군에서 강제로 삭제, 너무 size를 작게 잡으면 꼭 필요한 표현을 사용하지 못할 수도 있음\n",
    "result = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True, no_repeat_ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like this movie, but I don\\'t think it\\'s going to be as good as I thought it would be,\" he said.\\n\\n\"I\\'m not sure if it will be the best movie I\\'ve ever seen. It\\'s not going'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this movie, but I don't think it's going to be as good as I thought it would be,\" he said.\n",
      "\n",
      "\"I'm not sure if it will be the best movie I've ever seen. It's not going\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfomers 확률적 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = TFAutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('I like this movie', return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 무작위 추출\n",
    "# 실행할 때마다 달라지는 결과를 고정하기 위해 set_seed 를 통해 시드값 고정\n",
    "# do_sample = True를 통해, 랜덤하게 추출하며, top_k = 0 을 통해 모든 후보 고려\n",
    "tf.random.set_seed(0)\n",
    "result = model.generate(input_ids, max_length=50, do_sample=True, top_k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like this movie and it's perfect. As happy as they are along with Billy Smith and Mark Ruffalo it still brings a lot of satisfaction to me. The boda is terribly wrong and I'm fine with that. The vamp two seems\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 온도 조절\n",
    "# temperature 를 통해 온도 조절 (기본 온도 1.0), 상대적으로 매끄러운 문장 생성\n",
    "tf.random.set_seed(0)\n",
    "result = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7, top_k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I like this movie. The house is so cold that it looks like we're eating out. The shower is very cold and the wind is blowing. The best part, though, is that it's like you're in the middle of the movie.\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# top-k\n",
    "# 온도 설정 대신 top-k 설정\n",
    "tf.random.set_seed(0)\n",
    "result = model.generate(input_ids, max_length=50, do_sample=True, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this movie the most. This is like trying to figure out why your girlfriend did this to her, what she wanted, who she wasn't. I don't think it goes into all the details of what was done; the film does have\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# top-p\n",
    "# top-k 대신 top-p 설정\n",
    "tf.random.set_seed(0)\n",
    "result = model.generate(input_ids, max_length=50, do_sample=True, top_p=0.9, top_k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this movie to have that alternative but this movie just brings it down in scale and drags down the plot at times. The music can be kind of unique, but I prefer to give the movie more credit to myself and the overall score for\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformers 깁스 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import tf_top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 모형 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "model = TFAutoModelForMaskedLM.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마스크 토큰\n",
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "input_ids = tokenizer.encode('I like this movie <mask><mask><mask><mask><mask><mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>I like this movie<mask><mask><mask><mask><mask><mask></s>'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids) #앞 뒤의 <s>는 문장의 시작과 끝을 나타내는 기호; 문장 길이에 더해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이 측정\n",
    "SEQ_LEN = len(input_ids)\n",
    "SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 위치 하나를 골라 마스킹\n",
    "i = random.randint(1, SEQ_LEN - 2) # 일정 범위 정수 랜덤 생성; 0번과 12번의 토큰을 제외(시작과 끝 기호 토큰)\n",
    "input_ids[i] = tokenizer.mask_token_id # 랜덤하게 빈 칸을 만들고 예측 진행 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 모형에 입력; 모델에 입력하기 위해 리스트를 tensor 형태로 변경\n",
    "result = model(tf.convert_to_tensor([input_ids])) # 한 번에 모델에 여러 문장을 넣을 수 있기 때문에 리스트 형태로 감싸서 넣어줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로짓 확인\n",
    "logits = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 12, 50265])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape # 1개의 문장, 12개의 토큰, 50265개의 토큰 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 마스킹한 위치의 로짓을 선택\n",
    "logits = logits[:, i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50265])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape # 한 개의 토큰에 대한 50265개의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로짓에 따라 무작위로 토큰을 고름\n",
    "token_id = tf.random.categorical(logits, num_samples=1) # 로짓을 소프트맥스에 통과시켜 확률로 바꿔 랜덤하게 50265개 중 하나를 골라줌; num_samples = 1 한 개를 고름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고른 토큰의 번호를 마스킹된 위치에 대입\n",
    "input_ids[i] = token_id.numpy()[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>I like this movie<mask> Cast<mask><mask><mask><mask></s>'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 확인\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I like this movie<mask> Use<mask><mask><mask><mask></s>\n",
      "<s>I like this movie<mask> Use<mask><mask><mask><mask></s>\n",
      "<s>I like this movie<mask> Use Them<mask><mask><mask></s>\n",
      "<s>I like silly movie<mask> Use Them<mask><mask><mask></s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Looks like silly movie<mask> Use Them<mask><mask><mask></s>\n",
      "<s>Looks like silly movie<mask> Use Them<mask> �<mask></s>\n",
      "<s>Looks like Matrix movie<mask> Use Them<mask> �<mask></s>\n",
      "<s>Looks like Matrix movie<mask> Use TM<mask> �<mask></s>\n",
      "<s>Looks like Matrix movie<mask> Use Jackson<mask> �<mask></s>\n",
      "<s>Looks like Matrix movie actors Use Jackson<mask> �<mask></s>\n",
      "<s>Looks like Matrix movie actors Use Jackson<mask> �<mask></s>\n",
      "<s>Looks like Matrix movie actors Use Jackson Axe �<mask></s>\n",
      "<s>Looks like Matrix movie actors Use Jackson Wiki �<mask></s>\n",
      "<s>Looks like Matrix Singer actors Use Jackson Wiki �<mask></s>\n",
      "<s>Looks like Matrix Singer actors Use Jackson Wiki �<mask></s>\n",
      "<s>Looks like Matrix Singer actors Use Jackson Wiki 😉</s>\n",
      "<s>Looks like Matrix Singer actors Use Matrix Wiki 😉</s>\n",
      "<s>Looks like Matrix Singer actors Use Matrix Wiki 😉</s>\n",
      "<s>Looks like Matrix robot actors Use Matrix Wiki 😉</s>\n",
      "<s>Looks like Matrix robot actors Use Matrix Wiki 😉</s>\n"
     ]
    }
   ],
   "source": [
    "# 위 과정 20회 반복\n",
    "for _ in range(20):\n",
    "    i = random.randint(1, SEQ_LEN - 2)\n",
    "    input_ids[i] = tokenizer.mask_token_id\n",
    "\n",
    "    result = model(tf.convert_to_tensor([input_ids]))\n",
    "\n",
    "    logits = result[0]\n",
    "    logits = logits[:, i, :]\n",
    "\n",
    "    token_id = tf.random.categorical(logits, num_samples=1)\n",
    "    input_ids[i] = token_id.numpy()[0,0]\n",
    "    print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I like this movie<mask><mask><mask><mask><mask>?!</s>\n",
      "<s>I like this pumpkin<mask><mask><mask><mask><mask>?!</s>\n",
      "<s>I like this pumpkin<mask><mask><mask><mask><mask>....</s>\n",
      "<s>I like this pumpkin<mask><mask> pumpkin<mask><mask>....</s>\n",
      "<s>I like coconut pumpkin<mask><mask> pumpkin<mask><mask>....</s>\n",
      "<s>I like coconut pumpkin<mask> fried pumpkin<mask><mask>....</s>\n",
      "<s>I like coconut pumpkin<mask> fried pumpkin<mask> soup....</s>\n",
      "<s>I like coconut pumpkin<mask> fried pumpkin<mask> stew....</s>\n",
      "<s>I love coconut pumpkin<mask> fried pumpkin<mask> stew....</s>\n",
      "<s>I enjoy coconut pumpkin<mask> fried pumpkin<mask> stew....</s>\n",
      "<s>I enjoy coconut pumpkin spice fried pumpkin<mask> stew....</s>\n",
      "<s>Also enjoy coconut pumpkin spice fried pumpkin<mask> stew....</s>\n",
      "<s>Also enjoy coconut curry spice fried pumpkin<mask> stew....</s>\n",
      "<s>Also enjoy coconut curry spice plus pumpkin<mask> stew....</s>\n",
      "<s>Also enjoy coconut curry soup plus pumpkin<mask> stew....</s>\n",
      "<s>Also enjoy coconut curry soup plus pumpkin chili stew....</s>\n",
      "<s>Also enjoy pumpkin curry soup plus pumpkin chili stew....</s>\n",
      "<s>Also, pumpkin curry soup plus pumpkin chili stew....</s>\n",
      "<s>Also, pumpkin curry soup plus beet chili stew....</s>\n",
      "<s>Also, pumpkin curry soup plus beetroot stew....</s>\n"
     ]
    }
   ],
   "source": [
    "# top-k를 적용하여 20회 반복\n",
    "input_ids = tokenizer.encode('I like this movie <mask><mask><mask><mask><mask><mask>')\n",
    "for _ in range(20):\n",
    "    i = random.randint(1, SEQ_LEN - 2)\n",
    "    input_ids[i] = tokenizer.mask_token_id\n",
    "\n",
    "    result = model(tf.convert_to_tensor([input_ids]))\n",
    "\n",
    "    logits = result[0]\n",
    "    logits = logits[:, i, :]\n",
    "    logits = tf_top_k_top_p_filtering(logits, top_k=50) # top-k\n",
    "\n",
    "    token_id = tf.random.categorical(logits, num_samples=1)\n",
    "    input_ids[i] = token_id.numpy()[0,0]\n",
    "    print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I like this movie<mask><mask><mask><mask>VIEW<mask></s>\n",
      "<s>I like this �<mask><mask><mask><mask>VIEW<mask></s>\n",
      "<s>I like this BL<mask><mask><mask><mask>VIEW<mask></s>\n",
      "<s>I like this BL<mask><mask><mask><mask> OUT<mask></s>\n",
      "<s>I like this BL<mask><mask> FIN<mask> OUT<mask></s>\n",
      "<s>I like this BLOWN<mask> FIN<mask> OUT<mask></s>\n",
      "<s>I like this BLOWN<mask> SEE<mask> OUT<mask></s>\n",
      "<s>I like this GROWN<mask> SEE<mask> OUT<mask></s>\n",
      "<s>I like this GROWN<mask> SEE<mask> OUTAGE</s>\n",
      "<s>I like this GROWN<mask> SEE<mask> IMAGE</s>\n",
      "<s>I like this GROWN<mask> SEE<mask> IMAGE</s>\n",
      "<s>I like this GRANT<mask> SEE<mask> IMAGE</s>\n",
      "<s>I like MORE GRANT<mask> SEE<mask> IMAGE</s>\n",
      "<s>I like MORE GRANT<mask> SEE THE IMAGE</s>\n",
      "<s>I like MORE GRANT INFORMATION SEE THE IMAGE</s>\n",
      "<s>I like MORE GRANT INFORMATION SEE THIS IMAGE</s>\n",
      "<s>I like MORE GRANT TO SEE THIS IMAGE</s>\n",
      "<s>I like MORE GRANT NEWS SEE THIS IMAGE</s>\n",
      "<s>I like MORE GRANT HELP SEE THIS IMAGE</s>\n",
      "<s>I like MORE GRANT NEWS SEE THIS IMAGE</s>\n"
     ]
    }
   ],
   "source": [
    "# top-p를 적용하여 20회 반복\n",
    "input_ids = tokenizer.encode('I like this movie <mask><mask><mask><mask><mask><mask>')\n",
    "for _ in range(20):\n",
    "    i = random.randint(1, SEQ_LEN - 2)\n",
    "    input_ids[i] = tokenizer.mask_token_id\n",
    "\n",
    "    result = model(tf.convert_to_tensor([input_ids]))\n",
    "\n",
    "    logits = result[0]\n",
    "    logits = logits[:, i, :]\n",
    "    logits = tf_top_k_top_p_filtering(logits, top_p=0.9) # top-p\n",
    "\n",
    "    token_id = tf.random.categorical(logits, num_samples=1)\n",
    "    input_ids[i] = token_id.numpy()[0,0]\n",
    "    print(tokenizer.decode(input_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 03. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT of Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f176e8a081b9402ab63b55fc1d290992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucky\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2324500f80be48e48c89ddfadd5eae90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bdb0bbf922426ab43507e6f3ceb322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfb9eb5b7d64bea9069c9d997bcfaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c66d9b0a7d4c2faffd393c975c118a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensor flow용 Masked LM ; 문장 중간에 가려진 부분을 추측하는 모델\n",
    "# Pytorch 용은 앞에 TF를 빼면 됨\n",
    "from transformers import TFBertForMaskedLM, AutoTokenizer\n",
    "\n",
    "# 모델 호출\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# uncased; 대소문자 구분을 하지 않음\n",
    "\n",
    "# 토크나이저 호출\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 모형에서 사용하는 mask_token (빈 칸)\n",
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('Pizza is my [MASK] food.', return_tensors='tf') # 토크나이저를 함수처럼 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101, 10733,  2003,  2026,   103,  2833,  1012,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과가 사전 형식으로 구성\n",
    "# input_ids ; 토큰을 번호로 변환\n",
    "# tokenizer.cls_token_id = 101 , 문장 전체를 가리키는 특수 토큰\n",
    "# tokenizer.sep_token_id = 102 , 문장을 나눠주는 특수 토큰\n",
    "# token_type_ids ; 첫 번째 문장과 두 번째 문장을 구별하는 ids\n",
    "# attention_mask ; 여러 문장을 동시에 모형에 집어넣었을 때, 문장의 길이를 맞춰주는 패딩을 위한 ids\n",
    "# 리스트 형태이므로 tensor flow 모형을 사용하므로 return_tensors를 통해 tensor 형식으로 변환\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFMaskedLMOutput(loss=None, logits=<tf.Tensor: shape=(1, 8, 30522), dtype=float32, numpy=\n",
       "array([[[ -6.5892487,  -6.545541 ,  -6.550171 , ...,  -5.9745717,\n",
       "          -5.734547 ,  -4.0158253],\n",
       "        [ -7.9899735,  -7.829558 ,  -7.847027 , ...,  -7.5801945,\n",
       "          -6.8277617,  -6.485946 ],\n",
       "        [-12.688318 , -12.100923 , -12.303796 , ..., -10.958023 ,\n",
       "          -9.478668 , -11.172894 ],\n",
       "        ...,\n",
       "        [-10.789202 , -11.01828  , -10.619028 , ...,  -9.310557 ,\n",
       "          -7.9503617, -12.606783 ],\n",
       "        [-11.8699875, -11.202813 , -11.640641 , ...,  -8.01551  ,\n",
       "          -9.46438  ,  -7.81282  ],\n",
       "        [-16.216097 , -16.040514 , -16.043266 , ..., -16.037786 ,\n",
       "         -14.760448 , -13.9509945]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1, 8, 30522)\n",
    "# 문장은 1개를 넣었으며, 토큰의 수가 8개\n",
    "# 토큰 8개에 대해 30522개의 위치에 대해 어떤 토큰이 왔어야 하는가에 대한 예측\n",
    "# tokenizer.vocab_size = 30522\n",
    "result = model(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[  101, 10733,  2003,  2026,   103,  2833,  1012,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 30522), dtype=float32, numpy=\n",
       "array([[[ -6.5892487,  -6.545541 ,  -6.550171 , ...,  -5.9745717,\n",
       "          -5.734547 ,  -4.0158253],\n",
       "        [ -7.9899735,  -7.829558 ,  -7.847027 , ...,  -7.5801945,\n",
       "          -6.8277617,  -6.485946 ],\n",
       "        [-12.688318 , -12.100923 , -12.303796 , ..., -10.958023 ,\n",
       "          -9.478668 , -11.172894 ],\n",
       "        ...,\n",
       "        [-10.789202 , -11.01828  , -10.619028 , ...,  -9.310557 ,\n",
       "          -7.9503617, -12.606783 ],\n",
       "        [-11.8699875, -11.202813 , -11.640641 , ...,  -8.01551  ,\n",
       "          -9.46438  ,  -7.81282  ],\n",
       "        [-16.216097 , -16.040514 , -16.043266 , ..., -16.037786 ,\n",
       "         -14.760448 , -13.9509945]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = result[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKV2(values=<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([15.272723, 12.503539, 10.711415,  9.428525,  8.84376 ],\n",
       "      dtype=float32)>, indices=<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 5440,  8837,  7216,  6871, 18785])>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "top = tf.math.top_k(logits[0, 4], k=5)\n",
    "top\n",
    "# 네 번째 위치한 토큰의 30522개에 대한 확률값 중 가장 높은 것 5개 (5440, 8837, 7216, 6871, 18785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'favorite favourite comfort preferred staple'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(top.indices.numpy())\n",
    "# 위의 번호를 array 형태로 변환한 뒤 토크나이저에 디코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 자동으로 [MASK] 찾기 (빈칸 채우기)\n",
    "pip = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9178980588912964,\n",
       "  'token': 5440,\n",
       "  'token_str': 'favorite',\n",
       "  'sequence': 'pizza is my favorite food.'},\n",
       " {'score': 0.05756433680653572,\n",
       "  'token': 8837,\n",
       "  'token_str': 'favourite',\n",
       "  'sequence': 'pizza is my favourite food.'},\n",
       " {'score': 0.009590580128133297,\n",
       "  'token': 7216,\n",
       "  'token_str': 'comfort',\n",
       "  'sequence': 'pizza is my comfort food.'},\n",
       " {'score': 0.0026588451582938433,\n",
       "  'token': 6871,\n",
       "  'token_str': 'preferred',\n",
       "  'sequence': 'pizza is my preferred food.'},\n",
       " {'score': 0.0014816072070971131,\n",
       "  'token': 18785,\n",
       "  'token_str': 'staple',\n",
       "  'sequence': 'pizza is my staple food.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('Pizza is my [MASK] food.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반자동으로 [MASK] 찾기 (빈칸 채우기) (원하는 모델과 토크나이저 지정 가능)\n",
    "from transformers import FillMaskPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9178860187530518,\n",
       "  'token': 5440,\n",
       "  'token_str': 'favorite',\n",
       "  'sequence': 'pizza is my favorite food.'},\n",
       " {'score': 0.057563528418540955,\n",
       "  'token': 8837,\n",
       "  'token_str': 'favourite',\n",
       "  'sequence': 'pizza is my favourite food.'},\n",
       " {'score': 0.009590426459908485,\n",
       "  'token': 7216,\n",
       "  'token_str': 'comfort',\n",
       "  'sequence': 'pizza is my comfort food.'},\n",
       " {'score': 0.002658800221979618,\n",
       "  'token': 6871,\n",
       "  'token_str': 'preferred',\n",
       "  'sequence': 'pizza is my preferred food.'},\n",
       " {'score': 0.0014815806644037366,\n",
       "  'token': 18785,\n",
       "  'token_str': 'staple',\n",
       "  'sequence': 'pizza is my staple food.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip2 = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "pip2('Pizza is my [MASK] food.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KcBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b68e8cd7df74b69a1c0467fd9de60ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66eff7b701e41b98c3616a043db286c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForMaskedLM: ['cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFBertForMaskedLM from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMaskedLM from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 데이터에 학습시킨 BERT 모형\n",
    "from transformers import TFBertForMaskedLM, BertTokenizerFast, FillMaskPipeline\n",
    "\n",
    "# 한국어 댓글 데이터로 학습\n",
    "# kcbert-large 는 더 큰 데이터\n",
    "# pytorch로 학습되어 있으므로 from_pt = True를 통해 tensor flow로 활용\n",
    "model = TFBertForMaskedLM.from_pretrained('beomi/kcbert-base', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cdf3aa2f5f42d6bd32d62ae618dba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3d79ae27da4587b4e1ba056b3842b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/250k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7888962626457214,\n",
       "  'token': 10015,\n",
       "  'token_str': '좋아하는',\n",
       "  'sequence': '피자는 내가 가장 좋아하는 음식이다.'},\n",
       " {'score': 0.11531581729650497,\n",
       "  'token': 13414,\n",
       "  'token_str': '싫어하는',\n",
       "  'sequence': '피자는 내가 가장 싫어하는 음식이다.'},\n",
       " {'score': 0.031990665942430496,\n",
       "  'token': 10431,\n",
       "  'token_str': '먹는',\n",
       "  'sequence': '피자는 내가 가장 먹는 음식이다.'},\n",
       " {'score': 0.015781046822667122,\n",
       "  'token': 10583,\n",
       "  'token_str': '원하는',\n",
       "  'sequence': '피자는 내가 가장 원하는 음식이다.'},\n",
       " {'score': 0.011102357879281044,\n",
       "  'token': 15098,\n",
       "  'token_str': '사랑하는',\n",
       "  'sequence': '피자는 내가 가장 사랑하는 음식이다.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "pipeline('피자는 내가 가장 [MASK] 음식이다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT 미세조정 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3         Very little music or anything to speak of.          0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/imdb.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT를 이용한 빈칸 채우기\n",
    "from transformers import TFBertForMaskedLM, BertTokenizerFast, FillMaskPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1292145550251007,\n",
       "  'token': 5262,\n",
       "  'token_str': 'indeed',\n",
       "  'sequence': 'this is very interesting indeed.'},\n",
       " {'score': 0.06471988558769226,\n",
       "  'token': 2592,\n",
       "  'token_str': 'information',\n",
       "  'sequence': 'this is very interesting information.'},\n",
       " {'score': 0.044466882944107056,\n",
       "  'token': 2739,\n",
       "  'token_str': 'news',\n",
       "  'sequence': 'this is very interesting news.'},\n",
       " {'score': 0.03403373062610626,\n",
       "  'token': 2182,\n",
       "  'token_str': 'here',\n",
       "  'sequence': 'this is very interesting here.'},\n",
       " {'score': 0.02843305841088295,\n",
       "  'token': 2205,\n",
       "  'token_str': 'too',\n",
       "  'sequence': 'this is very interesting too.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "pipeline('This is very interesting [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "import numpy as np\n",
    "\n",
    "# 토크나이저의 단어 중, 특수 토큰을 제외하고 일반 토큰만 지정 (100, 102, 0, 101, 103 제외)\n",
    "# set을 활용해 차집합 계산 \n",
    "word_tokens = list(set(range(tokenizer.vocab_size)) - set(tokenizer.all_special_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_inputs(input_ids):\n",
    "    masked = list(input_ids) # 입력에 해당하는 부분\n",
    "    labels = [-100] * len(masked) # 출력에 해당하는 부분, 숫자에 리스트를 붙혀주면 해당 원소를 반복해줌 (-100을 입력의 길이만큼 반복한 리스트 형성)\n",
    "                                  # BERT는 -100을 손실 계산에서 제외, 예측을 했을 때 -100이면 손실을 계산하지 않고 손실 계산할 부분만 바꿔줌\n",
    "\n",
    "    for i in range(1, len(masked) - 1): # input_ids에서 cls token과 sep token을 제외 (무조건 붙기 때문에 맞출 필요가 없음)\n",
    "        choice1 = np.random.choice(['change', 'not change'], p=[0.15, 0.85]) # change와 not change 중 확률대로 랜덤하게 선택\n",
    "        if choice1 == 'change': # 토큰 중 하나를 골랐을 때 change 이다\n",
    "            labels[i] = masked[i] # labels 중 -100이 아닌 토큰의 값이 입력됨\n",
    "            choice2 = np.random.choice(['mask', 'random', 'same'], p=[0.8, 0.1, 0.1]) # 입력이 들어온대로 예측하는 것이 아닌, 마스크 토큰, 무작위, 같은 단어로 예측\n",
    "            if choice2 == 'mask':\n",
    "                masked[i] = tokenizer.mask_token_id # 마스크 토큰으로 배정\n",
    "            elif choice2 == 'random':\n",
    "                masked[i] = np.random.choice(word_tokens) # 단어 목록 중 무작위 배정\n",
    "    return masked, labels\n",
    "# 논문에 나온 방법대로 처리, 15% 단어만 골라 학습\n",
    "\n",
    "# masked의 103 이 마스킹된 토큰이며\n",
    "# labels에 해당하는 토큰의 정답이 有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_inputs(input_ids):\n",
    "    masked = list(input_ids)\n",
    "    labels = [-100] * len(masked)\n",
    "    i = np.random.randint(1, len(masked) - 1) # 문장 전체에서 단어 하나를 무조건 배치 (해당 범위에서 i 하나를 무조건 선택)\n",
    "    labels[i] = masked[i]\n",
    "    choice2 = np.random.choice(['mask', 'random', 'same'], p=[0.8, 0.1, 0.1])\n",
    "    if choice2 == 'mask':\n",
    "        masked[i] = tokenizer.mask_token_id\n",
    "    elif choice2 == 'random':\n",
    "        masked[i] = np.random.choice(word_tokens)\n",
    "    return masked, labels\n",
    "# 문장이 짧은 경우 모든 단어가 학습 대상에서 빠질 수 있음(손실을 전혀 계산하지 않음), 문장 중에 한 단어를 골라 학습 대상으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord로 저장\n",
    "import tensorflow as tf\n",
    "\n",
    "def int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "filename = 'bert_train.tfrecord'\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for row in df.itertuples(): # 1,000개의 데이터를 하나씩 순환하며 처리\n",
    "        x = tokenizer(row.review) # 행의 review를 토크나이저에 입력\n",
    "        masked, labels = mask_inputs(x['input_ids']) # 리뷰의 input_ids를 지정 함수에 입력\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'input_ids': int_feature(masked), # 마스킹된 ids를 입력\n",
    "            'token_type_ids': int_feature(x['token_type_ids']), # 원데이터 그대로 입력\n",
    "            'attention_mask': int_feature(x['attention_mask']), # 원데이터 그대로 입력\n",
    "            'labels': int_feature(labels) # labels를 입력\n",
    "        }))\n",
    "        s = example.SerializeToString() # Serialize 진행\n",
    "        writer.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT 미세조정 Ⅰ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForMaskedLM, BertTokenizerFast, FillMaskPipeline\n",
    "\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터 불러오기\n",
    "int_seq = tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True) # 전처리된 데이터의 모든 형태가 동일하므로 변수로 대입\n",
    "feature_description = { #feature_description 지정\n",
    "    'input_ids': int_seq,\n",
    "    'token_type_ids': int_seq,\n",
    "    'attention_mask': int_seq,\n",
    "    'labels': int_seq,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 feature_description 을 불러오는 함수 지정\n",
    "def preproc(example):\n",
    "    return tf.io.parse_single_example(example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord를 .map을 통해 위의 함수를 지정해, TFRecord의 모든 example에 적용\n",
    "# 문장 32개씩 묶어 하나의 배치로 처리\n",
    "dataset = tf.data.TFRecordDataset(['bert_train.tfrecord']).map(preproc).padded_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acda11906f9f4b4588d8b9795077d1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습\n",
    "import tqdm.notebook\n",
    "import math\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "for batch in tqdm.notebook.tqdm(dataset, total=math.ceil(1000 / 32)): # 순환을 통해 모델에 배치를 하나씩 넣어줌 # 1000개의 문장을 32개의 batch로 처리 (32개를 처리하면 1 epoch)\n",
    "    with tf.GradientTape() as tape:\n",
    "        result = model(batch) # result에 손실값 有, label에서 -100이라고 된 부분은 무시하고 0 이상의 숫자가 들어간 부분만 손실을 계산\n",
    "                              # 32개의 문장에서 655개의 손실값 계산\n",
    "        loss = tf.reduce_mean(result['loss']) # 손실값에 대한 평균 계산\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # 모델의 파라미터를 어떤 방향으로 계산해야 손실이 줄어드는지 계산\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables)) # 경사를 적용 (학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모형으로 빈 칸 채우기\n",
    "pipeline = FillMaskPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08684153854846954,\n",
       "  'token': 0,\n",
       "  'token_str': '[PAD]',\n",
       "  'sequence': 'this is very interesting.'},\n",
       " {'score': 0.005911157000809908,\n",
       "  'token': 26937,\n",
       "  'token_str': 'jonny',\n",
       "  'sequence': 'this is very interesting jonny.'},\n",
       " {'score': 0.005225041881203651,\n",
       "  'token': 25262,\n",
       "  'token_str': 'environmentally',\n",
       "  'sequence': 'this is very interesting environmentally.'},\n",
       " {'score': 0.003507291665300727,\n",
       "  'token': 24360,\n",
       "  'token_str': '##nting',\n",
       "  'sequence': 'this is very interestingnting.'},\n",
       " {'score': 0.0026332736015319824,\n",
       "  'token': 5456,\n",
       "  'token_str': 'discovery',\n",
       "  'sequence': 'this is very interesting discovery.'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline('This is very interesting [MASK].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT 미세조정 Ⅱ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForMaskedLM, BertTokenizerFast, FillMaskPipeline\n",
    "\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.models.bert.modeling_tf_bert.TFBertForMaskedLM,\n",
       " transformers.models.bert.modeling_tf_bert.TFBertPreTrainedModel,\n",
       " transformers.modeling_tf_utils.TFPreTrainedModel,\n",
       " keras.src.engine.training.Model,\n",
       " keras.src.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.trackable.autotrackable.AutoTrackable,\n",
       " tensorflow.python.trackable.base.Trackable,\n",
       " keras.src.utils.version_utils.LayerVersionSelector,\n",
       " keras.src.utils.version_utils.ModelVersionSelector,\n",
       " transformers.modeling_tf_utils.TFModelUtilsMixin,\n",
       " transformers.generation.tf_utils.TFGenerationMixin,\n",
       " transformers.utils.hub.PushToHubMixin,\n",
       " transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss,\n",
       " transformers.modeling_tf_utils.TFCausalLanguageModelingLoss,\n",
       " object]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 class의 상위 class 확인\n",
    "# TFBertForMaskedLM은 본인인 동시에 어떤 모델인가?\n",
    "# 동시에 tensorflow.python.keras.engine.training.Model 이기 때문에 .fit을 통해 학습 가능\n",
    "\n",
    "model.__class__.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터 불러오기\n",
    "int_seq = tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True)\n",
    "feature_description = {\n",
    "    'input_ids': int_seq,\n",
    "    'token_type_ids': int_seq,\n",
    "    'attention_mask': int_seq,\n",
    "    'labels': int_seq,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(example):\n",
    "    x = tf.io.parse_single_example(example, feature_description)\n",
    "    y = x.pop('labels') # y를 x에서 추출 (x에선 사라짐)\n",
    "    y = tf.cast(y, tf.int32) # y의 데이터 타입을 정수형으로 변경 (BERT 모델의 출력이 32비트)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(['bert_train.tfrecord']).map(preproc).padded_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "              loss=model.compute_loss, # 모델이 계산하는 손실함수\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset) #.fit 함수를 통해 편하게 학습 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모형으로 빈 칸 채우기\n",
    "pipeline = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "pipeline('This is very interesting [MASK].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 감성 분석을 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "nsmc = pd.read_csv('./data/ratings_train.txt', sep='\\t')\n",
    "nsmc.head()\n",
    "# 네이버 영화평 감성분석 데이터 (tap으로 구분된 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할\n",
    "small = nsmc.iloc[:10000]\n",
    "# train data와 test 데이터 분할을 위해 분할\n",
    "# 15만건의 큰 데이터이기 때문에도 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx_train, idx_test = train_test_split(small.index, test_size=0.2, random_state=42)\n",
    "# 2000개 test, 8000개 train 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord로 저장\n",
    "def int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_nsmc.tfrecord'\n",
    "df = small.loc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tfrecord 저장\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for row in df.itertuples():\n",
    "        x = tokenizer(row.document)\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'input_ids': int_feature(x['input_ids']),\n",
    "            'token_type_ids': int_feature(x['token_type_ids']),\n",
    "            'attention_mask': int_feature(x['attention_mask']),\n",
    "            'labels': int_feature([row.label])\n",
    "        }))\n",
    "        s = example.SerializeToString()\n",
    "        writer.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tfrecord 저장\n",
    "filename = 'train_nsmc.tfrecord'\n",
    "df = small.loc[idx_train]\n",
    "\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for row in df.itertuples():\n",
    "        x = tokenizer(row.document) # 행의 리뷰 내용을 토크나이저에 입력\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={ # x를 출력해보면 input_ids, oken_type_ids, attention_mask 가 나옴\n",
    "            'input_ids': int_feature(x['input_ids']), # 원래 리스트 형태이기 때문에 그대로 입력 (이하 동일)\n",
    "            'token_type_ids': int_feature(x['token_type_ids']),\n",
    "            'attention_mask': int_feature(x['attention_mask']),\n",
    "            'labels': int_feature([row.label]) # 원데이터의 label 사용, row.label은 정수 하나이기 때문에 반드시 리스트 형태로 감싸서 넣어줘야 함\n",
    "        }))\n",
    "        s = example.SerializeToString()\n",
    "        writer.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 감성분석 미세조정 Ⅰ (경사하강법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 연속으로 구성된 긍부정 분류 (=문장) : SequenceClassification\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('beomi/kcbert-base', from_pt=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "int_seq = tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True)\n",
    "int_value = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "feature_description = {\n",
    "    'input_ids': int_seq,\n",
    "    'token_type_ids': int_seq,\n",
    "    'attention_mask': int_seq,\n",
    "    'labels': int_value # 위의 셋은 토큰화되서 문장을 쪼개 놓기 때문에 길이가 있지만, label은 0 아니면 1이기 때문에 길이가 없어서 LenFeature를 통해 정의\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(example):\n",
    "    return tf.io.parse_single_example(example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(['train_nsmc.tfrecord']).map(preproc).padded_batch(32)\n",
    "test_dataset = tf.data.TFRecordDataset(['test_nsmc.tfrecord']).map(preproc).padded_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': <tf.Tensor: shape=(32, 74), dtype=int64, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>,\n",
       " 'input_ids': <tf.Tensor: shape=(32, 74), dtype=int64, numpy=\n",
       " array([[    2,  9376, 12746, ...,     0,     0,     0],\n",
       "        [    2,  7992,  2500, ...,     0,     0,     0],\n",
       "        [    2, 15801,    95, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 28684,  1594, ...,     0,     0,     0],\n",
       "        [    2,  2878, 29809, ...,     0,     0,     0],\n",
       "        [    2,  2346,  4227, ...,     0,     0,     0]], dtype=int64)>,\n",
       " 'labels': <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       " array([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int64)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(32, 74), dtype=int64, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)>}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_dataset)) # Serialize 된 상태라 parsing을 해줘야 함, 위의 식에서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': <tf.Tensor: shape=(32, 74), dtype=int64, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>,\n",
       " 'input_ids': <tf.Tensor: shape=(32, 74), dtype=int64, numpy=\n",
       " array([[    2,  9376, 12746, ...,     0,     0,     0],\n",
       "        [    2,  7992,  2500, ...,     0,     0,     0],\n",
       "        [    2, 15801,    95, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 28684,  1594, ...,     0,     0,     0],\n",
       "        [    2,  2878, 29809, ...,     0,     0,     0],\n",
       "        [    2,  2346,  4227, ...,     0,     0,     0]], dtype=int64)>,\n",
       " 'labels': <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       " array([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int64)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(32, 74), dtype=int64, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)>}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(test_dataset))\n",
    "batch # 문장 32개가 묶임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba0272248b54d0ea235f76944170779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.469"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 전 성능 테스트\n",
    "import tqdm.notebook\n",
    "import math\n",
    "\n",
    "a = 0\n",
    "for batch in tqdm.notebook.tqdm(test_dataset, total=math.ceil(2000 / 32)):\n",
    "    result = model(batch)\n",
    "    equals = tf.math.argmax(result['logits'], axis=1) == batch['labels'] # logits이 큰 쪽으로 예측 (0 혹은 1) # 모델을 통해 나온 예측값과 batch의 label(정답)과 비교\n",
    "    equals = equals.numpy() # equals.sum()/len(equals) = 배치에 들어있는 문장 중 맞춘 개수\n",
    "    a += equals.sum()\n",
    "    \n",
    "a / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a88f9a186e468c99a60973900b8b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "for batch in tqdm.notebook.tqdm(train_dataset, total=math.ceil(8000/32)):\n",
    "    with tf.GradientTape() as tape:\n",
    "        result = model(batch)\n",
    "        loss = tf.reduce_mean(result['loss'])\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48047c16cf514f54b53eb058b7ef87f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8585"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 후 성능 테스트\n",
    "a = 0\n",
    "for batch in tqdm.notebook.tqdm(test_dataset, total=math.ceil(2000 / 32)):\n",
    "    result = model(batch)\n",
    "    equals = tf.math.argmax(result['logits'], axis=1) == batch['labels']\n",
    "    equals = equals.numpy()\n",
    "    a += equals.sum()\n",
    "a / 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 감성분석 미세조정 Ⅱ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('beomi/kcbert-base', from_pt=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification,\n",
       " transformers.models.bert.modeling_tf_bert.TFBertPreTrainedModel,\n",
       " transformers.modeling_tf_utils.TFPreTrainedModel,\n",
       " keras.src.engine.training.Model,\n",
       " keras.src.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.trackable.autotrackable.AutoTrackable,\n",
       " tensorflow.python.trackable.base.Trackable,\n",
       " keras.src.utils.version_utils.LayerVersionSelector,\n",
       " keras.src.utils.version_utils.ModelVersionSelector,\n",
       " transformers.modeling_tf_utils.TFModelUtilsMixin,\n",
       " transformers.generation.tf_utils.TFGenerationMixin,\n",
       " transformers.utils.hub.PushToHubMixin,\n",
       " transformers.modeling_tf_utils.TFSequenceClassificationLoss,\n",
       " object]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.mro()\n",
    "# 상위 class 중  tensorflow.python.keras.engine.training.Model 有\n",
    "# fit 함수 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "int_seq = tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True)\n",
    "int_value = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "feature_description = {\n",
    "    'input_ids': int_seq,\n",
    "    'token_type_ids': int_seq,\n",
    "    'attention_mask': int_seq,\n",
    "    'labels': int_value\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(example): # 전처리 함수\n",
    "    x = tf.io.parse_single_example(example, feature_description)\n",
    "    y = x.pop('labels')\n",
    "    return x, tf.cast(y, tf.int32) # y를 32비트로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(['train_nsmc.tfrecord']).map(preproc).padded_batch(32)\n",
    "test_dataset = tf.data.TFRecordDataset(['test_nsmc.tfrecord']).map(preproc).padded_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 설정\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), loss=model.compute_loss, metrics=['accuracy'])\n",
    "# model compile, 손실함수는 모델이 자체적으로 갖고 있는 손실함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전 성능 테스트\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(train_dataset)\n",
    "# 일일이 기울기 구하고, optimizer 적용 필요 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 후 성능 테스트\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 질의 응답 실습 (QA Task를 BERT를 통해 실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa = pipeline('question-answering') # question-answering 모델을 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Youn Yuh-jung (born June 19, 1947) is a South Korean actress, whose career in film and television spans over five decades. \n",
    "Her accolades include an Academy Award.\n",
    "\"\"\"\n",
    "# 문장 입력, \"\"\" \"\"\" 을 통해 여러 줄로 된 텍스트를 입력할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7455781698226929, 'start': 54, 'end': 61, 'answer': 'actress'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question에 질문을 입력하고, context를 통해 텍스트를 입력\n",
    "# 위 텍스트에서 직업에 해당하는 위치를 보여줌\n",
    "# 54-61 : 글자의 위치\n",
    "# score = 확률을 통해 계산한 정답일 가능성\n",
    "# job 이라는 단어가 없다 = 문구가 똑같지 않더라도 텍스트에서 필요한 부분을 추출해서 답해줌\n",
    "qa(question=\"What is her job?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6411082744598389,\n",
       " 'start': 150,\n",
       " 'end': 163,\n",
       " 'answer': 'Academy Award'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"What did she win?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9665399789810181,\n",
       " 'start': 21,\n",
       " 'end': 34,\n",
       " 'answer': 'June 19, 1947'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"When was she born?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형을 지정해서 pipeline 만들기\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, QuestionAnsweringPipeline\n",
    "# AutoTokenizer나 AutoModel은 사전학습모델을 호출할 때, 모델이 어떤 class를 사용하고 있는지 정보가 있기 때문에, BERT, GPT2 등을 지정하지 않아도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 미세조정 되지 않은 모형\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Auto 지만 BERT인 것을 알고 있기 때문에 BERTTokenizer를 호출했어도 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline에 모델과 토크나이저를 입력\n",
    "# BERT 기본 모형이기 때문에 QA를 할 수 있도록 training 되어 있지 않음\n",
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.002011405536904931, 'start': 54, 'end': 62, 'answer': 'actress,'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"What is her job?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0020648217760026455, 'start': 54, 'end': 62, 'answer': 'actress,'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"What did she win?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0019660096149891615, 'start': 54, 'end': 62, 'answer': 'actress,'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트레이닝이 되어 있지 않은 모델을 사용하면 전혀 답을 하지 못함\n",
    "qa(question=\"When was she born?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 미세조정 된 모형\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "# BERT를 압축시켜 놓고(distiled), 작은 (base), 대소문자 구분한 (cased), 스탠포드 대학에서 만든 QA dataset (squad)\n",
    "# BERT 모형을 squad data에 미세조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7455788850784302, 'start': 54, 'end': 61, 'answer': 'actress'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"What is her job?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6411080956459045,\n",
       " 'start': 150,\n",
       " 'end': 163,\n",
       " 'answer': 'Academy Award'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"What did she win?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.966539740562439, 'start': 21, 'end': 34, 'answer': 'June 19, 1947'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question=\"When was she born?\", context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 48), dtype=int32, numpy=\n",
       "array([[  101,  1327,  1225,  1131,  1782,   136,   102,  1192,  1179,\n",
       "        10684,  1324,   118,   179,  4380,   113,  1255,  1340,  1627,\n",
       "          117,  3138,   114,  1110,   170,  1375,  3947,  3647,   117,\n",
       "         2133,  1578,  1107,  1273,  1105,  1778, 15533,  1166,  1421,\n",
       "         4397,   119,  1430,   170, 14566, 19872,  1511,  1126,  2127,\n",
       "         1698,   119,   102]])>, 'attention_mask': <tf.Tensor: shape=(1, 48), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수동 (미세조정을 위한)\n",
    "import tensorflow as tf\n",
    "\n",
    "# 질문 생성\n",
    "# 토크나이저에 질문을 입력 (question과 context의 순서는 사실 무관 = BERT 모형의 특성)\n",
    "# 모델 자체가 Q-A 순으로 학습되어 있기 때문에 토크나이징할 때, Q-A 순으로 입력\n",
    "# add_special_tokens\n",
    "question = 'What did she win?'\n",
    "inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"tf\")\n",
    "\n",
    "# 101 = CLS 토큰\n",
    "# 102 = SEP 토큰\n",
    "# 48개의 토큰\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What did she win? [SEP] Youn Yuh - jung ( born June 19, 1947 ) is a South Korean actress, whose career in film and television spans over five decades. Her accolades include an Academy Award. [SEP]'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 그대로 모델에 넣으면 됨\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 48), dtype=float32, numpy=\n",
       "array([[-3.2587407 , -4.115965  , -6.1125135 , -5.7539034 , -6.0889006 ,\n",
       "        -3.914919  , -4.3926554 , -2.0310116 , -5.905585  , -3.8264377 ,\n",
       "        -6.541384  , -6.294675  , -5.6275797 , -5.9331894 , -5.194643  ,\n",
       "        -3.7380962 , -2.697707  , -3.7542107 , -5.712838  , -2.1804962 ,\n",
       "        -4.863432  , -4.760373  , -3.570895  , -0.47888386, -2.6021426 ,\n",
       "        -1.3134253 , -6.2036495 , -5.035602  , -4.1924896 , -6.55541   ,\n",
       "        -2.4696975 , -7.3524957 , -3.1218536 , -5.6631937 , -4.4715004 ,\n",
       "        -3.9809253 , -5.159875  , -2.9622166 ,  2.0888474 , -0.665982  ,\n",
       "        -5.286133  , -4.834054  , -2.0916827 ,  8.230597  ,  8.933442  ,\n",
       "         1.8657247 , -2.8914907 , -4.3925533 ]], dtype=float32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력한 48개의 토큰 중, softmax 함수에 들어가면 확률로 변환되는데, 질문의 답변이 어느 곳에 시작할 것인가?\n",
    "# 값이 높을수록 답변이 시작하는 곳일 확률이 높음\n",
    "outputs.start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 48), dtype=float32, numpy=\n",
       "array([[-0.7095605 , -3.614283  , -6.2600956 , -5.9759374 , -6.0352654 ,\n",
       "        -4.777353  , -4.5723333 , -4.9856696 , -4.3647704 , -4.8074465 ,\n",
       "        -4.402199  , -5.2577233 , -5.4835253 , -1.9583439 , -4.912096  ,\n",
       "        -4.9395914 , -4.1950364 , -3.3444967 , -3.7767985 , -1.870179  ,\n",
       "        -1.4572221 , -5.2039084 , -5.880264  , -4.244818  ,  0.21101947,\n",
       "        -0.7073603 , -1.2311841 , -5.6432405 , -4.7058434 , -7.4447846 ,\n",
       "        -4.0362253 , -7.16896   , -2.56027   , -6.0628514 , -6.265037  ,\n",
       "        -4.800591  , -1.379861  ,  0.35461897, -3.190801  , -6.381058  ,\n",
       "        -5.4437137 , -1.0014491 , -3.896157  , -0.3924904 ,  3.264592  ,\n",
       "         9.571489  ,  6.337932  , -4.5721397 ]], dtype=float32)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어디서 답변이 끝날 것인가?\n",
    "outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 위치의 확률 중 가장 높은 곳을 확인\n",
    "# axis = 0, 행을 바꿔가며 가장 큰 값이 있는 행을 찾음\n",
    "# 열을 찾아야함 -> 이후 결과값을 numpy 형식으로 바꾼 뒤 [0]을 통해 위치에 해당하는 숫자만 출력\n",
    "start = tf.argmax(outputs.start_logits, axis=1).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 동일, 범위 지정에 있어 끝나는 토큰을 포함시키기 위해 +1을 해줌\n",
    "end = tf.argmax(outputs.end_logits, axis=1).numpy()[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Academy Award'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0번 행에서 시작 위치부터 끝나는 위치까지 뽑아서 디코드를 진행해 답변을 문장으로 바꿔줌\n",
    "tokenizer.decode(inputs['input_ids'][0, start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 호출 (Pytorch 버젼의 모델)\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, QuestionAnsweringPipeline\n",
    "import tensorflow as tf\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained('beomi/kcbert-base', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 호출\n",
    "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '윤여정은 1966년 연극 배우로 연기 경력을 시작하였고, 2021년 영화 《미나리》의 순자 역으로 아카데미 여우조연상을 수상했다.'\n",
    "question = '윤여정의 직업은?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= tokenizer(context, question, return_tensors='tf') # 원활한 전처리를 위해 context-question 순으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 윤여정은 1966년 연극 배우로 연기 경력을 시작하였고, 2021년 영화 [UNK] 미'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "start = tf.argmax(result.start_logits, axis=1).numpy()[0]\n",
    "end = tf.argmax(result.end_logits, axis=1).numpy()[0] + 1\n",
    "\n",
    "tokenizer.decode(inputs['input_ids'][0, start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.004640113562345505,\n",
       " 'start': 21,\n",
       " 'end': 45,\n",
       " 'answer': '경력을 시작하였고, 2021년 영화 《미나리'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 모형은 QA training이 안 되어 있다\n",
    "# 위와 동일하게 엉망\n",
    "qa(question='윤여정의 직업은?', context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KorQUAD 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('korquad.json', <http.client.HTTPMessage at 0x1f252bcf160>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 QA Dataset인 KorQUAD를 학습\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://korquad.github.io/dataset/KorQuAD_v1.0_train.json', 'korquad.json')\n",
    "# KorQUAD 사이트에서 링크 주소 복사를 한 뒤, urlib.request 모듈을 통해 해당 복사 주소를 다운받을 수 있으며, 다운받을 파일 이름 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 형식의 파일을 읽기 위해 import\n",
    "import json\n",
    "\n",
    "# jsom 파일을 읽기\n",
    "korquad = json.load(open('korquad.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paragraphs', 'title'])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary 형태로 되어 있는데, version과 data로 구성\n",
    "# data는 리스트 형태로 되어있기 때문에 [0] 을 통해 첫 번째 예제가 나옴\n",
    "# paragraphs 를 통해 문단의 내용 (context) 과 질문 (qas) 과 답 (answers)으로 구성되어 있음\n",
    "korquad['data'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'파우스트_서곡'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korquad['data'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = korquad['data'][0]\n",
    "para = item['paragraphs'][0]\n",
    "para['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': [{'text': '교향곡', 'answer_start': 54}],\n",
       " 'id': '6566495-0-0',\n",
       " 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 답변의 시작 위치가 나옴\n",
    "# 전처리 시, 질문을 앞에 놓을 시, 시작 위치에 질문의 길이를 더해줘야 하기 때문에, 간단하기 진행하기 위해 질문을 뒤에 넣고 전처리\n",
    "qas = para['qas'][0]\n",
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'교향곡'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas['answers'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # KcBERT가 300개의 토큰까지만 처리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    para['context'], \n",
    "    qas['question'], \n",
    "    truncation=True, # 길이가 길면 자름\n",
    "    max_length=tokenizer.model_max_length, # 길이를 지정해주지 않아도 내부적으로 설정되어 있지만, max_length를 통해 최대 길이를 넘어가면 자름 \n",
    "    return_tensors='tf')\n",
    "# 올바르게 처리하려면, context가 너무 길면, context의 길이를 자르던가\n",
    "# question을 앞으로 놓고, answer의 start 부분을 처리해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 300), dtype=int32, numpy=\n",
       "array([[    2,  8601,  4633, 29697,  1480,  4313,  4538,  4008,   336,\n",
       "         4065,  4042,  3231, 23243,  4104,  4027,  8793, 13985,   391,\n",
       "         9132,  4113, 10966, 11728, 12023, 14657,  4091,  8598, 16639,\n",
       "          341,  4573,  4771,  4027,  2139,  8478, 14416,   214,  8202,\n",
       "           17,  2451, 13007,  1480,  4313,  4538,  4008,  8601,  4633,\n",
       "        22903,  4113,  1676,   868,  4913,  7965,  1789,  4203,  4110,\n",
       "        15031,   786,   250,  4057, 10878,  4007,  2593,  4094,  4128,\n",
       "        10289,  4113, 10958,  4062,  9511,  1355,  4600,  4103,  4775,\n",
       "         5602, 10770,  4180, 26732,  3231, 23243,  4104,  4042,  2015,\n",
       "         4012,  4113,  9198,  8763,  8129,    17, 10384, 23008,  7971,\n",
       "         2170,  4408,  4011,  4147,  4042, 17015,  4091, 23008, 21056,\n",
       "         4165,   323,  4175,  4158, 11413,  2273,  4043,  7966,  1543,\n",
       "         4775,  4170,  4042,   341,  4573,  4771,    28,  4566,  4027,\n",
       "        10599, 18907,   208,  9504, 24835,    15, 11060,  2451,  4780,\n",
       "         4032, 18548,  4113,  3231, 23243,  4104,  4042,  1843,  4771,\n",
       "         7965, 28987,  4153,  2451, 15489,  4113, 13928, 17283,   575,\n",
       "         4261, 26783,  8114,  8852,  9107,  4082, 28498,  8131,    17,\n",
       "         8225,  4042,  1114,  4281,  4194, 17138,  4042,  9961,  8222,\n",
       "        14041, 10892,  4113,  2524,  4443,  8032, 12710, 21602, 18625,\n",
       "        24569,  4136,  2009, 11012, 17068,  4130,  8544,  8141,  1543,\n",
       "         4775,  4170,  4042,  3367,  4183,  4267,  4573,  4771, 17138,\n",
       "         4042, 17283,  9838,  9153,  1576,  1931,  8120,    17,  8117,\n",
       "          341,  4573,  4771,  2478,  4771,  4027,  8601,  4633, 29697,\n",
       "         8042, 25305,  4113,   254,  4133, 23008,  7971,  2840,  4110,\n",
       "         4062, 10410,    20,  4158,  8915,  2141, 11726, 13014,  8258,\n",
       "           17, 10384, 15489,  4042, 16520,  4128, 15831, 20048,  2451,\n",
       "         1843,  4771,    11,    20,  4158,  4099,    12,  2424, 23008,\n",
       "        21056, 18639,  2273,  4043, 27703,  2273,  4043,  4082,  3231,\n",
       "         4104,  4010,  7999,  9242, 22504, 10410,    15, 26271, 11877,\n",
       "         8840,  4008, 12626,    17,  8340,  2906,  4132,  4057, 16243,\n",
       "         1483,  4017,  9403, 14180,   947, 10770,  5076,  7971,  2273,\n",
       "         4043, 24984,     3,  1480,  4313,  4538,  4008,   336,  4065,\n",
       "         4042,  3231, 23243, 19143, 13985, 12449,  9194,  4105,  3385,\n",
       "         9411,    32,     3]])>, 'token_type_ids': <tf.Tensor: shape=(1, 300), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>, 'attention_mask': <tf.Tensor: shape=(1, 300), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습을 위해서 시작 토큰과 끝나는 토큰 위치를 추가해야 함\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qas['answers'][0]\n",
    "start_char = q['answer_start'] # 답변의 시작 부분\n",
    "end_char = start_char + len(q['text']) - 1 # 답변의 끝 부분 (모델에 넣기 위해서는 -1 을 해야함)\n",
    "start = inputs.char_to_token(0, start_char) \n",
    "# 현재는 글자의 위치이므로 토큰의 위치로 변환해줘야함 (준단어 토큰화를 했기 때문에)\n",
    "# 위에서 나오는 번호의 글자에 해당하는 토큰이 몇 번째 토큰인가 계산\n",
    "end = inputs.char_to_token(0, end_char)\n",
    "# 위와 동일한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'교향곡'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'].numpy()[0, start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['start_positions'] = [start] # 시작 위치 지정(리스트 형태 = tfrecord 형태로 저장할 때, 저장이 편하기 위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 예시를 처리\n",
    "# 모든 예시에 대해 처리한 것을 tfrecord 형태로 만들어주기 위해 반복을 위한 함수를 만듦\n",
    "inputs['end_positions'] = [end] # 끝 위치 지정(리스트 형태)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inputs(context, qas):\n",
    "    inputs = tokenizer(\n",
    "        context, \n",
    "        qas['question'], \n",
    "        truncation=True, \n",
    "        max_length=tokenizer.model_max_length)\n",
    "    q = qas['answers'][0]\n",
    "    start_char = q['answer_start']\n",
    "    end_char = start_char + len(q['text']) - 1\n",
    "    start = inputs.char_to_token(0, start_char)\n",
    "    end = inputs.char_to_token(0, end_char)\n",
    "    inputs['start_positions'] = [start]\n",
    "    inputs['end_positions'] = [end]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "filename = 'korquad.tfrecord'\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for item in korquad['data']:\n",
    "        for para in item['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qas in para['qas']:\n",
    "                inputs = make_inputs(context, qas)\n",
    "                if inputs['start_positions'][0] and inputs['end_positions'][0]: \n",
    "                # 300 토큰이 넘으면 truncate 되기 때문에 start와 end position이 input 안에 포함되어 있을 때만 저장\n",
    "                    feature = {k: int_feature(v) for k, v in inputs.items()} # inputs가 dictionary이기 때문에, dictionary의 모든 값에 대해 적용\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                    s = example.SerializeToString() # Example을 문자로 변환\n",
    "                    writer.write(s)\n",
    "                    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57653"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사례의 수\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 QA 미세조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "int_seq = tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True)\n",
    "int_value = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "feature_description = {\n",
    "    'input_ids': int_seq,\n",
    "    'token_type_ids': int_seq,\n",
    "    'attention_mask': int_seq,\n",
    "    'start_positions': int_value,\n",
    "    'end_positions': int_value\n",
    "}\n",
    "# 토큰은 sequence 로, start와 end position 은 value 로 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 만들기\n",
    "def preproc(example):\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example = {k : tf.cast(v, tf.int32) for k, v in example.items()} # 64비트 정수로 저장되어 있는 것을 32비트로 변경\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOM 발생 시, 문장 길이를 줄이던지, batch size를 줄이던지 해야함\n",
    "dataset = tf.data.TFRecordDataset(['korquad.tfrecord']).map(preproc).padded_batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미세조정\n",
    "import tqdm.notebook\n",
    "import math\n",
    "\n",
    "n = 57653\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-5) # 학습률을 매우 낮게 지정\n",
    "\n",
    "for batch in tqdm.notebook.tqdm(dataset, total=math.ceil(n / 8)): # dataset에서 batch를 하나씩 가져옴\n",
    "    with tf.GradientTape() as tape: \n",
    "        result = model(batch) # 모델에 8개 example을 동시에 넣어서 계산\n",
    "        loss = tf.reduce_mean(result['loss']) # loss의 평균을 도출\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # 손실값과 파라미터의 경사를 구해 손실이 줄어드는 방향을 구함\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables)) # 손실을 줄어드는 방향으로 파라미터 업데이트\n",
    "\n",
    "model.save_pretrained('./model/bert/kcbert-base-korquad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at ./model/bert/kcbert-base-korquad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 호출 (저장된)\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained('./model/bert/kcbert-base-korquad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용해보기\n",
    "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '윤여정은 1966년 연극 배우로 연기 경력을 시작하였고, 2021년 영화 《미나리》의 순자 역으로 아카데미 여우조연상을 수상했다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(question, context):\n",
    "    inputs = tokenizer(context, question, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    start = tf.argmax(outputs.start_logits, axis=1).numpy()[0]\n",
    "    end = tf.argmax(outputs.end_logits, axis=1).numpy()[0]\n",
    "    return tokenizer.decode(inputs['input_ids'][0, start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021년'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('윤여정은 언제 데뷔했나?', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'연극 배우'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('윤여정의 직업은?', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아카데미 여우조연상을'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('윤여정이 받은 상은?', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미나리'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('윤여정의 출역작은?', context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 02. GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab과 Google Drive 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, TFGPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "# GPT2LMHeadModel = GPT2를 이용해 다음 단어를 예측하는 언어 모델\n",
    "# TFGPT2LMHeadModel = Tensorflow용 버전, 위는 PyTorch용 버전; 사용하는 라이브러리에 맞게 하나만 사용\n",
    "# PreTrainedTokenizerFast = 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e7a7eb217b470689b15ea3ae90cf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucky\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68b9eb81ba441bbb583e2886a0e84a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PyTorch 모형 로딩\n",
    "model = GPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\") # 모델 이름 지정\n",
    "# PC는 지정 폴더 저장이라, 한 번 저장 시 재다운로드 X, Colab은 Google Drive 연동을 통해 필요할 때 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7dec2c9ca64b47902b8c3dd2614eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.2.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'lm_head.weight', 'transformer.h.4.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.6.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 모형 로딩. 원본 모형이 PyTorch 용이므로 form_pt=True 로 설정\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\", from_pt=True)\n",
    "# model card에서 전용 라이브러리를 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa3002e2a724dab8439b63aa1e63223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a56e000f11429687382f5d36a94652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dce3f1ede504ab2a8ee6e2d1823e54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로딩\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"taeminlee/kogpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Drive에 모형 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 드라이브 연동\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git을 이용해 모형 복제\n",
    "!sudo apt install git-lfs\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/taeminlee/kogpt2 /gdrive/MyDrive/kogpt2/taeminlee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 및 토크나이저 로딩\n",
    "model = GPT2LMHeadModel.from_pretrained(\"/gdrive/MyDrive/kogpt2/taeminlee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"/gdrive/MyDrive/kogpt2/taeminlee\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/gdrive/MyDrive/kogpt2/taeminlee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Flow 모형 사용 시, return_tensors='pt'\n",
    "# PyTorch 모형 사용 시, return_tensors='pt'\n",
    "input_ids = tokenizer.encode('자연어 처리를', add_special_tokens=False, \n",
    "                             return_tensors=\"tf\")\n",
    "# 특수 토큰은 붙히지 않음 (add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id)\n",
    "# eos_token = 문장의 끝을 나타내는 토큰\n",
    "# pad_token = 여러 문장의 길이를 맞출 때, 빈칸 대용으로 사용하는 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자연어 처리를 할 수 있다.</s><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자동 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자동 미분을 통해 경사하강법을 직접 적용하는 방법\n",
    "# 딥러닝은 경사하강법을 통해 업데이트를 진행하는데, 함수의 기울어진 정도(경사)를 구하기 위해 미분을 진행한다\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "# tensor의 특별한 형태 Variable, tensor flow 내부에서 변수에 해당하는 형태\n",
    "# x 라는 변수는 3.0이라는 값을 갖는다\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2\n",
    "# y 는 x의 제곱이 되며, 자동미분을 위해 GradientTape 구문 안에서 진행한다\n",
    "# with 는 특정 처리를 특정 맥락에서 진행하기 위해 사용한다\n",
    "# 계산기록을 tape라는 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y를 x로 미분\n",
    "# x가 변화할 때, y가 얼마나 변화하는가? 순간적인 기울기 계산\n",
    "grad = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산 결과를, 텐서 플로우 혹은 딥 러닝, 머신 러닝 모형을 만들었을 때 경사를 적용해줘야 함\n",
    "# 오차 혹은 손실을 계산하여 최소화하는 시점을 찾아야 함\n",
    "# y를 가장 작게 만들어주는 지점을 찾아주는 경사하강법 알고리즘 (Adam, SGD 등)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존에 구한 경사를 적용, 리스트를 만들어서 함수에 넣어줌\n",
    "# 기존에 구한 경사를 x에 적용해 x를 업데이트\n",
    "optimizer.apply_gradients([(grad, x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.9000008>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.0 에서 2.9로 줄어듦\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.410005>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.0 에서 8.41로 줄어듦\n",
    "# y가 줄어드는 방향으로 x를 변화시킨 것\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8001046 8.410005\n",
      "2.700384 7.8405857\n",
      "2.6009169 7.2920732\n",
      "2.5017836 6.7647686\n",
      "2.4030683 6.258921\n",
      "2.3048592 5.7747374\n",
      "2.2072473 5.3123755\n",
      "2.110327 4.8719406\n",
      "2.014196 4.4534802\n",
      "1.9189541 4.0569854\n",
      "1.8247049 3.682385\n",
      "1.7315532 3.329548\n",
      "1.6396064 2.9982765\n",
      "1.5489731 2.688309\n",
      "1.4597634 2.3993175\n",
      "1.372088 2.1309092\n",
      "1.2860576 1.8826253\n",
      "1.2017828 1.6539441\n",
      "1.1193731 1.4442819\n",
      "1.0389359 1.2529961\n",
      "0.9605769 1.0793878\n",
      "0.88439816 0.922708\n",
      "0.8104983 0.7821601\n",
      "0.7389711 0.6569075\n",
      "0.669905 0.5460783\n",
      "0.6033824 0.44877273\n",
      "0.53947866 0.36407033\n",
      "0.47826162 0.29103723\n",
      "0.41979086 0.22873418\n",
      "0.36411703 0.17622437\n"
     ]
    }
   ],
   "source": [
    "# old_y = 100 # 큰 값으로 지정\n",
    "# new_y = 10\n",
    "# while abs(old_y - new_y) > 0.1:\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         y = x ** 2\n",
    "#     grad = tape.gradient(y, x)\n",
    "#     optimizer.apply_gradients([(grad, x)])\n",
    "# 위와 같은 방식으로 코딩 가능\n",
    "\n",
    "for i in range(30):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x ** 2\n",
    "    grad = tape.gradient(y, x)\n",
    "    optimizer.apply_gradients([(grad, x)])\n",
    "    print(x.numpy(), y.numpy())\n",
    "# 같은 방식으로 지속적으로 x를 바꿔나가면서 y가 최소화 될 때까지 반복문 진행\n",
    "# y 계산 → 경사 구하기 → 경사를 x에 적용\n",
    "# .numpy를 통해 결과값을 숫자값만 표시\n",
    "# 지속적인 반복을 통해 최솟점 근처로 가게 되면, y가 상승할 수도 있음\n",
    "# y에 실제 딥러닝 모델을 집어넣게 되면 학습 진행 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor flow의 Data set 관리 기능\n",
    "# Data를 갖고 여러가지 처리를 할 수 있는 기능\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# Dataset 이후 함수로 여러 방식으로 만들 수 있음\n",
    "# 기존 tensor의 8건의 데이터를 하나의 Data set으로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# for문을 통해 각 값들을 하나 하나의 batch로 처리 가능\n",
    "for batch in dataset:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([5 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor([7 8], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# batch를 2개씩 묶어서 처리\n",
    "for batch in dataset.batch(2):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# batch를 4개씩 묶어서 처리\n",
    "for batch in dataset.batch(4):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set을 list 등의 고정된 데이터 셋이 아니라 generator로 저장 가능\n",
    "# for i in range(10): 이라는 함수를 실행할 때, 0부터 9까지 숫자들의 sequence를 만들어주는 역할을 함\n",
    "# 이와 range(10)과 list(range(10))의 차이가 generator의 역할; 데이터를 생성할 수 있는 기능만 만들어 놓은 뒤, 필요할 때마다 데이터를 생성하는 방식\n",
    "# return 은 함수의 결과값을 반환한 뒤 함수를 종료하지만, yield는 list의 경우, for 문을 돌면서 list의 원소를 하나씩 생성하게 됨\n",
    "# gen() 은 실제 데이터를 갖고 있지 않지만 list(gen())을 실행 시, 데이터 생성\n",
    "def gen():\n",
    "    for i in range(10):\n",
    "        yield [i] * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator에서 만드는 Dataset\n",
    "# output_type을 통해 자료형을 지정; generator는 아직 데이터를 생성한 결과가 아니기 때문에, tensor_slice와 달리 이미 데이터가 있는 걸 Data set으로 만드는 것이 아니다\n",
    "dataset = tf.data.Dataset.from_generator(gen, output_types=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]], shape=(4, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]], shape=(4, 7), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[8 8 8 8 8 8 8 8 0]\n",
      " [9 9 9 9 9 9 9 9 9]], shape=(2, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# padded_batch를 통해, 데이터를 묶을 때 패딩을 진행\n",
    "# 하나의 배치 안에서 문장의 길이가 서로 다른 데이터를 일정하게 처리\n",
    "# None 으로 지정 안할 시, 가장 긴 것에 맞춰서 길이를 맞춰줌 (예; 3의 길이에 맞춰 1, 2에 0이 추가되어 길이가 맞춰짐)\n",
    "for batch in dataset.padded_batch(4, padded_shapes=(None, )):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 챗봇 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !wget -c https://github.com/songys/Chatbot_data/raw/master/ChatbotData%20.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/ChatbotData.csv')\n",
    "df.head()\n",
    "# 사용자 입력에 대한 챗봇의 답변 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 전에 토큰화를 해야되므로 토크나이저 호출\n",
    "from transformers import TFGPT2LMHeadModel, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 모델 입력\n",
    "# pytorch 모델로 되어있을 경우, from_pt=True 옵션 지정\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\", from_pt=True)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"taeminlee/kogpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표를 한 행씩 보기; itertuple\n",
    "# f 문자열을 활용해, Question과 Answer를 연결해 하나의 문장으로 만듦\n",
    "# SKT에서 원래 만든 토크나이저엔 문제가 없었지만, 현재 토크나이저는 tokenizer.vocab_size가 50,000지만 .vocab하면 124개가 더 많다\n",
    "# 124개의 토큰이 토크나이저에 잘못 들어가 있기 때문에 for 문에 있는 t를 t로서 list에 넣는 식을 활용해 50,000개의 토큰만 활용\n",
    "data = []\n",
    "for row in df.itertuples():\n",
    "    tokens = tokenizer.encode(f'<s>Q: {row.Q} A: {row.A}</s>')\n",
    "    tokens = [t for t in tokens if t < tokenizer.vocab_size]\n",
    "    data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] # 토큰 번호\n",
    "# 문장의 시작과 끝을 나타내는 토큰을 넣기 위해 위의 순환문 중 <s>와 </s> 를 추가함\n",
    "tokenizer.bos_token # 문장의 시작을 나타내는 토큰\n",
    "tokenizer.eos_token # 문장의 끝을 나타내는 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 챗봇 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 28), dtype=int32, numpy=\n",
       "array([[    0,  4322, 47820,   385, 47460, 47437, 49108, 47812,   450,\n",
       "        47820, 33203,   252,   119,  7974, 47440,     1,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,   106, 47445, 47766,  1084,  1024, 47816,\n",
       "        47487,   450, 47820,  5886, 47466, 19188, 47440,     1,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,   141, 47650, 47514, 47471,  2211, 47593,\n",
       "         2999,  5314,   450, 47820,  1791, 47459, 15515,   548, 48159,\n",
       "        47440,     1,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,   141, 47650, 47514, 47471,  1057,  2211,\n",
       "        47593,  2999,  5314,   450, 47820,  1791, 47459, 15515,   548,\n",
       "        48159, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,   753,  9003,   415, 20572,   450, 47820,\n",
       "          637, 10855,  7776, 48071, 47628, 47445, 48159, 47440,     1,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 24138,  1506, 27165, 47816, 47487,   450,\n",
       "        47820,   768,  3278,  3641,   348,  1438,   560, 10831, 47440,\n",
       "            1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 24138,  1506, 12294,   450, 47820,   768,\n",
       "         3278,  3641,   348,  1438,   560, 10831, 47440,     1,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  3029,   412, 48035,  1654,   199,   425,\n",
       "          450, 47820,   699, 16300,   948,   989, 11926, 47440,     1,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  3029,   782, 48359, 34538,   241,   134,\n",
       "          501,  2922,   455,   133,   450, 47820,  3351, 14655,   171,\n",
       "         4042, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  3029,   782, 48359, 47506,  1733, 17124,\n",
       "        17295, 48397,   450, 47820,  3351, 14655,   171,  4042, 47440,\n",
       "            1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  3029,  3862,   155, 47494, 20341,   148,\n",
       "         2744, 26647, 47524,   450, 47820, 12841,  1507, 28359, 47440,\n",
       "            1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 11420, 20214,   450, 47820,   127, 12001,\n",
       "        12266, 17064, 47440,     1,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 11420,  4791, 25378, 20214,   450, 47820,\n",
       "          127, 12001, 12266, 17064, 47440,     1,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 11420, 47459,  1380,   956, 47517,  3311,\n",
       "          450, 47820,  1380,   806, 10743,  1312, 47440,     1,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 18861, 22527,   261, 47892,   450, 47820,\n",
       "        18781,   768, 19447, 17064, 47440,     1,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,   119, 47494, 16883, 10811,  5600,   450,\n",
       "        47820, 34718,   650, 48030, 14974, 47440,     1,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 25767, 47960, 35261, 48432,  2305, 47683,\n",
       "          450, 47820,   206, 47454, 38967,   290, 23161, 24155, 47440,\n",
       "            1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  5244, 47668, 12009, 47447, 23652, 47487,\n",
       "          450, 47820,  4756,  7892,  7736, 47454, 17079, 47447,  1325,\n",
       "         1312, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  5244, 47668, 12009,  5923,  1699, 47578,\n",
       "        10627,   450, 47820,  4756,  7892,  7736, 47454, 17079, 47447,\n",
       "         1325,  1312, 47440,     1,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  5244, 47506,  2236,   882,  2818, 47440,\n",
       "          450, 47820,   616,   302,   230,   280, 12963, 47466, 48240,\n",
       "        47580, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  5244, 47506,  9143, 47590, 22799, 47233,\n",
       "        47790, 47487,   450, 47820, 27064,   117,  1312, 47812,     1,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  5244, 47506, 10220,   134, 48053,   450,\n",
       "        47820,   616,   302,   230,   280, 12963, 47466, 48240, 47580,\n",
       "        47440,     1,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,   461, 12964,   282, 35927, 47774,   450,\n",
       "        47820,   461, 12964, 10874,   820, 19991, 47440, 37498,   164,\n",
       "         2052,  2695,  2767, 27792,  3351, 19166,   425, 28859, 47440,\n",
       "            1],\n",
       "       [    0,  4322, 47820,  1193,  1791,   119,   868,  1816, 47487,\n",
       "          450, 47820,   580, 10840,   567, 11616, 11290,  8081,   119,\n",
       "         4042, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  1193,  1791, 24845,   450, 47820,   580,\n",
       "        10840,   567, 11616, 11290,  8081,   119,  4042, 47440,     1,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  1193,  1791, 21156,  1902, 47774,   450,\n",
       "        47820,   580, 10840,   567, 11616, 11290,  8081,   119,  4042,\n",
       "        47440,     1,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  1193,   605, 47774,   450, 47820, 25858,\n",
       "         1126,   610,  5267, 16449, 47453,   319, 47859,  2532, 47466,\n",
       "         2465,  5267,  2430, 47446,  9797, 11926,     1,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  1193,  1752,   695, 17033,   450, 47820,\n",
       "        25858,  1126,   610,  5267, 16449, 47453,   319, 47859,  2532,\n",
       "        47466,  2465,  5267,  2430, 47446,  9797, 11926,     1,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820,  1193,  8127,  1791,  4104, 47440,   450,\n",
       "        47820,   280, 13917, 47734,  6017,   283, 47790,  7974, 47440,\n",
       "            1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 39627,   793,  9654,   450, 47820, 18702,\n",
       "        47580, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 16813, 47859,   151, 48012, 47466,   450,\n",
       "        47820, 28748, 38502, 47454, 47130,  3414,  1899, 21642, 48240,\n",
       "        47580, 47440,     1,     3,     3,     3,     3,     3,     3,\n",
       "            3],\n",
       "       [    0,  4322, 47820, 16813, 47859,   151, 48012, 47466, 47816,\n",
       "        47487,   450, 47820, 28748, 38502, 47454, 47130,  3414,  1899,\n",
       "        21642, 48240, 47580, 47440,     1,     3,     3,     3,     3,\n",
       "            3]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만든 데이터를 tensor flow에서 다룰 수 있게 처리\n",
    "import tensorflow as tf\n",
    "\n",
    "# 변환을 위해 함수를 만듦\n",
    "def data_generator():\n",
    "    for datum in data:\n",
    "        yield datum\n",
    "        \n",
    "# Data를 필요할 때마다 가져와 tensor flow에 Data set 형태로 덮어줌\n",
    "dataset = tf.data.Dataset.from_generator(data_generator, output_types=tf.int32)\n",
    "\n",
    "# 딥러닝의 학습은 여러 개의 데이터를 묶은 하나의 배치 단위로 이뤄지기 때문에 배치로 바꿔줘야함\n",
    "# 각각의 대화가 길이가 다르기 때문에 padded_batch를 통해 길이를 맞춰줘야 함\n",
    "# 토크나이저의 패딩용 토큰을 채워넣을 값을 지정\n",
    "dataset = dataset.padded_batch(32, padded_shapes=(None, ), padding_values=tokenizer.pad_token_id)\n",
    "\n",
    "# 하나의 배치를 꺼내볼 수 있음\n",
    "# 32개의 데이터가 묶여 있으며, 가장 긴 길이의 데이터를 기준으로 28의 길이로 맞춰져 있음\n",
    "# 모자란 토큰은 3번 토큰으로 채워져 있으며, 패딩용 토큰이다\n",
    "# 이후 batch를 하나씩 꺼내 모형에 넣어 학습을 진행\n",
    "batch = next(iter(dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 챗봇 데이터로 미세 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미세조정을 위한 optimizer 알고리즘 초기화\n",
    "# 세분적인 방법에 따라 여러 알고리즘 선택 가능 (Adam을 가장 많이 사용)\n",
    "# 한 번에 파라미터를 얼마나 업데이트 할 것이냐? 클수록 학습은 빠르지만 학습이 불안정해짐 (learning_rate) ; 3e-5 = 3 x 10의 -5승\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.824298], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(batch, labels=batch) # 정답을 batch를 통해 자기자신으로 주어져야 예측만 하지 않고 손실 값을 구해줌\n",
    "result['loss'] # 손실값은 문장의 시작을 나타내는 토큰 이후 부터 예측을 시작해 Q라는 단어가 나올 확률(교차 엔트로피 값)부터 이어짐\n",
    "# padding으로 되어있는 부분은 손실을 계산하지 않기 때문에 0으로 계산해 loss에 반영하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.824298>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(result['loss']) # 평균적인 손실을 낮춰 다음에 나온 단어를 높은 확률로 예측할 수 있게 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 길이를 batch 사이즈로 구한 뒤 소숫점 뒤를 버리고 총 batch 개수는 하나 더 더해서 구함\n",
    "total = df.shape[0] // 32 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.824298"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사하강법을 통해 mean_loss를 y 값을 지정해 평균 손실이 최소화되는 지점을 찾음\n",
    "# trainable_variables을 조정해 loss를 낮추는 것을 찾음\n",
    "# apply_gradient를 통해 loss를 적용해 기존 파라미터를 loss가 줄어드는 방향으로 수정함\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    result = model(batch, labels=batch)\n",
    "    loss = result['loss']\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "grads = tape.gradient(mean_loss, model.trainable_variables)\n",
    "opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "mean_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5220ff7a1149459fb2ffb80b81bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm.notebook\n",
    "\n",
    "# 위의 코드를 서로 다른 batch에 대해 반복\n",
    "# 한 번의 순환이 한 epoche 이며, 미세 조정은 너무 많은 epoche를 진행할 경우, 기존 학습을 망각할 수 있기 때문에 좋지 않을 수 있음\n",
    "for batch in tqdm.notebook.tqdm(dataset, total=total):\n",
    "    with tf.GradientTape() as tape:\n",
    "        result = model(batch, labels=batch)\n",
    "        loss = result['loss']\n",
    "        mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    grads = tape.gradient(mean_loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추후 활용을 위해 모델 저장\n",
    "model.save_pretrained(\"./model/chat_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 챗봇과 대화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰을 입력 후 Q 를 입력하고 A 이후로 챗봇의 말을 만듦\n",
    "text = '오늘도 좋은 하루!'\n",
    "sent = f'<s>Q: {text} A:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Q: 오늘도 좋은 하루! A:'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저에 인코딩하며, 전처리할 때와 달리 바로 tensor flow 형식으로 변환\n",
    "input_ids = tokenizer.encode(sent, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 모델에 입력\n",
    "# 최대 길이 50 토큰으로 지정\n",
    "# early_stopping = Ture 를 통해, 문장의 끝을 나타내는 토큰을 만났을 시 종료\n",
    "# eos token 은 토크나이져에 지정되어있는 토큰 지정\n",
    "# pad_token 은 필수는 아니지만 토크나이져의 패드 토큰 지정\n",
    "output = model.generate(input_ids, max_length=50, early_stopping=True, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Q: 오늘도 좋은 하루! A: 좋은 하루 되세요.</s>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': 좋은 하루 되세요.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = tokenizer.decode(output[0])\n",
    "# 입력된 문장의 길이가 sent의 길이이므로\n",
    "answer[len(sent):-len('</s>')]\n",
    "# 이와 같이 대답 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Q: 오늘도 좋은 하루! A: 좋은 일이 있을 거예요.</s>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤하게 답변을 생성\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True, early_stopping=True, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nltk\n",
      "Version: 3.8.1\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: https://www.nltk.org/\n",
      "Author: NLTK Team\n",
      "Author-email: nltk.team@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\users\\lucky\\anaconda3\\envs\\test\\lib\\site-packages\n",
      "Requires: click, joblib, regex, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show nltk\n",
    "# 설치된 nltk의 버젼을 확인 (현재 3.2.5)\n",
    "# 낮은 버젼에서 BLEU 계산에 문제가 생김 (3.5 이상이여야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U nltk\n",
    "# nltk를 업데이트하여 BLEU 계산 문제 해결 (3.2.5를 제거하고 3.5 이상 설치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk의 BLEU 계산 함수 모듈 호출\n",
    "from nltk.translate.bleu_score import SmoothingFunction, modified_precision, sentence_bleu\n",
    "\n",
    "# 기준 문장과 생성했다고 가정할 문장\n",
    "# .split을 통해 빈 칸 단위로 끊어 토큰화\n",
    "# BLEU는 정답이 여러 개라면, 여러 개를 갖고 계산할 수 있으므로, 기준 문장을 리스트에 담아 변수 지정\n",
    "ref = ['the cat is on the mat'.split()]\n",
    "sentence = 'the the the the the the the'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUP 계산, 몇 개의 단위의 토큰으로 비교할 것인가 3번째 인자로 지정 (현재, unigram)\n",
    "p1 = modified_precision(ref, sentence, 1)\n",
    "float(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-gram preicision (bi-gram)\n",
    "p2 = modified_precision(ref, sentence, 2)\n",
    "float(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\lucky\\anaconda3\\envs\\TEST\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.331960397810445e-231"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N이 1부터 4까지, 기하평균을 구한 뒤 패널티를 포함해 BLEU 점수 계산\n",
    "# 0점이 들어가기 때문에 아주 작은 수를 도출함(smoothing method_1)\n",
    "sentence_bleu(ref, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19205612637498934"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoothing function 지정을 통해 다른 방식으로 BLEU 계산\n",
    "# method2 = 분모와 분자에 각각 1씩 더해주는 방식\n",
    "sentence_bleu(ref, sentence, smoothing_function=SmoothingFunction().method2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor flow에서 사용하는 데이터 저장 방식\n",
    "# 너무 큰 데이터인 경우, 한 번에 불러올 경우 메모리를 과도하게 차지하기 때문에, 나눠서 불러오는데, 기존 csv 등의 포멧은 데이터를 신속하게 불러올 수 없음\n",
    "# 데이터 하나를 동시에 읽을 수도 있어야 하기 때문에 tensor flow에서 TFRecord 라는 용량도 작으며, 빠른 속도로 불러올 수 있는 데이터 형식 제공\n",
    "# TFRecord는 일반적으로 example(=row)로 저장하며, Feature를 통해 columns을 형성한다\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def bytes_feature(value):  # 문자열\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def float_feature(value):  # 실수\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def int_feature(value):  # 정수\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "# TFRecord에서 위 3가지 형태의 데이터만 제시함\n",
    "# 다른 자료형의 경우, 위 3가지 형태로 변환시켜 줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\nG\\n\\x12\\n\\x04name\\x12\\n\\n\\x08\\n\\x06\\xea\\xb9\\x80\\xea\\xb0\\x91\\n\\x0f\\n\\x04item\\x12\\x07\\x1a\\x05\\n\\x03\\x01\\x02\\x03\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x12\\n\\x06height\\x12\\x08\\x12\\x06\\n\\x04\\x9a\\x19.C'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'name': bytes_feature(['김갑'.encode('utf8')]),\n",
    "    'age': int_feature([16]),\n",
    "    'height': float_feature([174.1]),\n",
    "    'item': int_feature([1, 2, 3])\n",
    "}))\n",
    "# {} 내부에 사전형태로 정보 지정\n",
    "# Feature 들은 무조건 리스트 형태로 지정되어야 하며, 문자열은 인코딩을 포함해야 함\n",
    "example.SerializeToString()\n",
    "# 데이터 전체를 압축된 형태로 표현 (컴퓨터가 읽고 쓰기 쉬운 형태)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFRecord 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(name, age, height, item):\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'name': bytes_feature([name.encode('utf8')]),\n",
    "        'age': int_feature([age]),\n",
    "        'height': float_feature([height]),\n",
    "        'item': int_feature(item)\n",
    "    }))\n",
    "    return example.SerializeToString()\n",
    "# 위의 과정을 함수로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test1.tfrecord'\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    s = serialize_example('김갑', 16, 174.1, [1, 2, 3])\n",
    "    writer.write(s)\n",
    "\n",
    "    s = serialize_example('이을', 35, 182.6, [1, 2])\n",
    "    writer.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test2.tfrecord'\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    s = serialize_example('박병', 24, 169.5, [2])\n",
    "    writer.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFRecord 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['test1.tfrecord', 'test2.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "# file을 리스트 형태로 이름을 써준 뒤, tensorflow의 TFRecordDataset에 데이터 이름을 넣어주면, 파일이 몇 개든 하나의 Dataset으로 취급해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"name\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\352\\271\\200\\352\\260\\221\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"item\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 1\n",
      "        value: 2\n",
      "        value: 3\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"height\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 174.1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 16\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"name\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\354\\235\\264\\354\\235\\204\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"item\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 1\n",
      "        value: 2\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"height\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 182.6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 35\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "features {\n",
      "  feature {\n",
      "    key: \"name\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\353\\260\\225\\353\\263\\221\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"item\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 2\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"height\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 169.5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 24\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset 사용법\n",
    "# 하나의 example 씩 읽어주게 됨\n",
    "for batch in dataset:\n",
    "    example = tf.train.Example() # 빈 Example을 생성\n",
    "    example.ParseFromString(batch.numpy()) # batch에서 Example을 읽어옴\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .features.feature['age'] 를 통해 자료를 확인할 수 있고, 해당 자료 출력 형태를 따라가면 최종적인 값만 출력 가능\n",
    "example.features.feature['age'].int64_list.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'박병'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.features.feature['name'].bytes_list.value[0].decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169.5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.features.feature['height'].float_list.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.features.feature['item'].int64_list.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset을 TFRecord로 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucky\\AppData\\Local\\Temp\\ipykernel_12440\\105125575.py:20: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 하나씩 저장, 호출하는 것이 아닌, 한꺼번에 저장, 호출하는 방식\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 형성\n",
    "df = pd.DataFrame({\n",
    "    'name': ['김갑', '이을', '박병'],\n",
    "    'age': [16, 35, 24],\n",
    "    'height': [174.1, 182.6, 169.5],\n",
    "    'items': [[1,2,3], [1,2], [2]]\n",
    "})\n",
    "\n",
    "# 한 행씩 example로 만들어서 문자열로 바꾸는 함수 생성 (위에서 생성한 함수 사용)\n",
    "def generator():\n",
    "    for row in df.itertuples():\n",
    "        yield serialize_example(row.name, row.age, row.height, row.items)\n",
    "        \n",
    "recset = tf.data.Dataset.from_generator(generator, output_types=tf.string) # output_types 지정\n",
    "\n",
    "# Dataset을 저장할 땐 experimental.TFRecordWriter 을 사용 (하나씩 저장할 땐 io.TFRecordWirter 사용)\n",
    "writer = tf.data.experimental.TFRecordWriter('test3.tfrecord') # 파일명 지정\n",
    "writer.write(recset) # Dataset을 한 번에 지정해서, 한 번에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFRecord 일괄 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 단위로 통채로 불러오기\n",
    "dataset = tf.data.TFRecordDataset(['test1.tfrecord', 'test2.tfrecord'])\n",
    "# 기존에 진행했던 하나씩 불러오는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일에 들어있는 example 의 형태를 정의\n",
    "# 길이가 정해져 있다면 FixedLenFeature, 아니라면 FixedSequenceFeature (예를 들어, 리스트 형태)\n",
    "# shape = () 빈 괄호 지정, 기존에 byte 였다면 dtpye = tf.string\n",
    "# int는 64, float은 32 bit\n",
    "feature_description = {\n",
    "    'name': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n",
    "    'age': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "    'height': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n",
    "    'item': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\nG\\n\\x12\\n\\x04name\\x12\\n\\n\\x08\\n\\x06\\xea\\xb9\\x80\\xea\\xb0\\x91\\n\\x0f\\n\\x04item\\x12\\x07\\x1a\\x05\\n\\x03\\x01\\x02\\x03\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x12\\n\\x06height\\x12\\x08\\x12\\x06\\n\\x04\\x9a\\x19.C'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하나만 불러왔을 때\n",
    "batch = next(iter(dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map 함수는 아래 함수를 가져와, dataset에 있는 모든 사례에 아래 함수를 적용해줌\n",
    "dataset = dataset.map(lambda x: tf.io.parse_single_example(x, feature_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': <tf.Tensor: shape=(), dtype=int64, numpy=16>,\n",
       " 'height': <tf.Tensor: shape=(), dtype=float32, numpy=174.1>,\n",
       " 'item': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3], dtype=int64)>,\n",
       " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xea\\xb9\\x80\\xea\\xb0\\x91'>}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataset))\n",
    "batch\n",
    "# dictionary에 tensor 형태로 데이터가 입력되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화된 데이터를 TFRecord로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!wget -c https://github.com/e9t/nsmc/raw/master/ratings_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "nsmc = pd.read_csv('./data/ratings_train.txt', sep='\\t')\n",
    "nsmc.head()\n",
    "# 영화 리뷰 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"taeminlee/kogpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "small = nsmc.iloc[:10000]\n",
    "\n",
    "# train_test_split\n",
    "idx_train, idx_test = train_test_split(small.index, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_test = train_test_split(small.index, test_size=0.2, random_state=42)\n",
    "def int_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=9254, id=6213543, document='80년대 최고의 공포영화..', label=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = small.loc[idx_train]\n",
    "row = next(df.itertuples())\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(row.document)\n",
    "tokens = [t for t in tokens if t < tokenizer.vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 길이를 알아야 마지막 토큰을 알 수 있음\n",
    "n = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =tf.train.Example(features=tf.train.Features(feature={\n",
    "    'tokens':int_feature(tokens),\n",
    "    'n': int_feature([n]),\n",
    "    'label':int_feature([row.label])\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 저장\n",
    "filename = 'train_nsmc.tfrecord'\n",
    "df = small.loc[idx_train]\n",
    "\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for row in df.itertuples():\n",
    "        tokens = tokenizer.encode(row.document)\n",
    "        tokens = [t for t in tokens if t < tokenizer.vocab_size]\n",
    "        n = len(tokens)\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'tokens': int_feature(tokens),\n",
    "            'n': int_feature([n]),\n",
    "            'label': int_feature([row.label])\n",
    "        }))\n",
    "\n",
    "        s = example.SerializeToString()\n",
    "        writer.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data 저장\n",
    "filename = 'test_nsmc.tfrecord'\n",
    "df = small.loc[idx_test]\n",
    "\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "    for row in df.itertuples():\n",
    "        tokens = tokenizer.encode(row.document)\n",
    "        tokens = [t for t in tokens if t < tokenizer.vocab_size]\n",
    "        n = len(tokens)\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'tokens': int_feature(tokens),\n",
    "            'n': int_feature([n]),\n",
    "            'label': int_feature([row.label])\n",
    "        }))\n",
    "\n",
    "        s = example.SerializeToString()\n",
    "        writer.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 특별한 미세 조정 없이, 문장이 주어졌을 때, 문장 뒤 올 단어를 예측해서 문장 끝에 긍/부정을 예측\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"taeminlee/kogpt2\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nsmc.document.iloc[0]\n",
    "text\n",
    "# 부정적인 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# 문장을 토크나이저에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 50000), dtype=float32, numpy=\n",
       "array([[[  2.1121006 ,   7.5509257 ,  -8.365714  , ...,  -2.09952   ,\n",
       "           0.9347404 ,  -0.232217  ],\n",
       "        [  0.69625425,   9.109236  , -11.216259  , ...,  -4.9662447 ,\n",
       "          -4.410963  ,  -1.8078165 ],\n",
       "        [  1.9094037 ,   8.90713   , -12.061606  , ...,  -3.8685226 ,\n",
       "          -3.4481714 ,  -3.701539  ],\n",
       "        ...,\n",
       "        [ -0.7769085 ,  12.743277  , -13.720602  , ...,  -4.797595  ,\n",
       "          -0.4287318 ,  -2.1140919 ],\n",
       "        [  3.6529002 ,  13.500687  , -12.352482  , ...,  -2.738575  ,\n",
       "          -2.8930151 ,  -2.0195506 ],\n",
       "        [ -0.5156875 ,  10.563933  , -11.987965  , ...,  -2.1986065 ,\n",
       "          -3.1426308 ,  -2.0834565 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(input_ids)\n",
    "result['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3311]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('좋다')\n",
    "# '좋다'가 해당되는 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9321]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('별로')\n",
    "# '별로'가 해당되는 위치 (싫다는 2개 위치가 나오므로, 1개 위치가 출력되는 표현 선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([5.3892064, 5.831721 ], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 좌표를 넣어주면, 좌표에 해당하는 데이터를 추출해 새로운 tensor를 만들어줌\n",
    "# 0번부터 9번까지 (10개) 범위와, 긍부정에 해당하는 토큰을 지정\n",
    "logits = tf.gather_nd(result['logits'], [[0, 9, 9321], [0, 9, 3311]])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.391142, 0.608858], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits 값을 확률로 바꿔줌\n",
    "tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot 감성 분석 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장으론 작동 테스트에 부적합하므로, 여러 문장을 통해 정확도 평가 진행\n",
    "test_dataset = tf.data.TFRecordDataset(['test_nsmc.tfrecord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 넣기 적합한 형태로 지정\n",
    "feature_description = {\n",
    "    'tokens': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n",
    "    'n': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "    'label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n",
       " 'n': <tf.Tensor: shape=(), dtype=int64, numpy=24>,\n",
       " 'tokens': <tf.Tensor: shape=(24,), dtype=int64, numpy=\n",
       " array([ 1069,  7651,  8000,  4722, 47541, 48397, 47437,  7625,   127,\n",
       "         8462,   677, 47494, 10206, 47818, 47461, 47569,   246, 47442,\n",
       "         3461,   409, 10360,  4104, 47440, 10258], dtype=int64)>}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 하나 예시 생성\n",
    "tf.io.parse_single_example(batch, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .cast = data의 type을 변경해줌\n",
    "def parse_data(example):\n",
    "    data = tf.io.parse_single_example(example, feature_description)\n",
    "    data['tokens'] = tf.cast(data['tokens'], tf.int32)\n",
    "    data['n'] = tf.cast(data['n'], tf.int32)\n",
    "    data['label'] = tf.cast(data['label'], tf.int32)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data의 위의 함수를 일괄적용하며 패딩을 함께 진행\n",
    "test_dataset = test_dataset.map(parse_data).padded_batch(32, padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0628c08d0e594ea9b110621092cec71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 성능 테스트\n",
    "import tqdm.notebook\n",
    "n_total = 2000 # test data 전체 개수\n",
    "n_correct = 0 # 몇 개 맞았는지 저장\n",
    "\n",
    "for batch in tqdm.notebook.tqdm(test_dataset, total=2000 // 32 + 1): # 전체 갯수를 batch 수로 나눈 뒤 +1\n",
    "    result = model(batch['tokens']) # 모델에 batch의 tokens 값을 입력\n",
    "    indices = [] # 문장 길이를 표시하기 위한 빈 list\n",
    "    for i, n in enumerate(batch['n']): # n은 문장마다 길이를 의미\n",
    "        j = n - 1 # 마지막 토큰의 위치 (예) 길이가 24개의 토큰이라면, 마지막 토큰의 길이는 23이 됨\n",
    "        indices.append([[i, j, 9321], [i, j, 3311]])\n",
    "    logits = tf.gather_nd(result['logits'], indices)\n",
    "\n",
    "    probs = tf.nn.softmax(logits) # 일괄 처리를 통해 확률로 변환\n",
    "    good_probs = probs[:, 1] # 긍부정의 기본적인 확률 자체가 차이가 있으므로, 기준을 0.5로 잡으면 긍정이 유리하므로 긍정에 해당하는 확률열만 따로 추출\n",
    "    is_good = tf.where(good_probs > 0.5, 1, 0) # 긍정의 확률이 0.5보다 높으면 긍정, 낮으면 부정으로 예측, 기준값 0.5는 변경 가능\n",
    "    n_correct += tf.reduce_sum(tf.cast(is_good == batch['label'], tf.int32)).numpy() # is_good과 batch의 label과 비교해, True, False가 출력되고 자료형을 변경해 결과 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5835"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero-shot 을 통해 아무런 추가 조정 없이 결과 도출\n",
    "n_correct / 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoGPT2 감성 분석을 위한 미세 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2ForSequenceClassification: ['transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing TFGPT2ForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2ForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFGPT2ForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 감성 분석 시 긍부정으로 문장 분류\n",
    "from transformers import TFGPT2ForSequenceClassification\n",
    "\n",
    "# 모델 호출 후 사전학습 진행\n",
    "model = TFGPT2ForSequenceClassification.from_pretrained(\"taeminlee/kogpt2\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 2), dtype=float32, numpy=\n",
       "array([[[-0.06495173, -0.03280668],\n",
       "        [ 0.23053408,  0.02685528],\n",
       "        [ 0.2291107 , -0.18320113],\n",
       "        [-0.11420687, -0.15777808],\n",
       "        [ 0.2834201 ,  0.15727204],\n",
       "        [-0.1615361 , -0.23664558],\n",
       "        [ 0.01690693,  0.06465401],\n",
       "        [ 0.03262112,  0.08841263],\n",
       "        [ 0.13037543,  0.03150433],\n",
       "        [ 0.19479348, -0.17799076]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1, 10, 2)\n",
    "# 긍부정을 예측하기 때문에 2, 언어모형은 50,000\n",
    "result['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 train dataset 형성\n",
    "train_dataset = tf.data.TFRecordDataset(['train_nsmc.tfrecord'])\n",
    "train_dataset = train_dataset.map(parse_data).padded_batch(32, padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "n_total = 8000 # data 전체 개수\n",
    "n_correct = 0\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08) # optimizer 정의\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True) # logit과 비교하기 위한 손실 함수 정의\n",
    "\n",
    "for batch in tqdm.notebook.tqdm(train_dataset, total=math.ceil(8000 / 32)): # = (8000 // 32 + 1)\n",
    "    with tf.GradientTape() as tape: # logit 도출 부분 경사 계산\n",
    "        result = model(batch['tokens'])\n",
    "        indices = []\n",
    "        for i, n in enumerate(batch['n']):\n",
    "            j = n - 1\n",
    "            indices.append([[i, j, 0], [i, j, 1]]) # 긍부정을 예측하기 때문에 0과 1만 가져오면 됨\n",
    "        logits = tf.gather_nd(result['logits'], indices)\n",
    "        loss = loss_func(batch['label'], logits[:, 1]) # logit과 loss를 바로 비교 (확률 도출 x) # logit과 batch의 실제 label과 비교\n",
    "        # BinaryCrossEntropy기 때문에 긍정 부분만 비교하면 됨\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables)) # 경사를 모델에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 만들어둔 test 코드\n",
    "n_total = 2000\n",
    "n_correct = 0\n",
    "\n",
    "for batch in tqdm.notebook.tqdm(test_dataset, total=math.ceil(2000 / 32)):\n",
    "    result = model(batch['tokens'])\n",
    "    indices = []\n",
    "    for i, n in enumerate(batch['n']):\n",
    "        j = n - 1\n",
    "        indices.append([[i, j, 0], [i, j, 1]])\n",
    "    logits = tf.gather_nd(result['logits'], indices)\n",
    "\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    good_probs = probs[:, 1]\n",
    "    is_good = tf.where(good_probs > 0.5, 1, 0)\n",
    "    n_correct += tf.reduce_sum(tf.cast(is_good == batch['label'], tf.int32)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_correct / 2000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

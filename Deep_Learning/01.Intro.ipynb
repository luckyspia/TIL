{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 01. Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 호출\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar\n",
    "- 상수값\n",
    "- `torch.tensor` 함수를 통해 정의할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "scalar1 = torch.tensor([1.]) # () 안에 scalar 값을 넣으면, 하나의 element를 갖는 실수가 지정, .은 실수를 의미\n",
    "print(scalar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "scalar2 = torch.tensor([3.])    \n",
    "print(scalar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정의한 `tensor`(상수값)을 갖고 사칙연산 수행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "add_scalar = scalar1 + scalar2\n",
    "print(add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "sub_scalar = scalar1 - scalar2\n",
    "print(sub_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "mul_scalar = scalar1 * scalar2\n",
    "print(mul_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "div_scalar = scalar1 / scalar2\n",
    "print(div_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch` 모듈의 내장 함수를 통해서도 사칙연산 수행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector\n",
    "- `torch.tensor`를 통해 벡터를 정의하는 것 또한 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "vector1 = torch.tensor([1., 2., 3.])\n",
    "print(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "vector2 = torch.tensor([4., 5., 6.])\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사칙연산 또한 Scalar와 같은 방식으로 수행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "add_vector = vector1 + vector2\n",
    "print(add_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3., -3., -3.])\n"
     ]
    }
   ],
   "source": [
    "sub_vector = vector1 - vector2\n",
    "print(sub_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4., 10., 18.])\n"
     ]
    }
   ],
   "source": [
    "mul_vector = vector1 * vector2\n",
    "print(mul_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "div_vector = vector1 / vector2\n",
    "print(div_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3., -3., -3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 10., 18.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(vector1, vector2) # 여기까지 함수나 연산은 같은 위치 원소끼리 계산 (elementwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(vector1, vector2) # 벡터의 내적 (4 + 10 + 18 = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix\n",
    "- 행렬 (2개 이상의 벡터로 구성된 선형 대수의 기본 단위)\n",
    "- `torch.tensor`를 통해 행렬 정의 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "matrix1 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(matrix1) # 대괄호의 갯수가 차원을 의미하므로, 현재 2차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "matrix2 = torch.tensor([[5., 6.], [7., 8.]])\n",
    "print(matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scalar, Vector tensor와 같은 방식으로 사칙연산 수행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "sum_matrix = matrix1 + matrix2\n",
    "print(sum_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n"
     ]
    }
   ],
   "source": [
    "sub_matrix = matrix1 - matrix2\n",
    "print(sub_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `*` 와 `/`의 경우, 행렬의 같은 위치에 존재하는 값들끼리 연산이 수행됨 (element-wise operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n"
     ]
    }
   ],
   "source": [
    "mul_matrix = matrix1 * matrix2\n",
    "print(mul_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "div_matrix = matrix1 / matrix2\n",
    "print(div_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  8.],\n",
       "        [10., 12.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4., -4.],\n",
       "        [-4., -4.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5., 12.],\n",
       "        [21., 32.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.3333],\n",
       "        [0.4286, 0.5000]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬 곱을 위해서는 `@` 연산 기호나 `torch.matmul` 함수를 사용하여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "matmul_matrix = matrix1 @ matrix2\n",
    "print(matmul_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 22.],\n",
       "        [43., 50.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "- Vector를 1차원, Matrix를 2차원이라고 한다면, **Tensor는 3차원 이상의 배열을 의미**\n",
    "  - 그냥 Vector, Matrix, Tensor를 전부 Tensor로 지칭하기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = torch.tensor([[[9., 10.], [11., 12.]], [[13., 14.], [15., 16.]]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Element-wise 사칙연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n"
     ]
    }
   ],
   "source": [
    "sum_tensor = tensor1 + tensor2\n",
    "print(sum_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n"
     ]
    }
   ],
   "source": [
    "sub_tensor = tensor1 - tensor2\n",
    "print(sub_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n"
     ]
    }
   ],
   "source": [
    "mul_tensor = tensor1 * tensor2\n",
    "print(mul_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "div_tensor = tensor1 / tensor2\n",
    "print(div_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 31.,  34.],\n",
       "         [ 71.,  78.]],\n",
       "\n",
       "        [[155., 166.],\n",
       "         [211., 226.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10., 12.],\n",
       "         [14., 16.]],\n",
       "\n",
       "        [[18., 20.],\n",
       "         [22., 24.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8., -8.],\n",
       "         [-8., -8.]],\n",
       "\n",
       "        [[-8., -8.],\n",
       "         [-8., -8.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9.,  20.],\n",
       "         [ 33.,  48.]],\n",
       "\n",
       "        [[ 65.,  84.],\n",
       "         [105., 128.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1111, 0.2000],\n",
       "         [0.2727, 0.3333]],\n",
       "\n",
       "        [[0.3846, 0.4286],\n",
       "         [0.4667, 0.5000]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9., 10.],\n",
       "         [11., 12.]],\n",
       "\n",
       "        [[13., 14.],\n",
       "         [15., 16.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 31.,  34.],\n",
       "         [ 71.,  78.]],\n",
       "\n",
       "        [[155., 166.],\n",
       "         [211., 226.]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagtion Parameter Update in MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 \t Loss:  909.9966430664062\n",
      "Iteration:  200 \t Loss:  12.81046199798584\n",
      "Iteration:  300 \t Loss:  0.33858126401901245\n",
      "Iteration:  400 \t Loss:  0.011064114049077034\n",
      "Iteration:  500 \t Loss:  0.0006726818392053246\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): # 현재 파이썬이 실행되고 있는 환경에서 torch 모듈을 이용할 때 GPU를 이용할 수 있는지 확인하는 함수\n",
    "    DEVICE = torch.device('cuda') # 참이면 'cuda' 이용\n",
    "else:\n",
    "    DEVICE = torch.device('cpu') # 거짓이면 'cpu' 이용\n",
    "\n",
    "BATCH_SIZE = 64 # 파라미터를 업데이트할 때 계산되는 데이터의 개수. 즉, 64개의 데이터로 Output을 계산하여 이를 바탕으로 Loss를 산출하여 파라미터를 1회 Update함\n",
    "                # = Batch Size가 64이고, 전체 데이터가 256이라면, 4개의 Batch로 나뉨\n",
    "INPUT_SIZE = 1000 # Input 레이어의 노드 수. BATCH_SIZE와 혼동하지 말 것. 크기가 1000인 벡터 64개로 Output 계산\n",
    "                  # = 한 개의 데이터 샘플(하나의 Batch)이 총 1,000개의 숫자로 구성된 벡터\n",
    "HIDDEN_SIZE = 100 # Hidden 레이어의 노드 수\n",
    "OUTPUT_SIZE = 10 # Output 레이어의 노드 수\n",
    "\n",
    "# (BATCH_SIZE, INPUT_SIZE)의 크기를 갖는 Input 데이터를 생성하기 위해 torch.randn 함수 활용(표준정규분포를 따르는 Element들로 Tensor(행렬, 벡터 등)가 구성되도록 하는 함수)\n",
    "# device는 위에서 정의한 DEVICE 활용, 데이터 Type은 실수(torch.float)\n",
    "# Gradient 계산이 필요 없는 부분이기 때문에 requires_grad = False (Gradient 계산은 Parameter update 할 때 필요) \n",
    "x = torch.randn(BATCH_SIZE,\n",
    "                INPUT_SIZE, # => 크기가 1,000인 Vector가 64개 있도록 인풋 생성\n",
    "                device = DEVICE, \n",
    "                dtype = torch.float, \n",
    "                requires_grad = False) # 인풋데이터는 업데이트될 필요 X\n",
    "\n",
    "# (BATCH_SIZE, OUTPUT_SIZE)의 크기를 갖는 정답 데이터도 같은 방식으로 임의로 생성\n",
    "y = torch.randn(BATCH_SIZE, \n",
    "                OUTPUT_SIZE, # => Batch SIZE * OUTPUT SIZE의 행렬 생성 \n",
    "                device = DEVICE,\n",
    "                dtype = torch.float, \n",
    "                requires_grad = False)  \n",
    "\n",
    "# (INPUT_SIZE, HIDDEN_SIZE)의 크기를 갖는 가중치 행렬 생성\n",
    "# x와 w1의 행렬 곱을 통해 Hidden 레이어의 노드 수 만큼 연산값이 만들어지게\n",
    "# Update를 해야하는 대상이기 때문에 requires_grad = True 로 설정\n",
    "w1 = torch.randn(INPUT_SIZE, \n",
    "                 HIDDEN_SIZE, \n",
    "                 device = DEVICE, \n",
    "                 dtype = torch.float,\n",
    "                 requires_grad = True)\n",
    "\n",
    "# (HIDDEN_SIZE, OUTPUT_SIZE)의 크기를 갖는 가중치 행렬 생성\n",
    "# x와 w1의 행렬 곱을 통해 만들어진 Hidden 레이어의 연산 값들과 w2를 행렬 곱하여 Output 레이어의 노드 수 만큼 Output 생성\n",
    "# Update를 해야하는 대상이기 때문에 requires_grad = True 로 설정\n",
    "w2 = torch.randn(HIDDEN_SIZE,\n",
    "                 OUTPUT_SIZE, \n",
    "                 device = DEVICE,\n",
    "                 dtype = torch.float,\n",
    "                 requires_grad = True)  \n",
    "\n",
    "# 학습 정도를 결정하는 learning_rate 설정\n",
    "learning_rate = 1e-6                                           \n",
    "for t in range(1, 501): # 500번 반복\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2) # x.mm(w1) : x와 w1의 행렬곱을 의미, clamp : min 값보다 작으면 min값으로 (ReLU와 같음)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum() # pow(2) : 제곱, sum() : 행렬(Tensor)의 모든 element를 더함\n",
    "    if t % 100 == 0:\n",
    "        print(\"Iteration: \", t, \"\\t\", \"Loss: \", loss.item()) # loss.item(): tensor type을 Python number type으로 바꿔줌\n",
    "    loss.backward() # Parameter Update에 사용되는 Gradient 계산                                          \n",
    "\n",
    "    with torch.no_grad(): # 아래 코드들은 Gradient를 추가 계산하지 않고 고정시킨 상태에서 실행됨                                     \n",
    "        w1 -= learning_rate * w1.grad # w1 Update\n",
    "        w2 -= learning_rate * w2.grad # w2 Update              \n",
    "\n",
    "        w1.grad.zero_() # w1에 저장되어 있는 Gradient를 0으로 초기화             \n",
    "        w2.grad.zero_() # w2에 저장되어 있는 Gradient를 0으로 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Backpropagation Prac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8500, 0.4800]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.1910]], grad_fn=<MmBackward0>)\n",
      "Iteration:  0 \t Loss:  0.327240526676178\n",
      "tensor([[0.1100, 0.1200],\n",
      "        [0.2100, 0.0800]], requires_grad=True)\n",
      "tensor([[0.1400],\n",
      "        [0.1500]], requires_grad=True)\n",
      "y_pred:  tensor([[0.1910]], grad_fn=<MmBackward0>)\n",
      "1 step loss:  tensor(0.3272, grad_fn=<SumBackward0>)\n",
      "tensor([[0.9236, 0.5589]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.2557]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.1213, 0.1321],\n",
      "        [0.2270, 0.0982]], requires_grad=True)\n",
      "tensor([[0.1744],\n",
      "        [0.1694]], requires_grad=True)\n",
      "y_pred:  tensor([[0.2557]], grad_fn=<MmBackward0>)\n",
      "2 step loss:  tensor(0.2770, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "x = torch.tensor([[2.0, 3.0]], requires_grad=False)\n",
    "y = torch.tensor([[1.0]], requires_grad=False)\n",
    "\n",
    "w1 = torch.tensor([[0.11, 0.12],\n",
    "                  [0.21, 0.08]], requires_grad=True)\n",
    "w2 = torch.tensor([[0.14],\n",
    "                  [0.15]], requires_grad=True)\n",
    "\n",
    "learning_rate = 0.05\n",
    "for t in range(2):\n",
    "    hidden = x @ w1\n",
    "    print(hidden)\n",
    "    y_pred = hidden @ w2\n",
    "    print(y_pred)\n",
    "    \n",
    "    loss = torch.sum(((y_pred-y).pow(2))*(1/2))\n",
    "    if t % 100 == 0:\n",
    "        print(\"Iteration: \", t, \"\\t\", \"Loss: \", loss.item())\n",
    "    print(w1)\n",
    "    print(w2)\n",
    "    print(\"y_pred: \", y_pred)\n",
    "    print(str(t+1)+\" step loss: \",loss)\n",
    "    \n",
    "    if t == 0:\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w1 -= learning_rate * w1.grad \n",
    "            w2 -= learning_rate * w2.grad         \n",
    "\n",
    "            w1.grad.zero_()              \n",
    "            w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

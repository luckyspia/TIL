{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 06. Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 받기\n",
    "import requests\n",
    "\n",
    "res = requests.get('https://github.com/euphoris/datasets/raw/master/imdb.zip')\n",
    "\n",
    "with open('imdb.zip', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment\n",
       "0    A very, very, very slow-moving, aimless movie ...          0\n",
       "1    Not sure who was more lost - the flat characte...          0\n",
       "2    Attempting artiness with black & white and cle...          0\n",
       "3           Very little music or anything to speak of.          0\n",
       "4    The best scene in the movie was when Gerardo i...          1\n",
       "..                                                 ...        ...\n",
       "995  I just got bored watching Jessice Lange take h...          0\n",
       "996  Unfortunately, any virtue in this film's produ...          0\n",
       "997                     In a word, it is embarrassing.          0\n",
       "998                                 Exceptionally bad!          0\n",
       "999  All in all its an insult to one's intelligence...          0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('imdb.zip')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 1,\n",
       " 'the': 2,\n",
       " 'and': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'is': 6,\n",
       " 'this': 7,\n",
       " 'i': 8,\n",
       " 'it': 9,\n",
       " 'to': 10,\n",
       " 'in': 11,\n",
       " 'was': 12,\n",
       " 'movie': 13,\n",
       " 'film': 14,\n",
       " 'that': 15,\n",
       " 'for': 16,\n",
       " 'as': 17,\n",
       " 'but': 18,\n",
       " 'with': 19,\n",
       " 'one': 20,\n",
       " 'on': 21,\n",
       " 'you': 22,\n",
       " 'are': 23,\n",
       " 'not': 24,\n",
       " 'bad': 25,\n",
       " \"it's\": 26,\n",
       " 'very': 27,\n",
       " 'all': 28,\n",
       " 'just': 29,\n",
       " 'so': 30,\n",
       " 'good': 31,\n",
       " 'at': 32,\n",
       " 'an': 33,\n",
       " 'be': 34,\n",
       " 'there': 35,\n",
       " 'about': 36,\n",
       " 'have': 37,\n",
       " 'by': 38,\n",
       " 'like': 39,\n",
       " 'from': 40,\n",
       " 'if': 41,\n",
       " 'acting': 42,\n",
       " 'time': 43,\n",
       " 'out': 44,\n",
       " 'his': 45,\n",
       " 'or': 46,\n",
       " 'really': 47,\n",
       " 'great': 48,\n",
       " 'even': 49,\n",
       " 'he': 50,\n",
       " 'who': 51,\n",
       " 'were': 52,\n",
       " 'has': 53,\n",
       " 'see': 54,\n",
       " 'my': 55,\n",
       " 'characters': 56,\n",
       " 'well': 57,\n",
       " 'most': 58,\n",
       " 'how': 59,\n",
       " 'more': 60,\n",
       " 'no': 61,\n",
       " 'only': 62,\n",
       " 'when': 63,\n",
       " 'ever': 64,\n",
       " '10': 65,\n",
       " 'movies': 66,\n",
       " 'plot': 67,\n",
       " 'story': 68,\n",
       " 'made': 69,\n",
       " 'some': 70,\n",
       " 'they': 71,\n",
       " 'best': 72,\n",
       " 'because': 73,\n",
       " 'your': 74,\n",
       " 'can': 75,\n",
       " 'also': 76,\n",
       " \"don't\": 77,\n",
       " 'films': 78,\n",
       " 'than': 79,\n",
       " 'its': 80,\n",
       " 'script': 81,\n",
       " 'other': 82,\n",
       " 'character': 83,\n",
       " 'would': 84,\n",
       " 'seen': 85,\n",
       " 'way': 86,\n",
       " 'love': 87,\n",
       " 'make': 88,\n",
       " \"didn't\": 89,\n",
       " 'do': 90,\n",
       " 'me': 91,\n",
       " 'watching': 92,\n",
       " 'her': 93,\n",
       " 'which': 94,\n",
       " 'what': 95,\n",
       " 'up': 96,\n",
       " 'any': 97,\n",
       " 'think': 98,\n",
       " 'real': 99,\n",
       " 'could': 100,\n",
       " 'will': 101,\n",
       " 'had': 102,\n",
       " 'every': 103,\n",
       " 'much': 104,\n",
       " 'work': 105,\n",
       " 'too': 106,\n",
       " 'look': 107,\n",
       " 'funny': 108,\n",
       " 'scenes': 109,\n",
       " 'actors': 110,\n",
       " 'better': 111,\n",
       " 'over': 112,\n",
       " 'cast': 113,\n",
       " 'never': 114,\n",
       " 'wonderful': 115,\n",
       " 'little': 116,\n",
       " 'them': 117,\n",
       " 'into': 118,\n",
       " 'watch': 119,\n",
       " 'show': 120,\n",
       " 'everything': 121,\n",
       " 'excellent': 122,\n",
       " 'anyone': 123,\n",
       " 'their': 124,\n",
       " 'totally': 125,\n",
       " 'both': 126,\n",
       " 'here': 127,\n",
       " 'music': 128,\n",
       " 'scene': 129,\n",
       " 'waste': 130,\n",
       " 'people': 131,\n",
       " 'screen': 132,\n",
       " 'go': 133,\n",
       " 'years': 134,\n",
       " 'nothing': 135,\n",
       " 'stupid': 136,\n",
       " 'awful': 137,\n",
       " 'get': 138,\n",
       " 'know': 139,\n",
       " 'still': 140,\n",
       " 'many': 141,\n",
       " 'man': 142,\n",
       " 'art': 143,\n",
       " 'two': 144,\n",
       " 'right': 145,\n",
       " 'say': 146,\n",
       " 'recommend': 147,\n",
       " 'dialogue': 148,\n",
       " 'worth': 149,\n",
       " 'writing': 150,\n",
       " 'pretty': 151,\n",
       " 'after': 152,\n",
       " 'thing': 153,\n",
       " 'again': 154,\n",
       " 'saw': 155,\n",
       " 'thought': 156,\n",
       " 'those': 157,\n",
       " 'life': 158,\n",
       " 'line': 159,\n",
       " \"doesn't\": 160,\n",
       " 'things': 161,\n",
       " 'interesting': 162,\n",
       " 'been': 163,\n",
       " 'such': 164,\n",
       " 'terrible': 165,\n",
       " 'performance': 166,\n",
       " 'being': 167,\n",
       " 'enough': 168,\n",
       " 'beautiful': 169,\n",
       " 'short': 170,\n",
       " 'part': 171,\n",
       " 'did': 172,\n",
       " 'give': 173,\n",
       " \"can't\": 174,\n",
       " 'worst': 175,\n",
       " \"i've\": 176,\n",
       " 'though': 177,\n",
       " 'first': 178,\n",
       " 'ending': 179,\n",
       " 'should': 180,\n",
       " 'end': 181,\n",
       " 'worse': 182,\n",
       " 'she': 183,\n",
       " 'camera': 184,\n",
       " 'find': 185,\n",
       " 'through': 186,\n",
       " 'predictable': 187,\n",
       " 'loved': 188,\n",
       " 'cinematography': 189,\n",
       " 'quite': 190,\n",
       " 'simply': 191,\n",
       " 'actually': 192,\n",
       " \"i'm\": 193,\n",
       " 'actor': 194,\n",
       " 'feeling': 195,\n",
       " 'now': 196,\n",
       " 'liked': 197,\n",
       " 'piece': 198,\n",
       " 'big': 199,\n",
       " 'going': 200,\n",
       " 'played': 201,\n",
       " 'boring': 202,\n",
       " 'however': 203,\n",
       " 'director': 204,\n",
       " 'between': 205,\n",
       " 'highly': 206,\n",
       " 'off': 207,\n",
       " 'drama': 208,\n",
       " 'black': 209,\n",
       " 'almost': 210,\n",
       " 'lot': 211,\n",
       " 'directing': 212,\n",
       " 'does': 213,\n",
       " 'game': 214,\n",
       " 'play': 215,\n",
       " 'least': 216,\n",
       " 'effects': 217,\n",
       " \"that's\": 218,\n",
       " 'enjoyed': 219,\n",
       " 'absolutely': 220,\n",
       " 'these': 221,\n",
       " 'amazing': 222,\n",
       " 'horror': 223,\n",
       " 'definitely': 224,\n",
       " 'understand': 225,\n",
       " 'watched': 226,\n",
       " 'truly': 227,\n",
       " 'job': 228,\n",
       " 'whole': 229,\n",
       " 'we': 230,\n",
       " 'am': 231,\n",
       " 'special': 232,\n",
       " 'white': 233,\n",
       " 'anything': 234,\n",
       " 'kids': 235,\n",
       " 'long': 236,\n",
       " 'cinema': 237,\n",
       " 'where': 238,\n",
       " 'certainly': 239,\n",
       " 'series': 240,\n",
       " 'kind': 241,\n",
       " 'tv': 242,\n",
       " 'found': 243,\n",
       " 'suspense': 244,\n",
       " 'especially': 245,\n",
       " 'myself': 246,\n",
       " 'then': 247,\n",
       " 'fact': 248,\n",
       " 'believe': 249,\n",
       " 'minutes': 250,\n",
       " \"there's\": 251,\n",
       " 'probably': 252,\n",
       " 'got': 253,\n",
       " 'used': 254,\n",
       " 'done': 255,\n",
       " 'old': 256,\n",
       " 'makes': 257,\n",
       " 'mess': 258,\n",
       " 'avoid': 259,\n",
       " 'sucks': 260,\n",
       " 'lines': 261,\n",
       " 'wasted': 262,\n",
       " 'cool': 263,\n",
       " 'budget': 264,\n",
       " 'together': 265,\n",
       " 'far': 266,\n",
       " \"wasn't\": 267,\n",
       " 'written': 268,\n",
       " 'another': 269,\n",
       " 'playing': 270,\n",
       " 'must': 271,\n",
       " '1': 272,\n",
       " 'experience': 273,\n",
       " 'sucked': 274,\n",
       " 'self': 275,\n",
       " 'john': 276,\n",
       " 'action': 277,\n",
       " 'cheap': 278,\n",
       " 'subtle': 279,\n",
       " 'everyone': 280,\n",
       " 'our': 281,\n",
       " 'around': 282,\n",
       " 'each': 283,\n",
       " 'top': 284,\n",
       " 'garbage': 285,\n",
       " 'comedy': 286,\n",
       " 'slow': 287,\n",
       " 'half': 288,\n",
       " 'disappointed': 289,\n",
       " 'poor': 290,\n",
       " 'lacks': 291,\n",
       " 'casting': 292,\n",
       " 'hilarious': 293,\n",
       " 'editing': 294,\n",
       " 'making': 295,\n",
       " 'perfect': 296,\n",
       " 'history': 297,\n",
       " 'rather': 298,\n",
       " 'money': 299,\n",
       " 'crap': 300,\n",
       " 'flick': 301,\n",
       " 'take': 302,\n",
       " 'family': 303,\n",
       " 'entire': 304,\n",
       " 'whatever': 305,\n",
       " 'having': 306,\n",
       " 'performances': 307,\n",
       " 'believable': 308,\n",
       " 'portrayal': 309,\n",
       " 'annoying': 310,\n",
       " 'gives': 311,\n",
       " 'want': 312,\n",
       " 'classic': 313,\n",
       " 'nice': 314,\n",
       " \"i'd\": 315,\n",
       " 'brilliant': 316,\n",
       " 'rent': 317,\n",
       " 'world': 318,\n",
       " 'why': 319,\n",
       " 'horrible': 320,\n",
       " 'few': 321,\n",
       " 'sound': 322,\n",
       " 'incredible': 323,\n",
       " 'him': 324,\n",
       " 'own': 325,\n",
       " 'mostly': 326,\n",
       " 'holes': 327,\n",
       " 'use': 328,\n",
       " 'human': 329,\n",
       " 'memorable': 330,\n",
       " 'seeing': 331,\n",
       " 'gets': 332,\n",
       " 'throughout': 333,\n",
       " 'second': 334,\n",
       " 'style': 335,\n",
       " 'recommended': 336,\n",
       " 'audience': 337,\n",
       " 'clever': 338,\n",
       " 'ridiculous': 339,\n",
       " 'non': 340,\n",
       " 'works': 341,\n",
       " 'guess': 342,\n",
       " 'bit': 343,\n",
       " 'face': 344,\n",
       " 'low': 345,\n",
       " 'since': 346,\n",
       " 'single': 347,\n",
       " 'put': 348,\n",
       " 'production': 349,\n",
       " '\\x96': 350,\n",
       " 'mind': 351,\n",
       " 'amount': 352,\n",
       " 'strong': 353,\n",
       " 'fun': 354,\n",
       " 'enjoy': 355,\n",
       " 'lame': 356,\n",
       " 'away': 357,\n",
       " 'girl': 358,\n",
       " 'tom': 359,\n",
       " 'often': 360,\n",
       " 'word': 361,\n",
       " 'overall': 362,\n",
       " 'gave': 363,\n",
       " 'terrific': 364,\n",
       " 'same': 365,\n",
       " 'hour': 366,\n",
       " 'joy': 367,\n",
       " 'before': 368,\n",
       " 'oh': 369,\n",
       " 'night': 370,\n",
       " \"couldn't\": 371,\n",
       " 'direction': 372,\n",
       " 'cult': 373,\n",
       " 'times': 374,\n",
       " 'beyond': 375,\n",
       " 'lead': 376,\n",
       " 'silent': 377,\n",
       " 'hitchcock': 378,\n",
       " 'thriller': 379,\n",
       " 'new': 380,\n",
       " 'full': 381,\n",
       " 'completely': 382,\n",
       " 'pathetic': 383,\n",
       " 'talk': 384,\n",
       " 'care': 385,\n",
       " 'fails': 386,\n",
       " 'yet': 387,\n",
       " 'said': 388,\n",
       " 'fans': 389,\n",
       " 'solid': 390,\n",
       " 'felt': 391,\n",
       " 'child': 392,\n",
       " 'day': 393,\n",
       " 'started': 394,\n",
       " 'shot': 395,\n",
       " 'mention': 396,\n",
       " 'year': 397,\n",
       " 'different': 398,\n",
       " 'involved': 399,\n",
       " 'period': 400,\n",
       " \"won't\": 401,\n",
       " 'particularly': 402,\n",
       " 'come': 403,\n",
       " 'superb': 404,\n",
       " 'fine': 405,\n",
       " 'fast': 406,\n",
       " 'moving': 407,\n",
       " 'young': 408,\n",
       " 'lost': 409,\n",
       " 'trying': 410,\n",
       " 'song': 411,\n",
       " 'rest': 412,\n",
       " 'hours': 413,\n",
       " 'adorable': 414,\n",
       " 'songs': 415,\n",
       " 'consider': 416,\n",
       " 'tale': 417,\n",
       " \"i'll\": 418,\n",
       " 'true': 419,\n",
       " 'easily': 420,\n",
       " 'something': 421,\n",
       " 'minute': 422,\n",
       " 'level': 423,\n",
       " 'idea': 424,\n",
       " 'mediocre': 425,\n",
       " 'pg': 426,\n",
       " 'point': 427,\n",
       " 'itself': 428,\n",
       " 'perhaps': 429,\n",
       " 'glad': 430,\n",
       " 'lovely': 431,\n",
       " 'hard': 432,\n",
       " '2': 433,\n",
       " 'let': 434,\n",
       " 'utterly': 435,\n",
       " 'convincing': 436,\n",
       " 'book': 437,\n",
       " 'whether': 438,\n",
       " \"they're\": 439,\n",
       " 'follow': 440,\n",
       " 'energy': 441,\n",
       " 'generally': 442,\n",
       " 'pretentious': 443,\n",
       " 'occasionally': 444,\n",
       " 'try': 445,\n",
       " 'extremely': 446,\n",
       " 'back': 447,\n",
       " 'maybe': 448,\n",
       " 'sometimes': 449,\n",
       " 'during': 450,\n",
       " 'chemistry': 451,\n",
       " 'last': 452,\n",
       " 'unfortunately': 453,\n",
       " 'depth': 454,\n",
       " 'imagination': 455,\n",
       " 'barely': 456,\n",
       " 'storyline': 457,\n",
       " 'keep': 458,\n",
       " 'already': 459,\n",
       " 'attempt': 460,\n",
       " 'down': 461,\n",
       " 'mean': 462,\n",
       " 'us': 463,\n",
       " 'without': 464,\n",
       " 'torture': 465,\n",
       " 'premise': 466,\n",
       " 'seem': 467,\n",
       " 'ups': 468,\n",
       " 'age': 469,\n",
       " 'ray': 470,\n",
       " 'usual': 471,\n",
       " 'living': 472,\n",
       " 'themselves': 473,\n",
       " 'else': 474,\n",
       " 'tell': 475,\n",
       " 'visual': 476,\n",
       " 'plain': 477,\n",
       " 'soundtrack': 478,\n",
       " 'trash': 479,\n",
       " 'came': 480,\n",
       " 'either': 481,\n",
       " 'less': 482,\n",
       " 'hope': 483,\n",
       " 'eyes': 484,\n",
       " 'indeed': 485,\n",
       " 'theater': 486,\n",
       " 'rating': 487,\n",
       " 'three': 488,\n",
       " 'huge': 489,\n",
       " 'intelligence': 490,\n",
       " 'intelligent': 491,\n",
       " 'entertaining': 492,\n",
       " 'bored': 493,\n",
       " 'become': 494,\n",
       " 'roles': 495,\n",
       " 'drago': 496,\n",
       " 'looked': 497,\n",
       " 'parts': 498,\n",
       " 'sets': 499,\n",
       " 'stories': 500,\n",
       " 'created': 501,\n",
       " 'role': 502,\n",
       " 'scamp': 503,\n",
       " 'comes': 504,\n",
       " 'place': 505,\n",
       " 'star': 506,\n",
       " 'always': 507,\n",
       " 'insult': 508,\n",
       " 'death': 509,\n",
       " 'may': 510,\n",
       " 'expect': 511,\n",
       " 'serious': 512,\n",
       " 'original': 513,\n",
       " 'james': 514,\n",
       " 'unbelievable': 515,\n",
       " 'costs': 516,\n",
       " 'small': 517,\n",
       " 'dance': 518,\n",
       " 'beginning': 519,\n",
       " 'appreciate': 520,\n",
       " 'easy': 521,\n",
       " 'sure': 522,\n",
       " 'speak': 523,\n",
       " 'head': 524,\n",
       " 'meaning': 525,\n",
       " 'today': 526,\n",
       " 'showed': 527,\n",
       " 'delivers': 528,\n",
       " 'average': 529,\n",
       " 'main': 530,\n",
       " 'greatest': 531,\n",
       " 'gem': 532,\n",
       " 'sea': 533,\n",
       " 'faux': 534,\n",
       " 'important': 535,\n",
       " 'words': 536,\n",
       " 'yes': 537,\n",
       " 'significant': 538,\n",
       " 'picture': 539,\n",
       " 'graphics': 540,\n",
       " 'massive': 541,\n",
       " 'pure': 542,\n",
       " 'brilliance': 543,\n",
       " 'complete': 544,\n",
       " 'while': 545,\n",
       " 'moment': 546,\n",
       " 'told': 547,\n",
       " 'talented': 548,\n",
       " 'stars': 549,\n",
       " 'hill': 550,\n",
       " 'ed': 551,\n",
       " 'quality': 552,\n",
       " 'obviously': 553,\n",
       " 'addition': 554,\n",
       " 'grace': 555,\n",
       " 'negative': 556,\n",
       " 'pointless': 557,\n",
       " 'children': 558,\n",
       " \"wouldn't\": 559,\n",
       " 'dialog': 560,\n",
       " 'shots': 561,\n",
       " \"aren't\": 562,\n",
       " 'given': 563,\n",
       " 'provoking': 564,\n",
       " 'plus': 565,\n",
       " 'paced': 566,\n",
       " 'wind': 567,\n",
       " 'lion': 568,\n",
       " 'acted': 569,\n",
       " 'decent': 570,\n",
       " 'checking': 571,\n",
       " 'touching': 572,\n",
       " 'looking': 573,\n",
       " 'including': 574,\n",
       " 'embarrassing': 575,\n",
       " 'scenery': 576,\n",
       " 'house': 577,\n",
       " 'wish': 578,\n",
       " 'along': 579,\n",
       " 'happened': 580,\n",
       " 'seems': 581,\n",
       " 'mature': 582,\n",
       " 'episode': 583,\n",
       " 'remake': 584,\n",
       " 'fear': 585,\n",
       " 'nobody': 586,\n",
       " 'conflict': 587,\n",
       " 'incredibly': 588,\n",
       " 'possible': 589,\n",
       " 'whatsoever': 590,\n",
       " 'stereotypes': 591,\n",
       " 'cartoon': 592,\n",
       " 'paul': 593,\n",
       " 'women': 594,\n",
       " 'brain': 595,\n",
       " 'left': 596,\n",
       " 'features': 597,\n",
       " 'presents': 598,\n",
       " 'free': 599,\n",
       " 'screenwriter': 600,\n",
       " 'close': 601,\n",
       " 'seemed': 602,\n",
       " \"you'll\": 603,\n",
       " 'indulgent': 604,\n",
       " 'spent': 605,\n",
       " \"isn't\": 606,\n",
       " 'charles': 607,\n",
       " 'remember': 608,\n",
       " 'working': 609,\n",
       " 'attention': 610,\n",
       " 'singing': 611,\n",
       " 'etc': 612,\n",
       " 'bore': 613,\n",
       " 'dancing': 614,\n",
       " 'dvd': 615,\n",
       " 'might': 616,\n",
       " 'theme': 617,\n",
       " 'aerial': 618,\n",
       " 'interest': 619,\n",
       " 'narrative': 620,\n",
       " 'actress': 621,\n",
       " 'called': 622,\n",
       " 'spoilers': 623,\n",
       " 'stunning': 624,\n",
       " 'fx': 625,\n",
       " 'note': 626,\n",
       " 'surprisingly': 627,\n",
       " 'released': 628,\n",
       " 'ranks': 629,\n",
       " 'journey': 630,\n",
       " 'location': 631,\n",
       " 'thoroughly': 632,\n",
       " 'turn': 633,\n",
       " 'memories': 634,\n",
       " 'places': 635,\n",
       " 'billy': 636,\n",
       " 'possibly': 637,\n",
       " 'trilogy': 638,\n",
       " 'favourite': 639,\n",
       " 'awesome': 640,\n",
       " 'earlier': 641,\n",
       " 'directed': 642,\n",
       " 'someone': 643,\n",
       " 'sense': 644,\n",
       " 'games': 645,\n",
       " 'genuine': 646,\n",
       " 'smart': 647,\n",
       " \"haven't\": 648,\n",
       " 'particular': 649,\n",
       " 'stage': 650,\n",
       " 'next': 651,\n",
       " '8': 652,\n",
       " 'super': 653,\n",
       " 'wonderfully': 654,\n",
       " 'actresses': 655,\n",
       " 'exactly': 656,\n",
       " 'shows': 657,\n",
       " 'drive': 658,\n",
       " 'scared': 659,\n",
       " 'enjoyable': 660,\n",
       " 'bought': 661,\n",
       " '90': 662,\n",
       " 'god': 663,\n",
       " 'effective': 664,\n",
       " 'set': 665,\n",
       " 'learn': 666,\n",
       " 'values': 667,\n",
       " 'photography': 668,\n",
       " 'ruthless': 669,\n",
       " 'although': 670,\n",
       " 'war': 671,\n",
       " 'type': 672,\n",
       " 'example': 673,\n",
       " 'feel': 674,\n",
       " 'seriously': 675,\n",
       " 'ready': 676,\n",
       " 'fan': 677,\n",
       " 'angel': 678,\n",
       " 'under': 679,\n",
       " 'coming': 680,\n",
       " '20': 681,\n",
       " 'charming': 682,\n",
       " 'clichés': 683,\n",
       " 'thrown': 684,\n",
       " 'reason': 685,\n",
       " 'scale': 686,\n",
       " 'problems': 687,\n",
       " 'score': 688,\n",
       " 'frightening': 689,\n",
       " 'oscar': 690,\n",
       " 'knew': 691,\n",
       " 'lots': 692,\n",
       " 'space': 693,\n",
       " 'footage': 694,\n",
       " 'course': 695,\n",
       " 'perfectly': 696,\n",
       " 'finally': 697,\n",
       " 'share': 698,\n",
       " 'rate': 699,\n",
       " 'sort': 700,\n",
       " 'conclusion': 701,\n",
       " 'heart': 702,\n",
       " 'race': 703,\n",
       " 'appearance': 704,\n",
       " 'looks': 705,\n",
       " 'happen': 706,\n",
       " 'final': 707,\n",
       " 'unconvincing': 708,\n",
       " 'produced': 709,\n",
       " 'early': 710,\n",
       " 'documentary': 711,\n",
       " 'martin': 712,\n",
       " 'racial': 713,\n",
       " 'mickey': 714,\n",
       " 'watchable': 715,\n",
       " 'weak': 716,\n",
       " 'due': 717,\n",
       " \"film's\": 718,\n",
       " 'recent': 719,\n",
       " \"90's\": 720,\n",
       " 'nonsense': 721,\n",
       " 'fantastic': 722,\n",
       " 'south': 723,\n",
       " \"you're\": 724,\n",
       " 'plays': 725,\n",
       " 'thinking': 726,\n",
       " 'wrong': 727,\n",
       " 'giallo': 728,\n",
       " 'sub': 729,\n",
       " 'emotions': 730,\n",
       " 'ability': 731,\n",
       " 'write': 732,\n",
       " 'writer': 733,\n",
       " 'puppets': 734,\n",
       " 'animation': 735,\n",
       " 'flat': 736,\n",
       " 'whom': 737,\n",
       " 'walked': 738,\n",
       " 'angles': 739,\n",
       " 'became': 740,\n",
       " 'keeps': 741,\n",
       " 'running': 742,\n",
       " 'charm': 743,\n",
       " 'empty': 744,\n",
       " 'jimmy': 745,\n",
       " 'appealing': 746,\n",
       " 'case': 747,\n",
       " 'clearly': 748,\n",
       " 'review': 749,\n",
       " 'sisters': 750,\n",
       " 'terms': 751,\n",
       " 'aspect': 752,\n",
       " 'masterpiece': 753,\n",
       " 'masterpieces': 754,\n",
       " 'ask': 755,\n",
       " 'form': 756,\n",
       " 'imaginable': 757,\n",
       " 'pieces': 758,\n",
       " 'fit': 759,\n",
       " 'create': 760,\n",
       " 'deserves': 761,\n",
       " 'levels': 762,\n",
       " 'canada': 763,\n",
       " 'buy': 764,\n",
       " '13': 765,\n",
       " 'sequel': 766,\n",
       " 'rated': 767,\n",
       " 'interested': 768,\n",
       " 'unfunny': 769,\n",
       " 'joke': 770,\n",
       " 'morgan': 771,\n",
       " 'jonah': 772,\n",
       " 'lazy': 773,\n",
       " 'presence': 774,\n",
       " 'obvious': 775,\n",
       " 'cost': 776,\n",
       " 'despite': 777,\n",
       " 'choice': 778,\n",
       " 'lesser': 779,\n",
       " 'french': 780,\n",
       " 'fall': 781,\n",
       " 'cause': 782,\n",
       " 'regret': 783,\n",
       " 'front': 784,\n",
       " 'whiny': 785,\n",
       " 'future': 786,\n",
       " 'anne': 787,\n",
       " '9': 788,\n",
       " 'reading': 789,\n",
       " 'voice': 790,\n",
       " 'warmth': 791,\n",
       " 'twice': 792,\n",
       " 'delivering': 793,\n",
       " 'honestly': 794,\n",
       " 'unpredictable': 795,\n",
       " 'badly': 796,\n",
       " 'parents': 797,\n",
       " 'alexander': 798,\n",
       " 'artist': 799,\n",
       " 's': 800,\n",
       " 'gonna': 801,\n",
       " 'business': 802,\n",
       " 'needed': 803,\n",
       " 'mouth': 804,\n",
       " 'run': 805,\n",
       " 'superbly': 806,\n",
       " 'turned': 807,\n",
       " 'b': 808,\n",
       " 'list': 809,\n",
       " 'problem': 810,\n",
       " 'heaven': 811,\n",
       " 'lives': 812,\n",
       " 'church': 813,\n",
       " 'uplifting': 814,\n",
       " 'frankly': 815,\n",
       " 'lane': 816,\n",
       " 'chick': 817,\n",
       " 'disappointing': 818,\n",
       " 'lousy': 819,\n",
       " 'bring': 820,\n",
       " 'fresh': 821,\n",
       " 'bold': 822,\n",
       " 'helps': 823,\n",
       " 'idiot': 824,\n",
       " 'occupied': 825,\n",
       " 'instead': 826,\n",
       " 'accused': 827,\n",
       " 'edge': 828,\n",
       " 'somewhat': 829,\n",
       " 'afraid': 830,\n",
       " 'advise': 831,\n",
       " 'tremendously': 832,\n",
       " 'sandra': 833,\n",
       " 'bullock': 834,\n",
       " 'supposedly': 835,\n",
       " 'several': 836,\n",
       " 'moments': 837,\n",
       " 'need': 838,\n",
       " 'friends': 839,\n",
       " 'disappointment': 840,\n",
       " 'cannot': 841,\n",
       " 'stand': 842,\n",
       " 'losing': 843,\n",
       " 'putting': 844,\n",
       " 'ratings': 845,\n",
       " 'dramatic': 846,\n",
       " 'tension': 847,\n",
       " 'central': 848,\n",
       " 'themes': 849,\n",
       " 'handled': 850,\n",
       " 'pictures': 851,\n",
       " 'flawed': 852,\n",
       " 'core': 853,\n",
       " 'following': 854,\n",
       " 'bunch': 855,\n",
       " 'high': 856,\n",
       " 'hell': 857,\n",
       " 'viewing': 858,\n",
       " 'disaster': 859,\n",
       " 'paid': 860,\n",
       " 'quinn': 861,\n",
       " 'crazy': 862,\n",
       " 'hate': 863,\n",
       " 'yeah': 864,\n",
       " 'girlfriend': 865,\n",
       " 'disliked': 866,\n",
       " 'five': 867,\n",
       " 'mad': 868,\n",
       " '50': 869,\n",
       " 'cardboard': 870,\n",
       " 'predictably': 871,\n",
       " 'crafted': 872,\n",
       " 'depressing': 873,\n",
       " 'racism': 874,\n",
       " 'took': 875,\n",
       " 'redeeming': 876,\n",
       " 'appalling': 877,\n",
       " 'setting': 878,\n",
       " 'charisma': 879,\n",
       " 'explanation': 880,\n",
       " 'era': 881,\n",
       " 'wanted': 882,\n",
       " 'missed': 883,\n",
       " 'step': 884,\n",
       " 'freedom': 885,\n",
       " 'below': 886,\n",
       " 'received': 887,\n",
       " 'wayne': 888,\n",
       " 'industry': 889,\n",
       " 'noteworthy': 890,\n",
       " 'blood': 891,\n",
       " 'jamie': 892,\n",
       " 'genius': 893,\n",
       " 'owned': 894,\n",
       " 'daughter': 895,\n",
       " 'material': 896,\n",
       " 'goes': 897,\n",
       " 'hence': 898,\n",
       " 'machine': 899,\n",
       " 'flaws': 900,\n",
       " 'mishima': 901,\n",
       " 'uninteresting': 902,\n",
       " 'chilly': 903,\n",
       " 'schrader': 904,\n",
       " 'recently': 905,\n",
       " 'struck': 906,\n",
       " 'contained': 907,\n",
       " 'realistic': 908,\n",
       " 'lacked': 909,\n",
       " 'talent': 910,\n",
       " \"director's\": 911,\n",
       " 'chance': 912,\n",
       " 'master': 913,\n",
       " 'thrilled': 914,\n",
       " 'senses': 915,\n",
       " 'deeply': 916,\n",
       " 'june': 917,\n",
       " 'considering': 918,\n",
       " 'offers': 919,\n",
       " 'mexican': 920,\n",
       " 'matter': 921,\n",
       " 'noir': 922,\n",
       " 'complex': 923,\n",
       " 'psychological': 924,\n",
       " 'soul': 925,\n",
       " 'water': 926,\n",
       " 'gripping': 927,\n",
       " 'control': 928,\n",
       " 'disturbing': 929,\n",
       " 'jerky': 930,\n",
       " 'camerawork': 931,\n",
       " 'sick': 932,\n",
       " 'summary': 933,\n",
       " \"weren't\": 934,\n",
       " 'witty': 935,\n",
       " 'above': 936,\n",
       " 'ceases': 937,\n",
       " 'directors': 938,\n",
       " 'visually': 939,\n",
       " 'spoiler': 940,\n",
       " 'remaining': 941,\n",
       " 'suffering': 942,\n",
       " 'smile': 943,\n",
       " 'literally': 944,\n",
       " '25': 945,\n",
       " 'unfolds': 946,\n",
       " 'leaves': 947,\n",
       " 'room': 948,\n",
       " 'contrast': 949,\n",
       " 'sublime': 950,\n",
       " '5': 951,\n",
       " 'offensive': 952,\n",
       " 'poetry': 953,\n",
       " 'reviewer': 954,\n",
       " 'masterful': 955,\n",
       " 'pitiful': 956,\n",
       " 'nature': 957,\n",
       " 'female': 958,\n",
       " 'nuts': 959,\n",
       " 'dangerous': 960,\n",
       " 'reactions': 961,\n",
       " 'twist': 962,\n",
       " 'shed': 963,\n",
       " 'forget': 964,\n",
       " 'underneath': 965,\n",
       " 'call': 966,\n",
       " 'assistant': 967,\n",
       " 'laugh': 968,\n",
       " 'de': 969,\n",
       " 'cute': 970,\n",
       " 'guy': 971,\n",
       " \"he's\": 972,\n",
       " 'taped': 973,\n",
       " 'likes': 974,\n",
       " 'wholesome': 975,\n",
       " 'ways': 976,\n",
       " 'portraying': 977,\n",
       " 'pleased': 978,\n",
       " 'modern': 979,\n",
       " 'taking': 980,\n",
       " 'towards': 981,\n",
       " 'attempts': 982,\n",
       " 'supposed': 983,\n",
       " 'light': 984,\n",
       " 'situation': 985,\n",
       " 'leave': 986,\n",
       " 'bear': 987,\n",
       " 'kinda': 988,\n",
       " 'question': 989,\n",
       " 'lucy': 990,\n",
       " 'wonder': 991,\n",
       " 'composition': 992,\n",
       " 'brian': 993,\n",
       " 'son': 994,\n",
       " 'member': 995,\n",
       " 'identify': 996,\n",
       " \"huston's\": 997,\n",
       " 'steve': 998,\n",
       " 'robert': 999,\n",
       " 'father': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "import tensorflow as tf # 텐서플로의 Tokenizer를 사용한다\n",
    "\n",
    "# 상위 2000개의 단어까지만 번호로 변환하고 나머지 <unk>로 취급한다\n",
    "tk = tf.keras.preprocessing.text.Tokenizer(num_words=2000, oov_token='<unk>')\n",
    "\n",
    "# 단어에 번호를 붙힌다\n",
    "tk.fit_on_texts(df['review']) # 빈 칸 단위로 바꾼 뒤, 소문자로 바꿔 많이 사용하는 순으로 번호를 붙힘\n",
    "# 한국어는 빈 칸 단위로 끊으면 안 되기 때문에, 다른 전처리가 필요\n",
    "\n",
    "# 2000번을 넘어가는 단어는 <unk>로 변환되며, 1번으로 통일\n",
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.index_word[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저를 저장한다\n",
    "import joblib\n",
    "joblib.dump(tk, 'tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 호출\n",
    "import joblib\n",
    "tk = joblib.load('tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 27, 27, 27, 287, 407, 1217, 13, 36, 4, 1218, 1219, 408, 142]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 언어 모형에 맞게 데이터 정리\n",
    "\n",
    "seqs = tk.texts_to_sequences(df['review']) # 텍스트를 번호로 바꿔줌\n",
    "seqs[0]\n",
    "# 첫 문장을 단어 번호로 바꿔서 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에 있는 단어가 들어가면, 이후 나올 단어의 확률을 예측하기 때문에, 연속된 데이터의 형태를 n개의 단위로 정리\n",
    "data = []\n",
    "for seq in seqs:\n",
    "    for i in range(0, len(seq) - 4):\n",
    "        data.append((seq[i:i+4], seq[i+4]))\n",
    "# 5-gram 형태로 변환 (4개의 단어를 입력받아, 다음의 단어를 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 섞기\n",
    "import random\n",
    "\n",
    "random.shuffle(data)\n",
    "# 유사한 데이터끼리 섞여 있기 때문에, 배치 단위로 학습을 진행할 때 배치 안의 데이터가 모두 비슷하면 학습이 안 되므로 랜덤하게 섞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([251, 24, 168, 66], 44)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lm-data.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x와 y로 데이터를 나눈다\n",
    "import numpy as np\n",
    "\n",
    "xs = np.array([x for x, y in data])\n",
    "ys = np.array([y for x, y in data])\n",
    "\n",
    "# 저장\n",
    "joblib.dump((xs, ys), 'lm-data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 준비된 데이터 호출\n",
    "\n",
    "import joblib\n",
    "tk = joblib.load('tokenizer.pkl')\n",
    "xs, ys = joblib.load('lm-data.pkl') # xs; 앞에 나올 단어들, ys; 뒤에 나올 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 언어 모형에 들어갈 임베딩 레이어를 만든다\n",
    "import tensorflow as tf\n",
    "\n",
    "# 단어 번호가 1번부터 붙으므로 0번까지 포함하면 총 단어 수에 1을 더해야 한다\n",
    "NUM_WORD = tk.num_words + 1\n",
    "# 모델을 만들기 위해, 입력의 크기를 결정해야 하는데, 입력의 크기는 단어의 수에 맞춰진다\n",
    "# 0번은 텍스트의 길이를 맞춰주기 위해서 0번을 채워 사용하는 토큰\n",
    "\n",
    "tk.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = tf.keras.layers.Embedding(\n",
    "    input_dim=NUM_WORD, # 들어갈 단어의 총 갯수\n",
    "    output_dim=8, # 만들 단어의 차원 크기, 클수록 성능은 좋지만 과적합이 일어날 수 있음\n",
    ")\n",
    "# (예) xs[0] = [2,1119,1,6] 을 one-hot encoding 해야 하지만, 임베딩 레이어를 활용할 시 안해도 됨\n",
    "# 추후 임베딩만 따로 확인하기 위해 임베딩 레이어를 따로 변수로 지정하여 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 모형을 만든다\n",
    "lm = tf.keras.Sequential([\n",
    "    emb1, # 임베딩 레이어\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),    # 임베딩 레이어의 바로 뒷부분에 추가, 계산의 효율성을 위해 추가, \n",
    "                                                 # 한 번에 네 개의 단어가 입력되어 임베딩이 생기고, 임베딩에 에버리지를 적용해줌 (학습 파라미터의 숫자를 줄임)\n",
    "    tf.keras.layers.Dense(8, activation='relu'), # 은닉층\n",
    "    tf.keras.layers.Dense(NUM_WORD)              # 출력층 = 입력의 총 갯수와 출력의 크기가 같음 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 8)           16008     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 8)                 0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2001)              18009     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34089 (133.16 KB)\n",
      "Trainable params: 34089 (133.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모형 확인\n",
    "lm.summary()\n",
    "# 첫 번째 None = 모델에 들어가는 데이터의 건수에 관한 부분이므로 무시 가능\n",
    "# 두 번째 None 은 입력한 단어의 갯수이므로 4, 8 의 형태로 출력\n",
    "# AveragePooling을 통해 평균을 내 8개로 줄여줌 = 아무 Param이 필요 없음 = 0\n",
    "# 8 개의 출력을 갖고 dense Layer로 가게 되면 출력의 8 x 8 = 72 로 나옴 (Average Pooling을 안할 시 32 x 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형을 학습시킨다\n",
    "lm.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # loss 지정\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# 다항 분류이기 때문에 출력층의 activation을 softmax로 설정해야 하지만, 위에서 생략했기 때문에 loss를 계산할 때, softmax로 activation을 한 것처럼\n",
    "# 계산해야함. 그렇기 때문에 SparseCategorical Crossentropy로 지정한 뒤, (from_logits=True) 옵션 사용\n",
    "# activation 이 softmax로 설정되어 있다면 from_logits=True 지정 X\n",
    "# Tensorflow 의 여러 계산 함수가 자체적으로 softmax를 적용해주는 경우가 많기 때문에 모델에서 softmax를 적용하면 이후 계산에서 번거로운 과정 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331/331 [==============================] - 1s 2ms/step - loss: 6.8515 - accuracy: 0.0851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c46e12dc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(xs, ys, epochs=1)\n",
    "# 하나의 epochs만 fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lm.krs\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lm.krs\\assets\n"
     ]
    }
   ],
   "source": [
    "# 모형 저장\n",
    "lm.save('lm.krs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.8729645e-02,  2.0781245e-02,  4.0416311e-02, ...,\n",
       "         1.7743055e-02,  3.4751307e-02,  4.5311917e-02],\n",
       "       [-4.1068035e-01, -3.2668084e-01, -3.4414935e-01, ...,\n",
       "        -3.2421193e-01,  3.4057826e-01,  3.3284202e-01],\n",
       "       [-3.2356501e-01, -2.9822266e-01, -3.1237251e-01, ...,\n",
       "        -2.8984061e-01,  3.2561234e-01,  3.1900981e-01],\n",
       "       ...,\n",
       "       [-3.6535315e-02,  1.5987823e-02, -2.6365547e-02, ...,\n",
       "        -9.5546460e-03, -4.4168383e-03,  7.8537770e-02],\n",
       "       [-2.2907217e-04, -3.5766780e-02, -6.2176559e-02, ...,\n",
       "         1.6648181e-02,  7.2397619e-02,  1.5873889e-02],\n",
       "       [-4.6569109e-04,  4.3570232e-02,  4.1937921e-02, ...,\n",
       "        -1.3980292e-02, -3.9353907e-02,  2.0474683e-02]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 임베딩 확인\n",
    "e = emb1.embeddings.numpy()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2001개의 모든 단어들에 대해 8차원으로 임베딩을 만들어 놓은 표\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 임베딩은 임베딩 레이어의 가중치와 동일하다\n",
    "import numpy as np\n",
    "\n",
    "w = emb1.get_weights()[0] # 임베딩 레이어의 가중치만 추출\n",
    "np.array_equal(e, w)\n",
    "# 임베딩은 일반적인 레이어의 가중치를 학습하는 것과 동일한 과정으로 진행된다\n",
    "# 그렇기 때문에, 이 가중치를 추출해 다른 모형에 덮어써도 그 모형의 그 레이어가 이미 학습된 것처럼 만들 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 저장\n",
    "np.savez('word-emb.npz', emb=e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 3.],\n",
       "        [3., 6., 9.]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# GlobalAveragePooling1D는 1번 인덱스를 기준으로 평균을 구한다. 예를 들기 위해 다음과 같은 행렬이 있다고 가정\n",
    "x = np.array([[[1, 2, 3], [3, 6, 9]]], dtype='float32')\n",
    "x\n",
    "# 1과 3의 평균, 2와 6의 평균, 3과 6의 평균을 도출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3차원 array\n",
    "# 1 = 신경망에서 데이터의 건수  = 텍스트의 갯수\n",
    "# 3 = 하나의 벡터의 길이        = 단어 하나를 몇 개의 숫자로 나타내는가?\n",
    "# 2 = 이런 벡터가 몇 개 있는가? = 단어가 몇 개냐?\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D at 0x23c490eca30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 행렬을 GlobalAveragePooling1D 레이어 통과시키면 다음과 같이 된다\n",
    "\n",
    "avg = tf.keras.layers.GlobalAveragePooling1D()\n",
    "avg\n",
    "# tensorflow의 레이어는 마치 함수처럼 사용할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 4., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = avg(x).numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "# 두 단어를 집어넣어서 각각의 임베딩이 나올 때, 임베딩의 평균을 내줌\n",
    "# 위의 예에서 [1,2,3]이 하나의 단어, [3,6,9]가 하나의 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음 토큰의 확률 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"there's\", 'not', 'enough', 'movies']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 준비\n",
    "import joblib\n",
    "tk = joblib.load('tokenizer.pkl') # 토크나이저와 단어의 번호\n",
    "xs, ys = joblib.load('lm-data.pkl') # 텍스트를 신경망 모형에 넣어줄수록 변환한 데이터\n",
    "\n",
    "# 학습된 모형 호출\n",
    "import tensorflow as tf\n",
    "lm = tf.keras.models.load_model('lm.krs')\n",
    "\n",
    "# 다음에 나올 단어의 확률 예측\n",
    "x = xs[0:1] # 모델은 여러 데이터를 입력 받을 수 있기 위해 shape을 (4,) 이 아닌 (1,4) 형태로 지정해야 함\n",
    "y = ys[0]\n",
    "\n",
    "# x의 4단어를 확인한다\n",
    "[tk.index_word[i] for i in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.1920993,  3.1606002,  2.7681746, ..., -3.2598271, -3.2116275,\n",
       "        -3.2491329]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모형에 넣는다\n",
    "import numpy as np\n",
    "logit = lm.predict(x.astype('float32')) # x의 자료형을 실수형으로 변경\n",
    "logit # (1,2001) = 2001개의 모든 단어에 대한 확률\n",
    "# 모델을 만들 때 소프트맥스를 해주지 않아서 확률이 아님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.4038006e-05, 4.2500786e-02, 2.8705738e-02, ..., 6.9189598e-05,\n",
       "        7.2606192e-05, 6.9933521e-05]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소프트맥스 함수를 적용하여 확률로 바꾼다\n",
    "p = tf.nn.softmax(logit).numpy()\n",
    "p\n",
    "# p[0, 57] = 57번째 단어의 확률\n",
    "# 학습된 데이터를 토대로 했을 때, x 뒤에 57번째 단어가 나올 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에서 실제로 나온 단어를 확인한다\n",
    "tk.index_word[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027336762"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당 단어의 확률을 본다\n",
    "p[0, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률이 가장 높은 단어를 알아본다\n",
    "i = p.argmax()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042500786"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.index_word[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning (전이 학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 27, 27, 27, 287, 407, 1217, 13, 36, 4, 1218, 1219, 408, 142]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB 리뷰 데이터\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://github.com/euphoris/datasets/raw/master/imdb.zip')\n",
    "# 이전의 단어 문서 행렬을 활용한 감성 분석이 아닌, 언어 모형을 활용한 감성 분석 진행\n",
    "\n",
    "# 토크나이저 호출\n",
    "import joblib\n",
    "tk = joblib.load('tokenizer.pkl')\n",
    "\n",
    "# 텍스트를 토큰의 번호 시퀀스로 변환\n",
    "seqs = tk.texts_to_sequences(df['review'])\n",
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스마다 길이가 모두 다르므로 앞에 0을 채워(padding) 길이를 맞춘다\n",
    "import tensorflow as tf\n",
    "pads = tf.keras.preprocessing.sequence.pad_sequences(seqs)\n",
    "# 가장 긴 텍스트를 기준으로 길이가 똑같아 지도록 앞에 0을 채워넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 임베딩 호출\n",
    "import numpy as np\n",
    "\n",
    "z = np.load('word-emb.npz')\n",
    "e = z['emb'] # 단어 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분석 (언어 모형에서 학습되었던 학습을 전이시킴)\n",
    "emb2 = tf.keras.layers.Embedding(\n",
    "    input_dim=tk.num_words + 1, # 단어의 갯수\n",
    "    output_dim=8,               # 8차원으로 출력\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(e) # 언어 모형 학습 시 사용한 가중치로 세팅\n",
    ")\n",
    "# 감성 분석 모형에 들어갈 임베딩 레이어를 만든다. 언어 모형에서 학습된 가중치로 초기화한다\n",
    "# 다음 단어 예측 기준 학습이 아닌, 감성 예측 기준으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분석 모형 만들기\n",
    "model = tf.keras.Sequential([\n",
    "    emb2,\n",
    "    tf.keras.layers.GlobalAveragePooling1D(), # 단어마다 임베딩 값이 주어질 시, 단어가 굉장히 많기 때문에, 텍스트 안의 단어 임베딩을 평균\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # 출력 Layer를 이진분류이기 때문에 activation 을 sigmoid로 지정\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 8)           16008     \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 8)                 0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16089 (62.85 KB)\n",
      "Trainable params: 16089 (62.85 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모형 요약 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 설정\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 학습\n",
    "y = df['sentiment'].values # sentiment열의 값을 array 형태로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c48023a90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pads, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙', '진짜', '짜증나네요', '목소리']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "import pandas as pd\n",
    "nsmc = pd.read_csv('https://github.com/e9t/nsmc/raw/master/ratings_train.txt ', sep='\\t')\n",
    "\n",
    "# 전처리\n",
    "import re # 정규 표현식 활용\n",
    "\n",
    "# 한글만 찾아서 추출\n",
    "def find_hangul(text):\n",
    "    return re.findall(r'[ㄱ-ㅎ가-힣]+', text) # 정규 표현식을 활용한 적용되는 모든 것을 찾음\n",
    "# ㄱ-ㅎ, 가-힣까지 글자 중 하나라도 있으면 찾음\n",
    "\n",
    "data = nsmc[nsmc['document'].notnull()]['document'].map(find_hangul)\n",
    "# notnull 을 통해 비어있는 함수의 경우 False, 안 비어있으면 True\n",
    "# .map을 통해 함수 적용 (document의 모든 항목에 함수 적용)\n",
    "\n",
    "data[0]\n",
    "# FastText의 경우, 단어 안의 글자 단위로 임베딩을 진행하므로, 굳이 형태소 분석을 진행하지 않아도 비슷한 효과를 냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글이 아닌 글자를 지우고 공백을 하나로 합침\n",
    "def only_hangul(text):\n",
    "    return ' '.join(find_hangul(text)) #리스트의 사이 사이에 빈칸을 하나씩 넣은 뒤 합침\n",
    "\n",
    "data2 = nsmc[nsmc['document'].notnull()]['document'].map(only_hangul)\n",
    "data2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nsmc.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(data2)) # 여러 행을, 행 사이사이마다 나눠서 저장\n",
    "# FastText는 전처리한 것을 파일로 불러와도 적용되지만, 공백을 끼워넣어 줘야됨 = only_hangul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText 모형 학습\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# FastText 모형 생성\n",
    "model = FastText(vector_size=16)\n",
    "# size : 임베딩의 크기 (기본값 100) ,단어 하나마다 n차원으로 학습, 학습량: 단어 갯수 * n 차원\n",
    "# sg : 0 이면 CBOW (기본값), 1이면 Skip-gram\n",
    "# alpha: 학습률 (기본값 0.025)\n",
    "# min_alpha: 최소 학습률. FastText는 학습과정에서 학습률을 이 수준까지 점점 낮춘다 (기본값 0.0001)\n",
    "# window: 문장 내 주변 단어와 대상 단어의 최대 거리 (기본값 5)\n",
    "# min_count: 임베딩을 학습할 단어의 최소 출현 빈도 (기본값 5)\n",
    "# Word2Vec도 사용방법은 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 파악, 파일로 저장한 경우 senetece=data 대신 corpus_file='nsmc.txt'\n",
    "model.build_vocab(corpus_file='nsmc.txt')\n",
    "# 어떤 단어로 학습하는가? 어휘 목록 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4011324, 5847440)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모형 학습\n",
    "model.train(\n",
    "    corpus_file='nsmc.txt', # 데이터 입력\n",
    "    epochs=5,       # 에포크 지정\n",
    "    total_examples=model.corpus_count,\n",
    "    total_words=model.corpus_total_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "model.save('nsmc.fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "model = FastText.load('nsmc.fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모형 불러오기\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "model = FastText.load('nsmc.fasttext')\n",
    "\n",
    "# 단어 임베딩\n",
    "'히어로' in model.wv.key_to_index\n",
    "# '히어로'는 단어 임베딩이 학습되어 있다\n",
    "# model.wv.vocab = 각 어절들의 임베딩이 dictionary 형태로 有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3049521 ,  0.5861554 ,  0.26280412,  0.52573955,  0.9322114 ,\n",
       "       -0.07087298, -0.9840946 , -0.36231336,  0.147674  ,  0.16906758,\n",
       "        0.0853401 , -0.9965359 ,  0.3709958 , -0.34485692,  0.0499838 ,\n",
       "        0.5252803 ], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['히어로']\n",
    "# 히어로를 나타내는 16개의 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'슈퍼히어로' in model.wv.key_to_index\n",
    "# '슈퍼히어로'는 단어 임베딩이 없지만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16042814,  0.24366389,  0.17598806,  0.23353092,  0.33474842,\n",
       "        0.01218682, -0.3438654 , -0.16338958,  0.11673915,  0.03473993,\n",
       "        0.07153479, -0.35117054,  0.11864723, -0.11423315,  0.05896696,\n",
       "        0.250619  ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 준단어 토큰의 임베딩을 더해서 임베딩을 계산해준다\n",
    "# 학습이 안 되이었어도 글자단위 n-gram으로 쪼개, n-gram의 임베딩을 더해 계산\n",
    "model.wv['슈퍼히어로']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813293"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도\n",
    "model.wv.similarity('슈퍼히어로', '히어로')\n",
    "# '히어로'와 '슈퍼히어로'의 유사도는 높다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5460062"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity # 코사인 유사도 평가 패키지\n",
    "# 안 쓰고 모델 내장 유사도 계산 기능을 사용해도 됨\n",
    "\n",
    "model.wv.similarity('히어로', '평론가')\n",
    "# '히어로'와 '평론가'의 유사도는 상대적으로 낮다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('점대지', 0.9895889163017273),\n",
       " ('점이야', 0.9863005876541138),\n",
       " ('평론가들', 0.9855533838272095),\n",
       " ('점대야', 0.9833608865737915),\n",
       " ('점이냐', 0.9829610586166382),\n",
       " ('점대나', 0.9829360246658325),\n",
       " ('평론', 0.9824333786964417),\n",
       " ('점대면', 0.9812780022621155),\n",
       " ('점이나', 0.9801117777824402),\n",
       " ('점대는', 0.9773420691490173)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('평론가')\n",
    "# '평론가'와 비슷한 단어들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 FastText 모형 호출\n",
    "from gensim.models.fasttext import FastText\n",
    "ft = FastText.load('nsmc.fasttext')\n",
    "nsmc = pd.read_csv('https://github.com/e9t/nsmc/raw/master/ratings_train.txt ', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "df = nsmc[nsmc['document'].notnull()] # 리뷰가 있는 데이터만 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "doc_train, doc_test, y_train, y_test = train_test_split(df['document'], df['label'], test_size=0.2, random_state=42)\n",
    "# 훈련용 데이터와 테스트용 데이터 분할\n",
    "# 원래는 임베딩 학습 전에, 데이터 분할을 먼저 진행했어야 함 (트레인 데이터를 활용해 임베딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글만 추출하는 함수\n",
    "import re\n",
    "def find_hangul(text):\n",
    "    return re.findall(r'[ㄱ-ㅎ가-힣]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.zeros((1000, 16))\n",
    "# 1000, 16 크기의 행렬을 만듦 (튜플 형태) (doc_train의 shape은 119996, 인데 1000개의 데이터만 뽑아서 16차원으로 표현)\n",
    "# 단어 문서 행렬은 단어의 수를 세, 행렬을 만들지만, 임베딩을 이용해 행렬 형태로 변환함\n",
    "# 단어마다 임베딩 값을 평균내 문서의 임베딩으로 사용 = GlobalAverage 의 수동 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(doc_train.iloc[:1000]): # doc_train에서 1,000개의 행만 사용\n",
    "    vs = [ft.wv[word] for word in find_hangul(doc) if word in ft.wv] # doc_trian에서 뽑힌 행이 단어 하나하나가 되서(word), 학습된 Fasttext의 단어 임베딩에서 word에 해당되는 단어를 찾고, 없으면 만듦 \n",
    "    if vs:\n",
    "        x_train[i,] = np.mean(vs, axis=0) # 16개의 차원을, 각각의 차원에 대해 평균을 내, x_train의 i번째 행에 덮어씀\n",
    "# 각 문서에서 한글 단어를 찾아 단어 임베딩을 구하고, 이를 문서마다 평균을 낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.82882804,  0.31164083,  0.21903975,  1.14677274,  1.19770491,\n",
       "        0.71183288, -1.05152655, -1.34460402,  1.07707155, -0.52714121,\n",
       "        0.34436479, -0.54720414, -1.77543116,  0.02745596,  0.92354923,\n",
       "        0.73833674])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]\n",
    "# 각 문서의 단어 임베딩 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서의 단어 임베딩 평균을 이용하여 감성을 예측하는 모형을 만든다\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "# Embedding과 GlobalAveragePooling1D가 없지만, 데이터에서 직접 수행함\n",
    "# FastText의 경우, 토큰이 없으면 변환하는 과정이 있기 때문에, 그냥 임베딩 레이어에 덮어쓰는 형태로는 진행하기 힘듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8001 - accuracy: 0.4520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c4801d460>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train.values[:1000], epochs=1) # x_train의 갯수에 맞게 y_train 갯수 설정\n",
    "# 모델이 단순해진다는 장점 (모델의 파라미터 수가 적음)\n",
    "# 원래는 단어가 들어오면 임베딩이 들어오고 등등... FastText를 활용하면, 몇 천개의 단어를 활용하지 않고 임베딩만을 사용하기 때문에\n",
    "# 신경망 모델이 단순해져, 효율성을 높일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/imdb.zip')\n",
    "\n",
    "import joblib\n",
    "tk = joblib.load('tokenizer.pkl')\n",
    "\n",
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "review_train, review_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 982,\n",
       " 32,\n",
       " 1,\n",
       " 52,\n",
       " 956,\n",
       " 3,\n",
       " 68,\n",
       " 6,\n",
       " 30,\n",
       " 137,\n",
       " 9,\n",
       " 1,\n",
       " 987,\n",
       " 726,\n",
       " 36,\n",
       " 94,\n",
       " 1190,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 10,\n",
       " 185,\n",
       " 663,\n",
       " 675,\n",
       " 8,\n",
       " 29,\n",
       " 89,\n",
       " 385,\n",
       " 36,\n",
       " 97,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 3,\n",
       " 369,\n",
       " 24,\n",
       " 10,\n",
       " 396,\n",
       " 1,\n",
       " 213,\n",
       " 4,\n",
       " 1,\n",
       " 518,\n",
       " 10,\n",
       " 1,\n",
       " 1191,\n",
       " 1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "seqs = tk.texts_to_sequences(review_train) # 학습용 리뷰 데이터 토큰화\n",
    "seqs[0] #review_train.iloc[0] 이 번호로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순방향 순환신경망\n",
    "import tensorflow as tf\n",
    "pads = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen=None, padding='pre', truncating='pre')\n",
    "# 순환신경망에 넣기 전에, 신경망을 한 번에 처리하기 위해 길이를 일정하게 맞춰주는 패딩 작업 진행\n",
    "# 패딩 진행, 길이가 짧으면 앞쪽에 0을 채운다(padding='pre')\n",
    "# maxlen은 최대 길이를 지정할 수 있다. 지정하지 않으면 가장 긴 문자열의 길이로 지정된다\n",
    "# truncating='pre'는 maxlen보다 긴 문자열일 경우 앞쪽을 자른다. 뒤쪽을 자르게 하려면 'post'로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS= tk.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(NUM_WORDS, 8, mask_zero=True), # 임베딩 레이어를 우선 배치, 단어의 배치(NUM_WORDS), 몇 차원으로 임베딩 할것인가?(8)\n",
    "    tf.keras.layers.LSTM(8), # 순환신경망의 레이어 설정\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # 마지막 출력 레이어\n",
    "])\n",
    "# Embedding에서 mask_zero=True로 설정하면 0으로 패딩된 부분의 예측은 손실에 반영하지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 8)           16008     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16561 (64.69 KB)\n",
      "Trainable params: 16561 (64.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # 데이터가 한 번에 하나씩 들어갈 수도, 열 개씩 들어갈 수도 있으므로 첫 번째 None은 미지수이다\n",
    "                # pads.shape = (800, 73)이므로 800건에 대해 73개의 토큰을 구성하고 있으므로, 두 번째 None은 73이 된다\n",
    "                # 73개의 토큰을 받아, 각각 8차원의 임베딩으로 변환\n",
    "                # LSTM으로 들어가, 73개의 토큰을 순차적으로 처리한 뒤 8개의 출력으로 만듦\n",
    "                # 73개 토큰을 입력받아 마지막으로 출력하는데, 패딩을 뒤에 하면, 문장이 끝난 뒤 지속적으로 0이 입력되므로, LSTM 레이어에서 정보가 손실되어 0에 영향을 받을 수밖에 없음\n",
    "                # = 0을 앞으로 붙히는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 2s 14ms/step - loss: 0.6921 - accuracy: 0.5525\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.6865 - accuracy: 0.7225\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.6742 - accuracy: 0.7950\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.6349 - accuracy: 0.7900\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.5314 - accuracy: 0.8575\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.4302 - accuracy: 0.8863\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.3605 - accuracy: 0.9187\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.3012 - accuracy: 0.9413\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.2583 - accuracy: 0.9525\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.2182 - accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c4e0e6700>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pads, y_train.values, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역방향 순환신경망\n",
    "pads = tf.keras.preprocessing.sequence.pad_sequences(seqs, padding='post')\n",
    "# 패딩 진행, 길이가 짧으면 뒤쪽에 0을 채운다 (padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(NUM_WORDS, 8, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(8, go_backwards=True),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양방향 순환신경망\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(NUM_WORDS, 8, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# LSTM을 Bidirectinal로 감싸주면 자동으로 순방향과 역방향 레이어를 넣어준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 8)           16008     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 16)                1088      \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17113 (66.85 KB)\n",
      "Trainable params: 17113 (66.85 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# 중간 레이어가 두 개로 복사되기 때문에, 순방향의 8차원 출력과 역방향의 8차원 출력이 나오기 때문에 16개의 출력이 나오게 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
